{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modul_ekspart_rev1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l8bVxYQzISQI"
      ],
      "authorship_tag": "ABX9TyNVJpO8pzZ9aaneGmQB4wNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyrofist/Extraction-Requirement/blob/main/modul_ekspart_rev1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqclWxczkBRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b93507-94e2-43dd-9a6d-b7f92a67e13c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38G-s9c_sw8"
      },
      "source": [
        "!pip install XlsxWriter \n",
        "!pip install pycorenlp\n",
        "!pip install stanfordcorenlp\n",
        "!pip install spacy==2.3.2\n",
        "!pip install lemminflect==0.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RktFgAG1Yz-j",
        "cellView": "form",
        "outputId": "a2c763d4-8c10-4c93-cc85-74b08559b755"
      },
      "source": [
        "# %%writefile ekspart.py\n",
        "\"\"\"\n",
        "Created on Th Oktober  7 18:07:24 2021\n",
        "@author: Rakha asyrofi\n",
        "Ekstraksi Kebutuhan partOf\n",
        "\"\"\"\n",
        "\n",
        "#@title Modul1: Ekstraksi Kebutuhan partOf { vertical-output: true }\n",
        "url_param = \"http://corenlp.run\" #@param {type:\"string\"}\n",
        "model_param = \"/content/drive/MyDrive/stanford-corenlp-4.0.0\" #@param {type:\"string\"}\n",
        "spacy_param = \"en_core_web_sm\" #@param {type:\"string\"}\n",
        "file_param = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "grd_param = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"}\n",
        "\n",
        "# save_param = \"/content/drive/MyDrive/dataset/visualPartOf/\" #@param {type:\"string\"}\n",
        "# srs_param = \"2005 - Grid 3D\" #@param [\"0000 - Inventory\", \"2001 - esa\", \"2001 - space fractions\", \"2003 - agentmom\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - Phillip\", \"2005 - Grid 3D\", \"2005 - triangle\", \"2007 - puget sound\", \"2008 - peering\"]\n",
        "\n",
        "save_param = \"/content/drive/MyDrive/dataset/partOfAll/\" #@param {type:\"string\"}\n",
        "srs_param = \"2005 - Grid 3D\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"]\n",
        "\n",
        "data_simpan = save_param +\"partOf{}\".format(srs_param)\n",
        "tab_param = \"pertama\" #@param ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']\n",
        "mode_data = \"manual\" #@param [\"manual\", \"stanford\", \"spacy\", 'clausy']\n",
        "col_param = \"Requirement Statement\"\n",
        "\n",
        "# library yang digunakan \n",
        "import graphviz as gf, pandas as pd, xlsxwriter, re, spacy, clausySent\n",
        "from tabulate import tabulate\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
        "from triplet import extractNlp      \n",
        "from spacySent import spacyClause\n",
        "from stanfordSent import stanford_clause\n",
        "\n",
        "class partOf: #template class partOf\n",
        "\n",
        "  def __init__(self, inputData  = file_param): \n",
        "      \"\"\" parameter inisialisasi, data yang digunakan pertama kali mengunakan\n",
        "      fileParameter untuk menginisiliasi data\n",
        "      \"\"\"\n",
        "      self.__data = inputData # data inisiliasi file parameter\n",
        "\n",
        "  def fulldataset(self, inputSRS): # function membuat dataset\n",
        "      \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan\n",
        "      berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk\n",
        "      menyiapkan data selanjutnya.\n",
        "      partOf().fulldataset(inputSRS)\n",
        "      \"\"\"\n",
        "      xl = pd.ExcelFile(self.__data)\n",
        "      dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}[inputSRS]\n",
        "      return dfs\n",
        "\n",
        "  def preprocessing(self): # function melihat struktur dataset di excel\n",
        "      \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan\n",
        "      fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan\n",
        "      data excel beseerta tab yang digunakan.\n",
        "      partOf().preprocssing()\n",
        "      \"\"\"\n",
        "      xl = pd.ExcelFile(self.__data)\n",
        "      for sh in xl.sheet_names:\n",
        "        df = xl.parse(sh)\n",
        "        print('Processing: [{}] ...'.format(sh))\n",
        "        print(df.head())\n",
        "\n",
        "  def visualisasiGraph(self, source_data, part_data, srs_param):\n",
        "      \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf.\n",
        "      data diambil berdasarkan referensi dari source data untuk parent node.\n",
        "      part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan\n",
        "      partOf().visualisasiGraph(source_data, part_data, srs_param)\n",
        "      \"\"\"\n",
        "      f = gf.Digraph('finite_state_machine', filename='partOf.gv', \n",
        "                       engine= 'neato')\n",
        "      f.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "      f.attr('node', shape='doublecircle') # node\n",
        "      for angka in source_data.ID:\n",
        "          f.node(angka)\n",
        "      f.attr(kw= 'node', shape='circle') # edge\n",
        "      for idx, num in zip(part_data.label, part_data.ID):\n",
        "          f.edge(idx, num, label='partOf')\n",
        "\n",
        "      f.attr(overlap='false')\n",
        "      f.attr(label=r'Visulasisasi relasi partOf {}\\n'.format(srs_param))\n",
        "      f.attr(fontsize='20')\n",
        "      f.view(data_simpan)\n",
        "      print(\"Gambar disimpan ke {}\".format(data_simpan))\n",
        "      return f\n",
        "\n",
        "  def evaluasi_data(self, data1, data2):\n",
        "      \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi\n",
        "      nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu.\n",
        "      cara menggunakan syntax ini yaitu melalui\n",
        "      partOf().evaluasi_data(data1, data2)\n",
        "      \"\"\"\n",
        "      y_actual = data1.values.astype(int) #define array of actual values\n",
        "      y_predicted = data2.values.astype(int) #define array of predicted values\n",
        "      nilai_akurasi = accuracy_score(y_actual, y_predicted, normalize=True)\n",
        "      nilai_recall = recall_score(y_actual, y_predicted, average= 'macro')\n",
        "      nilai_presisi = precision_score(y_actual, y_predicted, average= 'macro')\n",
        "      print(\"akurasi {}\\n recall {}\\n presisi {}\\n\".format(nilai_akurasi, nilai_recall, nilai_presisi))\n",
        "      print(classification_report(y_actual, y_predicted))\n",
        "\n",
        "  def simpan_excel(self, data1, data2, data3, data4):\n",
        "      \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi\n",
        "      tabel kebutuhan, partOf, relasi, dan nilai data secara statistik.\n",
        "      cara menggunakan syntax ini yaitu melalui\n",
        "      partOf().simpan_excel(data1, data2, data3, data4)\n",
        "      \"\"\"\n",
        "      dfs  = { # save file\n",
        "                'tabel_kebutuhan' : data1, \n",
        "                'tabel_partOf' : data2,\n",
        "                'tabel_relasi' : data3,\n",
        "                'tabel_statistika' : data4,\n",
        "              } \n",
        "      writer = pd.ExcelWriter(data_simpan+ '.xlsx')\n",
        "      for name,dataframe in dfs.items():\n",
        "          dataframe.to_excel(writer,name,index=False)\n",
        "      writer.save()\n",
        "      print(\"data excel disimpan di {}\".format(data_simpan+ '.xlsx'))\n",
        "\n",
        "  # def tabulasi_filter(self, data, mode_data= ['manual', 'triplet']): # tabulasi_filter\n",
        "  def tabulasi_filter(self, data, mode= mode_data): # tabulasi_filter\n",
        "      \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan.\n",
        "      mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford.\n",
        "      sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya.\n",
        "      cara menggunakan syntax ini yaitu melalui\n",
        "      partOf().tabulasi_filter(data, mode= ['manual', 'triplet', 'spacy', 'stanford'])\n",
        "      \"\"\"\n",
        "      if 'manual' in mode:\n",
        "          hasil_srs = []\n",
        "          for idx, num in zip(data['ID'], data['Requirement Statement'].fillna(\"empty\")):\n",
        "              data = [x10 for x1 in num.split(\".\") for x2 in x1.split(\" that \")  \n",
        "                          for x3 in x2.split(\"/\") for x4 in x3.split(\" so \")  \n",
        "                          for x5 in x4.split(\",\") for x6 in x5.split(\" and \")\n",
        "                          for x7 in x6.split(\" i.e.\") for x8 in x7.split(\" or \")\n",
        "                          for x9 in x8.split(\" if \")  for x10 in x9.split(\" ; \")]\n",
        "          # for idx, num in zip(data['ID'], data['Requirement Statement']):\n",
        "          #     data = re.split(' ; |, | / | that | and | or | i.e. | if | so | e.g. ', num)\n",
        "              conv = lambda i : i or None\n",
        "              res = [conv(i) for i in data]\n",
        "              hasil_srs.append([idx, res])\n",
        "          a_df = pd.DataFrame(hasil_srs, columns = ['ID', 'data'])\n",
        "          return a_df\n",
        "\n",
        "      elif 'spacy' in mode:\n",
        "          dataSpacy = []\n",
        "          mySpacy = spacyClause(file_param)\n",
        "          nlp = spacy.load(spacy_param)\n",
        "          for idx, num in zip(data['ID'], data['Requirement Statement'].fillna(\"empty\")):\n",
        "              doc = nlp(num)\n",
        "              myClause = mySpacy.extractData(doc)\n",
        "              dataSpacy.append([idx, myClause])\n",
        "          a_df = pd.DataFrame(dataSpacy, columns = ['ID', 'data'])\n",
        "          return a_df\n",
        "\n",
        "      elif 'clausy' in mode:\n",
        "          data_c = []\n",
        "          nlp = spacy.load(spacy_param)\n",
        "          clausySent.add_to_pipe(nlp)\n",
        "          for id, num in zip(spacyClause(dataFile).fulldataset(srs_param)['ID'],\n",
        "                              spacyClause(dataFile).fulldataset(srs_param)['Requirement Statement']):\n",
        "              doc = nlp(num)\n",
        "              sent_c = [clause.to_propositions(as_text=True, capitalize=True)[0] for clause in doc._.clauses]\n",
        "              data_c.append([id, sent_c])\n",
        "          a_df = pd.DataFrame(data_c, columns = ['ID', 'data'])\n",
        "          return a_df\n",
        "\n",
        "      elif 'stanford' in mode:\n",
        "          data_clausa = []\n",
        "          myClause = stanford_clause(file_param, url_param, model_param)\n",
        "          for idx, num in zip(data['ID'], data['Requirement Statement'].fillna(\"empty\")):\n",
        "              sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\",\" \",num)\n",
        "              clause_list = [idx for idx in myClause.get_clause_list(sent)]\n",
        "              data_clausa.append([idx, clause_list])\n",
        "          a_df = pd.DataFrame(data_clausa, columns = ['ID', 'data'])\n",
        "          return a_df\n",
        "\n",
        "  def tabulasi_pertama(self, data, dataReq): # tabulasi_pertama\n",
        "      \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi\n",
        "      data atomik dan non atomik, dari banyak kalimat yang digunakan.\n",
        "      jika terdiri atas satu kalimat maka disebut sebagai atomik. \n",
        "      namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. \n",
        "      partOf().tabulasi_pertama(data)\n",
        "      \"\"\"\n",
        "      c_df = data.copy()\n",
        "      data_df = pd.DataFrame([sh for sh in c_df.data], index= dataReq.ID)\n",
        "      list_column = [\"data{}\".format(num) for num in range(data_df.columns.stop)]\n",
        "      data_df.columns = list_column\n",
        "\n",
        "      b_df = []\n",
        "      b_df_jumlah = []\n",
        "      for num in c_df.data: # menentukan data atomik dan \n",
        "        if len(num) > 1: # non atomik berdasarkan jumlah\n",
        "          b_df.append('non_atomik')\n",
        "          b_df_jumlah.append(len(num))\n",
        "        elif len(num) == 1:\n",
        "          b_df.append('atomik')\n",
        "          b_df_jumlah.append(len(num))\n",
        "      c_df['label'] = b_df\n",
        "      c_df['jumlah'] = b_df_jumlah\n",
        "      return c_df\n",
        "\n",
        "  def tabulasi_kedua(self, data): # tabulasi kedua\n",
        "      \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi\n",
        "      dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan \n",
        "      setiap non atomik memiliki kebergantungan partOf didalamnya.\n",
        "      partOf().tabulasi_kedua(data)\n",
        "      \"\"\"\n",
        "      c_df = data.copy()\n",
        "      na_data = c_df.loc[c_df['label'] == 'non_atomik']\n",
        "      data_na = [([na_data.ID[num], index, 'p{}'.format(idx)]) \n",
        "      for idx, num in enumerate(na_data.index) \n",
        "      for index in na_data.data[num] if index is not None]\n",
        "      na_df = pd.DataFrame(data_na, columns= ['ID', 'req', 'label'])\n",
        "      a_data = c_df.loc[c_df['label'] == 'atomik']\n",
        "      data_a = [([a_data.ID[num], index, 'atomik']) for num in a_data.index \n",
        "                for idx, index in enumerate(a_data.data[num]) \n",
        "                if index is not None]\n",
        "      a_df = pd.DataFrame(data_a, columns= ['ID', 'req', 'label'])\n",
        "      part_df = pd.concat([a_df, na_df], ignore_index= True)\n",
        "      part_srt = part_df.sort_values(by='ID', ignore_index= True).drop_duplicates()\n",
        "      return part_srt\n",
        "\n",
        "  def tabulasi_ketiga(self, data, data_index): # tabulasi ketiga\n",
        "      \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi\n",
        "      sebuah matriks indeks dan kolom yang saling berelasi satu sama lain.\n",
        "      sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan\n",
        "      partOf().tabulasi_ketiga(data)\n",
        "      \"\"\"\n",
        "      part_srt = data.copy()\n",
        "      list_data = [part_srt.loc[part_srt.ID == num].label \n",
        "                   for num in data_index.ID]\n",
        "      tb_part = pd.DataFrame(list_data).fillna(0)\n",
        "      tb_part.columns = part_srt.ID\n",
        "      tb_part.index = data_index.ID\n",
        "      return tb_part.reset_index()\n",
        "\n",
        "  def tabulasi_alternatifernatif(self, data): # Alternatif\n",
        "      \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif.\n",
        "      untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan \n",
        "      fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan.\n",
        "      Berikut ini syntax yang digunakan.\n",
        "      partOf().tabulasi_alternatifernatif(data)\n",
        "      \"\"\"\n",
        "      d_df = data.copy()\n",
        "      na_data = d_df.loc[d_df['label'] == 'non_atomik']\n",
        "      data_na = [([na_data.ID[num], index, 'p{}'.format(idx)]) \n",
        "                    for idx, num in enumerate(na_data.index) \n",
        "                    for index in na_data.data[num] if index is not None]\n",
        "      na_df = pd.DataFrame(data_na, columns= ['ID', 'data', 'label'])\n",
        "      a_data = d_df.loc[d_df['label'] == 'atomik']\n",
        "      dt = pd.concat([a_data, na_data, na_df], ignore_index= True)\n",
        "      part_br = dt.sort_values(by='ID', ignore_index= True)\n",
        "      list_data = [part_br.loc[part_br.ID == num].label for num in data.ID]\n",
        "      dt_part = pd.DataFrame(list_data).fillna(0)\n",
        "\n",
        "      # rename data\n",
        "      data_part = [(['{}_{}'.format(na_data.ID[num], idy), index, 'p{}'.format(idx)]) \n",
        "                  for idx, num in enumerate(na_data.index) \n",
        "                  for idy, index in enumerate(na_data.data[num]) if index is not None]\n",
        "      part_na = pd.DataFrame(data_part, columns= ['ID', 'req', 'label'])\n",
        "      dt_rename = pd.concat([a_data, na_data, part_na], ignore_index= True)\n",
        "      sort_rename = dt_rename.sort_values(by='ID')\n",
        "      dt_part.columns= sort_rename.ID\n",
        "      dt_part.index = data.ID\n",
        "      return dt_part.reset_index()\n",
        "\n",
        "  def tabulasi_visual(self, data): # visualisasi\n",
        "      \"\"\" fungsi ini digunakan untuk melihat data secara visual, \n",
        "      fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan\n",
        "      sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya \n",
        "      yaitu visual data \n",
        "      partOf().tabulasi_visual(data)\n",
        "      \"\"\"\n",
        "      c_df = data.copy()\n",
        "      na_data = c_df.loc[c_df['label'] == 'non_atomik']\n",
        "      part_list = [([na_data.ID[num], index, 'p{}_{}'.format(idx, idy)]) \n",
        "      for idx, num in enumerate(na_data.index) \n",
        "      for idy, index in enumerate(na_data.data[num]) if index is not None]\n",
        "      part_visual = pd.DataFrame(part_list, columns= ['ID', 'req', 'label'])\n",
        "      return partOf.visualisasiGraph(self, data, part_visual, srs_param)\n",
        "\n",
        "  def nilai_stat(self, data1, data2): # fungsi menentukan nilai statistik\n",
        "      \"\"\" fungsi ini digunakan untuk melihat data statistik test, \n",
        "      fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi\n",
        "      jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat\n",
        "      cara menggunakan syntax ini adalah dengan cara \n",
        "      partOf().stat_stat(data)\n",
        "      \"\"\"\n",
        "      jml_kebutuhan = len(data1)\n",
        "      jml_minimum = data1.jumlah.min()\n",
        "      jml_maksimum = data1.jumlah.max()\n",
        "      jml_atomik = len(data1.loc[data1['label'] == 'atomik'])\n",
        "      jml_nonatomik = len(data1.loc[data1['label'] == 'non_atomik'])\n",
        "      jml_klausa =len(data2.loc[data2['label'] != 'atomik'])\n",
        "      jml_df = pd.DataFrame([jml_kebutuhan,jml_atomik, jml_nonatomik, \n",
        "                             jml_minimum, jml_maksimum])\n",
        "      jml_df.index = ['jumlah_kebtuhan', 'jumlah_atomik', 'jumlah_nonatomik', \n",
        "                      'minimum_jumlah_kalimat', 'maksimum_jumlah_kalimat']\n",
        "      jml_df.columns = ['statistik_test']\n",
        "      return jml_df.reset_index()\n",
        "\n",
        "  def stat_grountruth(self, data):\n",
        "      \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, \n",
        "      fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi\n",
        "      jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat\n",
        "      cara menggunakan syntax ini adalah dengan cara \n",
        "      partOf().stat_grountruth(data)\n",
        "      \"\"\"\n",
        "      df_part = data.copy()\n",
        "      nlp = spacy.load('en_core_web_sm')\n",
        "      jml_atomik = df_part.loc[df_part['Sentence'] == 'a'].Sentence.count()\n",
        "      jml_nonAtomik = df_part.loc[df_part['Sentence'] != 'a'].drop_duplicates(subset='Sentence').Sentence.count()\n",
        "      jml_klausa = df_part.loc[df_part['Sentence'] != 'a'].Sentence.count()\n",
        "      jml_kebutuhan = jml_atomik + jml_nonAtomik\n",
        "      jml_data = [len([idx for idx in nlp(num).sents])for num in df_part['Requirement Statement']]\n",
        "      jml_a = [num for num in df_part.Sentence.value_counts().astype(int)]\n",
        "      jml_min = min(jml_data)\n",
        "      try:\n",
        "        jml_maks = max(jml_a[1:])\n",
        "      except:\n",
        "        jml_maks = max(jml_data)\n",
        "\n",
        "      jml_df = pd.DataFrame([jml_kebutuhan,jml_atomik, jml_nonAtomik, \n",
        "                              jml_min, jml_maks])\n",
        "      jml_df.index = ['jumlah_kebtuhan', 'jumlah_atomik', 'jumlah_nonatomik', \n",
        "                      'minimum_jumlah_kalimat', 'maksimum_jumlah_kalimat']\n",
        "      jml_df.columns = ['statistik_groundtruth']\n",
        "      return jml_df.reset_index()\n",
        "\n",
        "  def __del__(self):\n",
        "      \"\"\" fungsi ini digunakan untuk mendestruksi, \n",
        "      cara meggunakan panggil fungsi dengan syntax berikut ini:\n",
        "      partOf().__del__(self)\n",
        "      \"\"\"\n",
        "      print(\"Destructed\")\n",
        "\n",
        "  def extractPart(self, output= tab_param):\n",
        "      \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan.\n",
        "      fungsi ini menunjukkan data ekstraksi yang digunakan meliputi\n",
        "      - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, \n",
        "        data visual, data statistik, dan simpan\n",
        "      - par2; data groundtruth beserta nilai statistiknya\n",
        "      partOf().__del__(self)\n",
        "      \"\"\"\n",
        "      part2 = partOf(grd_param)\n",
        "      part_grd = part2.fulldataset(srs_param)\n",
        "      data_grountruth = part2.stat_grountruth(part_grd)\n",
        "      part2.__del__()\n",
        "\n",
        "      part1 = partOf(file_param)\n",
        "      dataReq = part1.fulldataset(srs_param)\n",
        "      data_filtrasi = part1.tabulasi_filter(dataReq)\n",
        "      data_pertama = part1.tabulasi_pertama(data_filtrasi, dataReq)\n",
        "      data_kedua = part1.tabulasi_kedua(data_pertama)\n",
        "      data_ketiga = part1.tabulasi_ketiga(data_kedua, data_pertama)\n",
        "      alternatif = part1.tabulasi_alternatifernatif(data_pertama)\n",
        "      data_visual = part1.tabulasi_visual(data_pertama)\n",
        "      data_stat = part1.nilai_stat(data_pertama, data_kedua)\n",
        "      part1.simpan_excel(data_pertama, data_kedua, data_ketiga, data_stat)\n",
        "      part1.__del__() \n",
        "\n",
        "      if 'pertama' in output:\n",
        "        print(\"\\nTabulasi Pertama {}\".format(srs_param))\n",
        "        print(tabulate(data_pertama, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      elif 'kedua' in output:\n",
        "        print(\"\\nTabulasi Kedua {}\".format(srs_param))\n",
        "        print(tabulate(data_kedua, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      elif 'ketiga' in output:\n",
        "        print(\"\\nTabulasi Ketiga  {}\".format(srs_param))\n",
        "        print(tabulate(data_ketiga, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      elif 'alternatif' in output:\n",
        "        print(\"\\nTabulasi Ketiga Alternatif  {}\".format(srs_param))\n",
        "        print(tabulate(alternatif, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      elif 'stat' in output:\n",
        "        print(\"\\nTabulasi Statistik  {}\".format(srs_param))\n",
        "        print(tabulate(data_grountruth, headers = 'keys', tablefmt = 'psql'))\n",
        "        print(tabulate(data_stat, headers = 'keys', tablefmt = 'psql'))\n",
        "        part2.evaluasi_data(data_stat.drop('index', axis= 1), data_grountruth.drop('index', axis= 1))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "    partOf().extractPart(tab_param)\n",
        "\n",
        "  except OSError as err:\n",
        "    print(\"OS error: {0}\".format(err))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructed\n",
            "Gambar disimpan ke /content/drive/MyDrive/dataset/partOfAll/partOf2005 - Grid 3D\n",
            "data excel disimpan di /content/drive/MyDrive/dataset/partOfAll/partOf2005 - Grid 3D.xlsx\n",
            "Destructed\n",
            "\n",
            "Tabulasi Pertama 2005 - Grid 3D\n",
            "+----+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+\n",
            "|    | ID   | data                                                                                                                                                                                       | label      |   jumlah |\n",
            "|----+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------|\n",
            "|  0 | F01  | ['The product shall plot the data points in a scientifically correct manner']                                                                                                              | atomik     |        1 |\n",
            "|  1 | F02  | ['The grid axis should be labelled correctly according to the input from the data file']                                                                                                   | atomik     |        1 |\n",
            "|  2 | F03  | ['Data points should be coloured in accordance to the cluster number']                                                                                                                     | atomik     |        1 |\n",
            "|  3 | F04  | ['The product should be able to handle up to 2000 data points']                                                                                                                            | atomik     |        1 |\n",
            "|  4 | F05  | ['A single click of the mouse over a data point should bring up the name of the data point']                                                                                               | atomik     |        1 |\n",
            "|  5 | F06  | ['A double-click of the mouse over the data point should display all the data details']                                                                                                    | atomik     |        1 |\n",
            "|  6 | F07  | ['The product should allow multiple points to clicked so', 'multiple names can be displayed']                                                                                              | non_atomik |        2 |\n",
            "|  7 | F08  | ['The product should allow the grid to be oriented by the user', ' Rotation', ' zoom', 'move functions should be employed']                                                                | non_atomik |        4 |\n",
            "|  8 | F09  | ['The data file should contain:\\nA name for the data point', ' 3 parameters from which the data point is to be plotted', ' A single parameter to designate the colour of the point', None] | non_atomik |        4 |\n",
            "|  9 | NF01 | ['The points should be large enough to see', 'select']                                                                                                                                     | non_atomik |        2 |\n",
            "| 10 | NF02 | ['The points should not be too big', None, 'as to distort the overall pattern of the point spread']                                                                                        | non_atomik |        3 |\n",
            "| 11 | NF03 | ['The axis should be clearly labelled', 'easily recognised after the grid has been oriented into a different position']                                                                    | non_atomik |        2 |\n",
            "| 12 | NF04 | ['The application should be coloured so', 'the screen shots can be printed out clearly']                                                                                                   | non_atomik |        2 |\n",
            "| 13 | NF05 | ['The application should be intuitive', 'not require any specialist training']                                                                                                             | non_atomik |        2 |\n",
            "| 14 | NF06 | ['The program should start within 30 seconds', ' This depends on the number of data points', 'are to be plotted']                                                                          | non_atomik |        3 |\n",
            "| 15 | NF07 | ['The interaction with the data points should have a delay of no longer than 2 seconds']                                                                                                   | atomik     |        1 |\n",
            "| 16 | NF08 | [\"The response to a change in the orientation should be fast enough to avoid interrupting the user's flow of thought\"]                                                                     | atomik     |        1 |\n",
            "+----+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+\n",
            "Destructed\n",
            "Destructed\n",
            "Destructed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "5uSoXoMPiah2",
        "outputId": "771f1da7-a6e4-4519-fa8c-2e28d8e5d1d2"
      },
      "source": [
        "partKu = partOf() # class partOf\n",
        "a = partKu.fulldataset(srs_param) # menentukan kebutuhan\n",
        "b = partKu.tabulasi_filter(a) # splitting sentence\n",
        "c = partKu.tabulasi_pertama(b, a) # membuat tabulasi atomik dan non atomik\n",
        "partKu.tabulasi_visual(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructed\n",
            "Gambar disimpan ke /content/drive/MyDrive/dataset/partOfAll/partOf2005 - Grid 3D\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f2e8109c050>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: finite_state_machine Pages: 1 -->\n<svg width=\"311pt\" height=\"360pt\"\n viewBox=\"0.00 0.00 310.56 360.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.4367 .4367) rotate(0) translate(4 820.2982)\">\n<title>finite_state_machine</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-820.2982 707.1024,-820.2982 707.1024,4 -4,4\"/>\n<text text-anchor=\"middle\" x=\"351.5512\" y=\"-10\" font-family=\"Times,serif\" font-size=\"20.00\" fill=\"#000000\">Visulasisasi relasi partOf 2005 &#45; Grid 3D</text>\n<!-- F01 -->\n<g id=\"node1\" class=\"node\">\n<title>F01</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"131.9977\" cy=\"-144.9977\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"131.9977\" cy=\"-144.9977\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"131.9977\" y=\"-141.2977\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F01</text>\n</g>\n<!-- F02 -->\n<g id=\"node2\" class=\"node\">\n<title>F02</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"403.9977\" cy=\"-127.9977\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"403.9977\" cy=\"-127.9977\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"403.9977\" y=\"-124.2977\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F02</text>\n</g>\n<!-- F03 -->\n<g id=\"node3\" class=\"node\">\n<title>F03</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"488.9977\" cy=\"-127.9977\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"488.9977\" cy=\"-127.9977\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"488.9977\" y=\"-124.2977\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F03</text>\n</g>\n<!-- F04 -->\n<g id=\"node4\" class=\"node\">\n<title>F04</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"539.9977\" cy=\"-450.9977\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"539.9977\" cy=\"-450.9977\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"539.9977\" y=\"-447.2977\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F04</text>\n</g>\n<!-- F05 -->\n<g id=\"node5\" class=\"node\">\n<title>F05</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"488.9977\" cy=\"-620.9977\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"488.9977\" cy=\"-620.9977\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"488.9977\" y=\"-617.2977\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F05</text>\n</g>\n<!-- F06 -->\n<g id=\"node6\" class=\"node\">\n<title>F06</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"29.9977\" cy=\"-365.9977\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"29.9977\" cy=\"-365.9977\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"29.9977\" y=\"-362.2977\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F06</text>\n</g>\n<!-- F07 -->\n<g id=\"node7\" class=\"node\">\n<title>F07</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"620.1492\" cy=\"-141.9778\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"620.1492\" cy=\"-141.9778\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"620.1492\" y=\"-138.2778\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F07</text>\n</g>\n<!-- F08 -->\n<g id=\"node8\" class=\"node\">\n<title>F08</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"507.0991\" cy=\"-282.5519\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"507.0991\" cy=\"-282.5519\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"507.0991\" y=\"-278.8519\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F08</text>\n</g>\n<!-- F09 -->\n<g id=\"node9\" class=\"node\">\n<title>F09</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"442.2642\" cy=\"-516.4962\" rx=\"25.9957\" ry=\"25.9957\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"442.2642\" cy=\"-516.4962\" rx=\"29.9954\" ry=\"29.9954\"/>\n<text text-anchor=\"middle\" x=\"442.2642\" y=\"-512.7962\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">F09</text>\n</g>\n<!-- NF01 -->\n<g id=\"node10\" class=\"node\">\n<title>NF01</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"231.8613\" cy=\"-574.699\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"231.8613\" cy=\"-574.699\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"231.8613\" y=\"-570.999\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF01</text>\n</g>\n<!-- NF02 -->\n<g id=\"node11\" class=\"node\">\n<title>NF02</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"129.8613\" cy=\"-642.699\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"129.8613\" cy=\"-642.699\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"129.8613\" y=\"-638.999\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF02</text>\n</g>\n<!-- NF03 -->\n<g id=\"node12\" class=\"node\">\n<title>NF03</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"61.8613\" cy=\"-234.699\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"61.8613\" cy=\"-234.699\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"61.8613\" y=\"-230.999\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF03</text>\n</g>\n<!-- NF04 -->\n<g id=\"node13\" class=\"node\">\n<title>NF04</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"299.8613\" cy=\"-166.699\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"299.8613\" cy=\"-166.699\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"299.8613\" y=\"-162.999\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF04</text>\n</g>\n<!-- NF05 -->\n<g id=\"node14\" class=\"node\">\n<title>NF05</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"605.8613\" cy=\"-710.699\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"605.8613\" cy=\"-710.699\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"605.8613\" y=\"-706.999\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF05</text>\n</g>\n<!-- NF06 -->\n<g id=\"node15\" class=\"node\">\n<title>NF06</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"243.4525\" cy=\"-320.2666\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"243.4525\" cy=\"-320.2666\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"243.4525\" y=\"-316.5666\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF06</text>\n</g>\n<!-- NF07 -->\n<g id=\"node16\" class=\"node\">\n<title>NF07</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"308.4971\" cy=\"-440.4971\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"308.4971\" cy=\"-440.4971\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"308.4971\" y=\"-436.7971\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF07</text>\n</g>\n<!-- NF08 -->\n<g id=\"node17\" class=\"node\">\n<title>NF08</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"87.4971\" cy=\"-457.4971\" rx=\"32.4945\" ry=\"32.4945\"/>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"87.4971\" cy=\"-457.4971\" rx=\"36.4942\" ry=\"36.4942\"/>\n<text text-anchor=\"middle\" x=\"87.4971\" y=\"-453.7971\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NF08</text>\n</g>\n<!-- p0_0 -->\n<g id=\"node18\" class=\"node\">\n<title>p0_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"672.5551\" cy=\"-210.5296\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"672.5551\" y=\"-206.8296\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p0_0</text>\n</g>\n<!-- p0_0&#45;&gt;F07 -->\n<g id=\"edge1\" class=\"edge\">\n<title>p0_0&#45;&gt;F07</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M653.9312,-186.1678C650.9966,-182.329 647.9232,-178.3087 644.8765,-174.3234\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"647.5671,-172.08 638.7132,-166.2612 642.006,-176.3314 647.5671,-172.08\"/>\n<text text-anchor=\"middle\" x=\"667.9038\" y=\"-169.0456\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p0_1 -->\n<g id=\"node19\" class=\"node\">\n<title>p0_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"591.5473\" cy=\"-60.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"591.5473\" y=\"-56.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p0_1</text>\n</g>\n<!-- p0_1&#45;&gt;F07 -->\n<g id=\"edge2\" class=\"edge\">\n<title>p0_1&#45;&gt;F07</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M601.7118,-89.4859C603.3325,-94.1002 605.0308,-98.9352 606.713,-103.7246\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"603.4974,-105.1312 610.1136,-113.4062 610.1018,-102.8114 603.4974,-105.1312\"/>\n<text text-anchor=\"middle\" x=\"585.7124\" y=\"-100.4053\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p1_0 -->\n<g id=\"node20\" class=\"node\">\n<title>p1_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"576.1348\" cy=\"-237.7926\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"576.1348\" y=\"-234.0926\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p1_0</text>\n</g>\n<!-- p1_0&#45;&gt;F08 -->\n<g id=\"edge3\" class=\"edge\">\n<title>p1_0&#45;&gt;F08</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M550.4285,-254.4593C547.4239,-256.4073 544.3239,-258.4172 541.2281,-260.4244\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"539.0838,-257.6433 532.5971,-266.0203 542.8919,-263.5169 539.0838,-257.6433\"/>\n<text text-anchor=\"middle\" x=\"527.3283\" y=\"-246.2418\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p1_1 -->\n<g id=\"node21\" class=\"node\">\n<title>p1_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"462.4169\" cy=\"-213.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"462.4169\" y=\"-209.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p1_1</text>\n</g>\n<!-- p1_1&#45;&gt;F08 -->\n<g id=\"edge4\" class=\"edge\">\n<title>p1_1&#45;&gt;F08</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M479.0549,-239.2421C480.9996,-242.2453 483.006,-245.3439 485.0097,-248.4383\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"482.2228,-250.5739 490.596,-257.0654 488.0986,-246.7691 482.2228,-250.5739\"/>\n<text text-anchor=\"middle\" x=\"463.5323\" y=\"-247.6402\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p1_2 -->\n<g id=\"node22\" class=\"node\">\n<title>p1_2</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"438.5473\" cy=\"-327.9501\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"438.5473\" y=\"-324.2501\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p1_2</text>\n</g>\n<!-- p1_2&#45;&gt;F08 -->\n<g id=\"edge5\" class=\"edge\">\n<title>p1_2&#45;&gt;F08</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M464.0735,-311.0455C467.057,-309.0697 470.1352,-307.0311 473.2094,-304.9952\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.3749,-307.7591 481.7799,-299.3195 471.5099,-301.9228 475.3749,-307.7591\"/>\n<text text-anchor=\"middle\" x=\"487.1414\" y=\"-311.8204\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p1_3 -->\n<g id=\"node23\" class=\"node\">\n<title>p1_3</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"552.4877\" cy=\"-351.201\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"552.4877\" y=\"-347.501\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p1_3</text>\n</g>\n<!-- p1_3&#45;&gt;F08 -->\n<g id=\"edge6\" class=\"edge\">\n<title>p1_3&#45;&gt;F08</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M535.5867,-325.6386C533.6113,-322.6509 531.5731,-319.5682 529.5377,-316.4897\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"532.298,-314.3184 523.8632,-307.9071 526.4588,-318.179 532.298,-314.3184\"/>\n<text text-anchor=\"middle\" x=\"551.0622\" y=\"-309.8642\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p2_0 -->\n<g id=\"node24\" class=\"node\">\n<title>p2_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"528.9775\" cy=\"-529.9549\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"528.9775\" y=\"-526.2549\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p2_0</text>\n</g>\n<!-- p2_0&#45;&gt;F09 -->\n<g id=\"edge7\" class=\"edge\">\n<title>p2_0&#45;&gt;F09</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M498.6494,-525.2477C493.2771,-524.4139 487.6106,-523.5344 482.0243,-522.6673\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"482.5549,-519.2079 472.1363,-521.1326 481.4812,-526.125 482.5549,-519.2079\"/>\n<text text-anchor=\"middle\" x=\"481.0868\" y=\"-512.7575\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p2_1 -->\n<g id=\"node25\" class=\"node\">\n<title>p2_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"410.9796\" cy=\"-434.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"410.9796\" y=\"-430.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p2_1</text>\n</g>\n<!-- p2_1&#45;&gt;F09 -->\n<g id=\"edge8\" class=\"edge\">\n<title>p2_1&#45;&gt;F09</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M421.9214,-463.209C423.8597,-468.2862 425.904,-473.6413 427.9195,-478.9207\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"424.6505,-480.1713 431.4869,-488.2654 431.1901,-477.6747 424.6505,-480.1713\"/>\n<text text-anchor=\"middle\" x=\"406.4205\" y=\"-474.8649\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p2_2 -->\n<g id=\"node26\" class=\"node\">\n<title>p2_2</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"387.5473\" cy=\"-585.048\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"387.5473\" y=\"-581.348\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p2_2</text>\n</g>\n<!-- p2_2&#45;&gt;F09 -->\n<g id=\"edge9\" class=\"edge\">\n<title>p2_2&#45;&gt;F09</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M406.6846,-561.0719C410.0746,-556.8248 413.6502,-552.3451 417.1752,-547.9288\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"419.9117,-550.1109 423.4146,-540.1119 414.4408,-545.7441 419.9117,-550.1109\"/>\n<text text-anchor=\"middle\" x=\"393.4299\" y=\"-543.3004\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p3_0 -->\n<g id=\"node27\" class=\"node\">\n<title>p3_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"289.2363\" cy=\"-649.7509\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"289.2363\" y=\"-646.0509\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p3_0</text>\n</g>\n<!-- p3_0&#45;&gt;NF01 -->\n<g id=\"edge10\" class=\"edge\">\n<title>p3_0&#45;&gt;NF01</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M270.4495,-625.1761C267.1814,-620.9011 263.7178,-616.3704 260.2599,-611.8471\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"262.9389,-609.5886 254.085,-603.7698 257.3778,-613.8399 262.9389,-609.5886\"/>\n<text text-anchor=\"middle\" x=\"283.8547\" y=\"-607.3116\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p3_1 -->\n<g id=\"node28\" class=\"node\">\n<title>p3_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"200.5473\" cy=\"-485.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"200.5473\" y=\"-481.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p3_1</text>\n</g>\n<!-- p3_1&#45;&gt;NF01 -->\n<g id=\"edge11\" class=\"edge\">\n<title>p3_1&#45;&gt;NF01</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M210.8007,-514.7389C212.5843,-519.8171 214.4747,-525.1989 216.3619,-530.5721\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.1158,-531.8918 219.7321,-540.1669 219.7203,-529.572 213.1158,-531.8918\"/>\n<text text-anchor=\"middle\" x=\"195.0813\" y=\"-526.4555\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p4_0 -->\n<g id=\"node29\" class=\"node\">\n<title>p4_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"187.2363\" cy=\"-717.7509\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"187.2363\" y=\"-714.0509\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p4_0</text>\n</g>\n<!-- p4_0&#45;&gt;NF02 -->\n<g id=\"edge12\" class=\"edge\">\n<title>p4_0&#45;&gt;NF02</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M168.4495,-693.1761C165.1814,-688.9011 161.7178,-684.3704 158.2599,-679.8471\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"160.9389,-677.5886 152.085,-671.7698 155.3778,-681.8399 160.9389,-677.5886\"/>\n<text text-anchor=\"middle\" x=\"181.8547\" y=\"-675.3116\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p4_2 -->\n<g id=\"node30\" class=\"node\">\n<title>p4_2</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"98.5473\" cy=\"-553.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"98.5473\" y=\"-549.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p4_2</text>\n</g>\n<!-- p4_2&#45;&gt;NF02 -->\n<g id=\"edge13\" class=\"edge\">\n<title>p4_2&#45;&gt;NF02</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M108.8007,-582.7389C110.5843,-587.8171 112.4747,-593.1989 114.3619,-598.5721\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"111.1158,-599.8918 117.7321,-608.1669 117.7203,-597.572 111.1158,-599.8918\"/>\n<text text-anchor=\"middle\" x=\"93.0813\" y=\"-594.4555\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p5_0 -->\n<g id=\"node31\" class=\"node\">\n<title>p5_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"119.2363\" cy=\"-309.7509\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"119.2363\" y=\"-306.0509\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p5_0</text>\n</g>\n<!-- p5_0&#45;&gt;NF03 -->\n<g id=\"edge14\" class=\"edge\">\n<title>p5_0&#45;&gt;NF03</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M100.4495,-285.1761C97.1814,-280.9011 93.7178,-276.3704 90.2599,-271.8471\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"92.9389,-269.5886 84.085,-263.7698 87.3778,-273.8399 92.9389,-269.5886\"/>\n<text text-anchor=\"middle\" x=\"113.8547\" y=\"-267.3116\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p5_1 -->\n<g id=\"node32\" class=\"node\">\n<title>p5_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"30.5473\" cy=\"-145.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"30.5473\" y=\"-141.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p5_1</text>\n</g>\n<!-- p5_1&#45;&gt;NF03 -->\n<g id=\"edge15\" class=\"edge\">\n<title>p5_1&#45;&gt;NF03</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M40.8007,-174.7389C42.5843,-179.8171 44.4747,-185.1989 46.3619,-190.5721\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"43.1158,-191.8918 49.7321,-200.1669 49.7203,-189.572 43.1158,-191.8918\"/>\n<text text-anchor=\"middle\" x=\"25.0813\" y=\"-186.4555\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p6_0 -->\n<g id=\"node33\" class=\"node\">\n<title>p6_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"357.2363\" cy=\"-241.7509\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"357.2363\" y=\"-238.0509\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p6_0</text>\n</g>\n<!-- p6_0&#45;&gt;NF04 -->\n<g id=\"edge16\" class=\"edge\">\n<title>p6_0&#45;&gt;NF04</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M338.4495,-217.1761C335.1814,-212.9011 331.7178,-208.3704 328.2599,-203.8471\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"330.9389,-201.5886 322.085,-195.7698 325.3778,-205.8399 330.9389,-201.5886\"/>\n<text text-anchor=\"middle\" x=\"351.8547\" y=\"-199.3116\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p6_1 -->\n<g id=\"node34\" class=\"node\">\n<title>p6_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"268.5473\" cy=\"-77.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"268.5473\" y=\"-73.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p6_1</text>\n</g>\n<!-- p6_1&#45;&gt;NF04 -->\n<g id=\"edge17\" class=\"edge\">\n<title>p6_1&#45;&gt;NF04</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M278.8007,-106.7389C280.5843,-111.8171 282.4747,-117.1989 284.3619,-122.5721\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.1158,-123.8918 287.7321,-132.1669 287.7203,-121.572 281.1158,-123.8918\"/>\n<text text-anchor=\"middle\" x=\"263.0813\" y=\"-118.4555\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p7_0 -->\n<g id=\"node35\" class=\"node\">\n<title>p7_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"663.2363\" cy=\"-785.7509\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"663.2363\" y=\"-782.0509\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p7_0</text>\n</g>\n<!-- p7_0&#45;&gt;NF05 -->\n<g id=\"edge18\" class=\"edge\">\n<title>p7_0&#45;&gt;NF05</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M644.4495,-761.1761C641.1814,-756.9011 637.7178,-752.3704 634.2599,-747.8471\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"636.9389,-745.5886 628.085,-739.7698 631.3778,-749.8399 636.9389,-745.5886\"/>\n<text text-anchor=\"middle\" x=\"657.8547\" y=\"-743.3116\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p7_1 -->\n<g id=\"node36\" class=\"node\">\n<title>p7_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"574.5473\" cy=\"-621.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"574.5473\" y=\"-617.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p7_1</text>\n</g>\n<!-- p7_1&#45;&gt;NF05 -->\n<g id=\"edge19\" class=\"edge\">\n<title>p7_1&#45;&gt;NF05</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M584.8007,-650.7389C586.5843,-655.8171 588.4747,-661.1989 590.3619,-666.5721\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"587.1158,-667.8918 593.7321,-676.1669 593.7203,-665.572 587.1158,-667.8918\"/>\n<text text-anchor=\"middle\" x=\"569.0813\" y=\"-662.4555\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p8_0 -->\n<g id=\"node37\" class=\"node\">\n<title>p8_0</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"338.3879\" cy=\"-335.0014\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"338.3879\" y=\"-331.3014\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p8_0</text>\n</g>\n<!-- p8_0&#45;&gt;NF06 -->\n<g id=\"edge20\" class=\"edge\">\n<title>p8_0&#45;&gt;NF06</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M308.0887,-330.2987C302.318,-329.403 296.156,-328.4467 290.0143,-327.4934\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"290.1885,-323.9786 279.77,-325.9034 289.1148,-330.8958 290.1885,-323.9786\"/>\n<text text-anchor=\"middle\" x=\"299.0515\" y=\"-317.6961\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p8_1 -->\n<g id=\"node38\" class=\"node\">\n<title>p8_1</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"209.2014\" cy=\"-230.5473\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"209.2014\" y=\"-226.8473\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p8_1</text>\n</g>\n<!-- p8_1&#45;&gt;NF06 -->\n<g id=\"edge21\" class=\"edge\">\n<title>p8_1&#45;&gt;NF06</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M220.2272,-259.4289C222.3069,-264.8765 224.5246,-270.6857 226.7333,-276.4712\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"223.5794,-278.0236 230.4158,-286.1176 230.1191,-275.527 223.5794,-278.0236\"/>\n<text text-anchor=\"middle\" x=\"204.9802\" y=\"-271.7501\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n<!-- p8_2 -->\n<g id=\"node39\" class=\"node\">\n<title>p8_2</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"183.5473\" cy=\"-395.3185\" rx=\"30.5947\" ry=\"30.5947\"/>\n<text text-anchor=\"middle\" x=\"183.5473\" y=\"-391.6185\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">p8_2</text>\n</g>\n<!-- p8_2&#45;&gt;NF06 -->\n<g id=\"edge22\" class=\"edge\">\n<title>p8_2&#45;&gt;NF06</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M202.8314,-371.1585C206.4687,-366.6015 210.3475,-361.742 214.2105,-356.9022\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.1485,-358.8319 220.6514,-348.8328 211.6776,-354.4651 217.1485,-358.8319\"/>\n<text text-anchor=\"middle\" x=\"190.021\" y=\"-352.8304\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">partOf</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkVFVHBvQnLU",
        "cellView": "form",
        "outputId": "79c95a7b-2d61-40f4-df27-e801ba9d15e2"
      },
      "source": [
        "# %%writefile evaluasi.py\n",
        "#@title evaluasi { vertical-output: true }\n",
        "data_param = \"/content/drive/MyDrive/dataset/partOfAll/partOfGrd.xlsx\" #@param {type:\"string\"}\n",
        "mode_param = \"describe\" #@param [\"main\", \"describe\", \"test\", \"grd\"]\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
        "from tabulate import tabulate\n",
        "\n",
        "class evaluasiData:\n",
        "    def __init__(self, inputFile):\n",
        "        self.data = inputFile\n",
        "\n",
        "    def fulldataset(self, inputSRS): # function membuat dataset\n",
        "        xl = pd.ExcelFile(self.data)\n",
        "        dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}[inputSRS]\n",
        "        return dfs\n",
        "\n",
        "    def preprocessing(self): # function melihat struktur dataset di excel\n",
        "        xl = pd.ExcelFile(self.data)\n",
        "        for sh in xl.sheet_names:\n",
        "          df = xl.parse(sh)\n",
        "          print('Processing: [{}] ...'.format(sh))\n",
        "          print(df.head())\n",
        "\n",
        "    def evaluation(self, data1, data2):\n",
        "        hasil_evaluasi = []\n",
        "        for idx, idy in zip(data1.drop('Dataset', axis= 1).values, data2.drop('Dataset', axis= 1).values):\n",
        "            y_actual = idx.astype(int) #define array of actual values\n",
        "            y_predicted = idy.astype(int) #define array of predicted values\n",
        "            nilai_akurasi = accuracy_score(y_actual, y_predicted, normalize=True)\n",
        "            nilai_recall = recall_score(y_actual, y_predicted, average= 'macro', zero_division= 1)\n",
        "            nilai_presisi = precision_score(y_actual, y_predicted, average= 'macro', zero_division= 1)\n",
        "            hasil_evaluasi.append([nilai_akurasi, nilai_recall, nilai_presisi])\n",
        "        evaluasi_df = pd.DataFrame(hasil_evaluasi, columns= ['akurasi', 'recall', 'presisi'], index= test_df.Dataset)\n",
        "        return evaluasi_df \n",
        "\n",
        "    def main(self, data1, data2, output= ['main', 'test', 'grd', 'describe']):\n",
        "      if 'main' in output:\n",
        "        eval = evaluasiData.evaluation(self, data1, data2).sort_values(by=['akurasi'], ascending= False).reset_index()\n",
        "        print(tabulate(eval, headers = 'keys', tablefmt = 'psql'))\n",
        "      elif 'describe' in output:\n",
        "        eval = evaluasiData.evaluation(self, data1, data2).sort_values(by=['akurasi'], ascending= False).reset_index().describe()\n",
        "        print(tabulate(eval, headers = 'keys', tablefmt = 'psql'))\n",
        "      elif 'test' in output:\n",
        "        print(tabulate(test_df, headers = 'keys', tablefmt = 'psql'))\n",
        "      elif 'grd' in output:\n",
        "        print(tabulate(grd_df, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "    evalData = evaluasiData(data_param)\n",
        "    test_df = evalData.fulldataset('test')\n",
        "    grd_df = evalData.fulldataset('groundtruth')\n",
        "    evalData.main(test_df, grd_df, mode_param)\n",
        "\n",
        "# d = {\n",
        "#     'akurasi': [0.8, 0.8, 1, 1, 1, 1, 0.4, 0.4, 1, 0.8, 0.8, 1], \n",
        "#      'recall': [0.833, 0.833, 1, 1, 1, 1,  0.625, 0.625, 1, 0.833, 0.833, 1],\n",
        "#      'presisi': [0.833, 0.833, 1, 1, 1, 1,  0.625, 0.625, 1, 0.833, 0.833, 1]}\n",
        "# df = pd.DataFrame(d, index= ['Grid-3D', 'colorcast', 'triangle', 'agentmom', 'philip', \n",
        "#                   'pugetSound', 'peering', 'gammaj', 'inventory', 'esa', 'spaceFraction', 'tachonet'])\n",
        "# df.describe()\n",
        "\n",
        "  except OSError as err:\n",
        "    print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-----------+-----------+\n",
            "|       |   akurasi |    recall |   presisi |\n",
            "|-------+-----------+-----------+-----------|\n",
            "| count | 62        | 62        | 62        |\n",
            "| mean  |  0.409677 |  0.615911 |  0.671032 |\n",
            "| std   |  0.254603 |  0.162266 |  0.134472 |\n",
            "| min   |  0.2      |  0.333333 |  0.5      |\n",
            "| 25%   |  0.2      |  0.555556 |  0.564732 |\n",
            "| 50%   |  0.4      |  0.571429 |  0.625    |\n",
            "| 75%   |  0.4      |  0.625    |  0.714286 |\n",
            "| max   |  1        |  1        |  1        |\n",
            "+-------+-----------+-----------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0vJktftlOZ"
      },
      "source": [
        "## Another Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rxVQ_YJj8DT",
        "cellView": "form",
        "outputId": "ba921844-ad3b-440b-ccd1-16af53ac9d07"
      },
      "source": [
        "%%writefile stanfordSent.py\n",
        "\"\"\"\n",
        "Created on Th Oktober  7 18:07:24 2021\n",
        "@author: Rakha asyrofi\n",
        "stanford clause\n",
        "\"\"\"\n",
        "\n",
        "#@title Modul2a Stanford Clause { vertical-output: true }\n",
        "url_param = \"http://corenlp.run\" #@param {type:\"string\"}\n",
        "model_param = \"/content/drive/MyDrive/stanford-corenlp-4.0.0\" #@param {type:\"string\"}\n",
        "dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "srs_param = \"2005 - Grid 3D\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"]\n",
        "col_param = \"Requirement Statement\" #@param [\"Requirement Statement\", \"req\"]\n",
        "\n",
        "import re, nltk, json, pandas as pd\n",
        "# from pycorenlp import StanfordCoreNLP\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from tabulate import tabulate\n",
        "\n",
        "class stanford_clause:\n",
        "    def __init__(self, fileName= dataFile, url_stanford= url_param, \n",
        "                 model_stanford = model_param):\n",
        "        # self.nlp = StanfordCoreNLP(url_stanford)\n",
        "        self.nlp = StanfordCoreNLP(url_stanford, port= 80)\n",
        "        # self.nlp = StanfordCoreNLP(model_stanford)\n",
        "        self.__data = fileName\n",
        "\n",
        "    def fulldataset(self, inputSRS= col_param):\n",
        "        xl = pd.ExcelFile(self.__data)\n",
        "        dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}[inputSRS]\n",
        "        return dfs\n",
        "\n",
        "    def preprocessing(self):\n",
        "        xl = pd.ExcelFile(self.__data)\n",
        "        for sh in xl.sheet_names:\n",
        "          df = xl.parse(sh)\n",
        "          print('Processing: [{}] ...'.format(sh))\n",
        "          print(df.head())\n",
        "\n",
        "    def get_verb_phrases(self, t):\n",
        "        verb_phrases = []\n",
        "        num_children = len(t)\n",
        "        num_VP = sum(1 if t[i].label() == \"VP\" else 0 for i in range(0, num_children))\n",
        "\n",
        "        if t.label() != \"VP\":\n",
        "            for i in range(0, num_children):\n",
        "                if t[i].height() > 2:\n",
        "                    verb_phrases.extend(stanford_clause.get_verb_phrases(self, t[i]))\n",
        "        elif t.label() == \"VP\" and num_VP > 1:\n",
        "            for i in range(0, num_children):\n",
        "                if t[i].label() == \"VP\":\n",
        "                    if t[i].height() > 2:\n",
        "                        verb_phrases.extend(stanford_clause.get_verb_phrases(self, t[i]))\n",
        "        else:\n",
        "            verb_phrases.append(' '.join(t.leaves()))\n",
        "\n",
        "        return verb_phrases\n",
        "\n",
        "    def get_pos(self, t):\n",
        "        vp_pos = []\n",
        "        sub_conj_pos = []\n",
        "        num_children = len(t)\n",
        "        children = [t[i].label() for i in range(0,num_children)]\n",
        "\n",
        "        flag = re.search(r\"(S|SBAR|SBARQ|SINV|SQ)\", ' '.join(children))\n",
        "        if \"VP\" in children and not flag:\n",
        "            # print(t[i].label())\n",
        "            for i in range(0, num_children):\n",
        "                if t[i].label() == \"VP\":\n",
        "                    vp_pos.append(t[i].treeposition())\n",
        "        elif not \"VP\" in children and not flag:\n",
        "            for i in range(0, num_children):\n",
        "                if t[i].height() > 2:\n",
        "                    temp1,temp2 = stanford_clause.get_pos(self, t[i])\n",
        "                    vp_pos.extend(temp1)\n",
        "                    sub_conj_pos.extend(temp2)\n",
        "        else:\n",
        "            for i in range(0, num_children):\n",
        "                if t[i].label() in [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]:\n",
        "                    temp1, temp2 = stanford_clause.get_pos(self, t[i])\n",
        "                    vp_pos.extend(temp1)\n",
        "                    sub_conj_pos.extend(temp2)\n",
        "                else:\n",
        "                    sub_conj_pos.append(t[i].treeposition())\n",
        "\n",
        "        return (vp_pos,sub_conj_pos)\n",
        "\n",
        "\n",
        "    def get_clause_list(self, sent):\n",
        "        parser = self.nlp.annotate(sent, properties={\"annotators\":\"parse\",\"outputFormat\": \"json\"})\n",
        "        # sent_tree = nltk.tree.ParentedTree.fromstring(parser[\"sentences\"][0][\"parse\"])\n",
        "        parser_json = json.loads(parser)\n",
        "        sent_tree = nltk.tree.ParentedTree.fromstring(parser_json[\"sentences\"][0][\"parse\"])\n",
        "        clause_level_list = [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]\n",
        "        clause_list = []\n",
        "        sub_trees = []\n",
        "\n",
        "        # break the tree into subtrees of clauses using\n",
        "        # clause levels \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"\n",
        "        for sub_tree in reversed(list(sent_tree.subtrees())):\n",
        "            # print(sub_tree.label() == 'CC')\n",
        "            if sub_tree.label() in clause_level_list:\n",
        "                if sub_tree.parent().label() in clause_level_list:\n",
        "                    continue\n",
        "                if (len(sub_tree) == 1 and sub_tree.label() == \"S\" and sub_tree[0].label() == \"VP\"\n",
        "                    and not sub_tree.parent().label() in clause_level_list):\n",
        "                    continue\n",
        "                sub_trees.append(sub_tree)\n",
        "                del sent_tree[sub_tree.treeposition()]\n",
        "\n",
        "        \n",
        "        for t in sub_trees: # for each clause level subtree, extract relevant simple sentence\n",
        "            verb_phrases = stanford_clause.get_verb_phrases(self, t) # get verb phrases from the new modified tree\n",
        "            vp_pos,sub_conj_pos = stanford_clause.get_pos(self, t)\n",
        "            for i in vp_pos:\n",
        "                del t[i]\n",
        "            for i in sub_conj_pos:\n",
        "                del t[i]\n",
        "            subject_phrase = ' '.join(t.leaves())\n",
        "            for i in verb_phrases: # update the clause_list\n",
        "                clause_list.append(subject_phrase + \" \" + i)\n",
        "        return clause_list\n",
        "\n",
        "    def __del__(self):\n",
        "      print(\"descructed\")\n",
        "\n",
        "    def main(self):\n",
        "        id_req = stanford_clause.fulldataset(self, srs_param)['ID']\n",
        "        req = stanford_clause.fulldataset(self, srs_param)[col_param]\n",
        "\n",
        "        data_clausa = []\n",
        "        for id, num in zip(id_req, req):\n",
        "            sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\",\" \",num)\n",
        "            clause_list = [idx for idx in stanford_clause.get_clause_list(self, sent)]\n",
        "            jml_clausa = len(clause_list)\n",
        "            data_clausa.append([id, num, clause_list, jml_clausa])\n",
        "\n",
        "        clausa_df = pd.DataFrame(data_clausa, columns= ['id', 'req', 'hasil', 'jml_clausa'])\n",
        "        print(tabulate(clausa_df, headers = 'keys', tablefmt = 'psql'))\n",
        "        stanford_clause.__del__(self)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "    stanford_clause().main()\n",
        "    # sent =  'Joe waited for the train, but the train was late.'\n",
        "    # sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\",\" \",sent)\n",
        "    # clause_list = stanford_clause().get_clause_list(sent)\n",
        "    # print(clause_list)\n",
        "\n",
        "  except OSError as err:\n",
        "    print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting stanfordSent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMaux6b5O9Qx",
        "cellView": "form",
        "outputId": "d3d63e24-71fe-4a57-ddf0-df78cf7c2807"
      },
      "source": [
        "%%writefile clausySent.py\n",
        "#@title spacy_sent { vertical-output: true }\n",
        "spacy_param = \"en_core_web_sm\" #@param {type:\"string\"}\n",
        "dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "srs_param = \"2005 - Grid 3D\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"]\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Th Oktober  7 18:07:24 2021\n",
        "@author: Rakha asyrofi\n",
        "spacy sent\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "import lemminflect\n",
        "import logging\n",
        "import typing\n",
        "\n",
        "from spacy.tokens import Span, Doc\n",
        "from spacy.matcher import Matcher\n",
        "from lemminflect import getInflection\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# DO NOT SET MANUALLY\n",
        "MOD_CONSERVATIVE = False\n",
        "\n",
        "Doc.set_extension(\"clauses\", default=[], force=True)\n",
        "Span.set_extension(\"clauses\", default=[], force=True)\n",
        "\n",
        "dictionary = {\n",
        "    \"non_ext_copular\": \"\"\"die walk\"\"\".split(),\n",
        "    \"ext_copular\": \"\"\"act\n",
        "appear\n",
        "be\n",
        "become\n",
        "come\n",
        "come out\n",
        "end up\n",
        "get\n",
        "go\n",
        "grow\n",
        "fall\n",
        "feel\n",
        "keep\n",
        "leave\n",
        "look\n",
        "prove\n",
        "remain\n",
        "seem\n",
        "smell\n",
        "sound\n",
        "stay\n",
        "taste\n",
        "turn\n",
        "turn up\n",
        "wind up\n",
        "live\n",
        "come\n",
        "go\n",
        "stand\n",
        "lie\n",
        "love\n",
        "do\n",
        "try\"\"\".split(),\n",
        "    \"complex_transitive\": \"\"\"\n",
        "bring\n",
        "catch\n",
        "drive\n",
        "get\n",
        "keep\n",
        "lay\n",
        "lead\n",
        "place\n",
        "put\n",
        "set\n",
        "sit\n",
        "show\n",
        "stand\n",
        "slip\n",
        "take\"\"\".split(),\n",
        "    \"adverbs_ignore\": \"\"\"so\n",
        "then\n",
        "thus\n",
        "why\n",
        "as\n",
        "even\"\"\".split(),\n",
        "    \"adverbs_include\": \"\"\"\n",
        "hardly\n",
        "barely\n",
        "scarcely\n",
        "seldom\n",
        "rarely\"\"\".split(),\n",
        "}\n",
        "\n",
        "\n",
        "class Clause:\n",
        "    def __init__(\n",
        "        self,\n",
        "        subject: typing.Optional[Span] = None,\n",
        "        verb: typing.Optional[Span] = None,\n",
        "        indirect_object: typing.Optional[Span] = None,\n",
        "        direct_object: typing.Optional[Span] = None,\n",
        "        complement: typing.Optional[Span] = None,\n",
        "        adverbials: typing.List[Span] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        subject : Span\n",
        "            Subject.\n",
        "        verb : Span\n",
        "            Verb.\n",
        "        indirect_object : Span, optional\n",
        "            Indirect object, The default is None.\n",
        "        direct_object : Span, optional\n",
        "            Direct object. The default is None.\n",
        "        complement : Span, optional\n",
        "            Complement. The default is None.\n",
        "        adverbials : list, optional\n",
        "            List of adverbials. The default is [].\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "\n",
        "        \"\"\"\n",
        "        if adverbials is None:\n",
        "            adverbials = []\n",
        "\n",
        "        self.subject = subject\n",
        "        self.verb = verb\n",
        "        self.indirect_object = indirect_object\n",
        "        self.direct_object = direct_object\n",
        "        self.complement = complement\n",
        "        self.adverbials = adverbials\n",
        "\n",
        "        self.doc = self.subject.doc\n",
        "\n",
        "        self.type = self._get_clause_type()\n",
        "\n",
        "    def _get_clause_type(self):\n",
        "        has_verb = self.verb is not None\n",
        "        has_complement = self.complement is not None\n",
        "        has_adverbial = len(self.adverbials) > 0\n",
        "        has_ext_copular_verb = (\n",
        "            has_verb and self.verb.root.lemma_ in dictionary[\"ext_copular\"]\n",
        "        )\n",
        "        has_non_ext_copular_verb = (\n",
        "            has_verb and self.verb.root.lemma_ in dictionary[\"non_ext_copular\"]\n",
        "        )\n",
        "        conservative = MOD_CONSERVATIVE\n",
        "        has_direct_object = self.direct_object is not None\n",
        "        has_indirect_object = self.indirect_object is not None\n",
        "        has_object = has_direct_object or has_indirect_object\n",
        "        complex_transitive = (\n",
        "            has_verb and self.verb.root.lemma_ in dictionary[\"complex_transitive\"]\n",
        "        )\n",
        "\n",
        "        clause_type = \"undefined\"\n",
        "\n",
        "        if not has_verb:\n",
        "            clause_type = \"SVC\"\n",
        "            return clause_type\n",
        "\n",
        "        if has_object:\n",
        "            if has_direct_object and has_indirect_object:\n",
        "                clause_type = \"SVOO\"\n",
        "            elif has_complement:\n",
        "                clause_type = \"SVOC\"\n",
        "            elif not has_adverbial or not has_direct_object:\n",
        "                clause_type = \"SVO\"\n",
        "            elif complex_transitive or conservative:\n",
        "                clause_type = \"SVOA\"\n",
        "            else:\n",
        "                clause_type = \"SVO\"\n",
        "        else:\n",
        "            if has_complement:\n",
        "                clause_type = \"SVC\"\n",
        "            elif not has_adverbial or has_non_ext_copular_verb:\n",
        "                clause_type = \"SV\"\n",
        "            elif has_ext_copular_verb or conservative:\n",
        "                clause_type = \"SVA\"\n",
        "            else:\n",
        "                clause_type = \"SV\"\n",
        "\n",
        "        return clause_type\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"<{}, {}, {}, {}, {}, {}, {}>\".format(\n",
        "            self.type,\n",
        "            self.subject,\n",
        "            self.verb,\n",
        "            self.indirect_object,\n",
        "            self.direct_object,\n",
        "            self.complement,\n",
        "            self.adverbials,\n",
        "        )\n",
        "\n",
        "    def to_propositions(\n",
        "        self, as_text: bool = False, inflect: str or None = \"VBD\", capitalize: bool = False\n",
        "    ):\n",
        "\n",
        "        if inflect and not as_text:\n",
        "            logging.warning(\"`inflect' argument is ignored when `as_text==False'. To suppress this warning call `to_propositions' with the argument `inflect=None'\")\n",
        "        if capitalize and not as_text:\n",
        "            logging.warning(\"`capitalize' argument is ignored when `as_text==False'. To suppress this warning call `to_propositions' with the argument `capitalize=False\")\n",
        "\n",
        "        propositions = []\n",
        "\n",
        "        subjects = extract_ccs_from_token_at_root(self.subject)\n",
        "        direct_objects = extract_ccs_from_token_at_root(self.direct_object)\n",
        "        indirect_objects = extract_ccs_from_token_at_root(self.indirect_object)\n",
        "        complements = extract_ccs_from_token_at_root(self.complement)\n",
        "        verbs = [self.verb] if self.verb else []\n",
        "\n",
        "        for subj in subjects:\n",
        "            if complements and not verbs:\n",
        "                for c in complements:\n",
        "                    propositions.append((subj, \"is\", c))\n",
        "                propositions.append((subj, \"is\") + tuple(complements))\n",
        "\n",
        "            for verb in verbs:\n",
        "                prop = [subj, verb]\n",
        "                if self.type in [\"SV\", \"SVA\"]:\n",
        "                    if self.adverbials:\n",
        "                        for a in self.adverbials:\n",
        "                            propositions.append(tuple(prop + [a]))\n",
        "                        propositions.append(tuple(prop + self.adverbials))\n",
        "                    else:\n",
        "                        propositions.append(tuple(prop))\n",
        "\n",
        "                elif self.type == \"SVOO\":\n",
        "                    for iobj in indirect_objects:\n",
        "                        for dobj in direct_objects:\n",
        "                            propositions.append((subj, verb, iobj, dobj))\n",
        "                elif self.type == \"SVO\":\n",
        "                    for obj in direct_objects + indirect_objects:\n",
        "                        propositions.append((subj, verb, obj))\n",
        "                        for a in self.adverbials:\n",
        "                            propositions.append((subj, verb, obj, a))\n",
        "                elif self.type == \"SVOA\":\n",
        "                    for obj in direct_objects:\n",
        "                        if self.adverbials:\n",
        "                            for a in self.adverbials:\n",
        "                                propositions.append(tuple(prop + [obj, a]))\n",
        "                            propositions.append(tuple(prop + [obj] + self.adverbials))\n",
        "\n",
        "                elif self.type == \"SVOC\":\n",
        "                    for obj in indirect_objects + direct_objects:\n",
        "                        if complements:\n",
        "                            for c in complements:\n",
        "                                propositions.append(tuple(prop + [obj, c]))\n",
        "                            propositions.append(tuple(prop + [obj] + complements))\n",
        "                elif self.type == \"SVC\":\n",
        "                    if complements:\n",
        "                        for c in complements:\n",
        "                            propositions.append(tuple(prop + [c]))\n",
        "                        propositions.append(tuple(prop + complements))\n",
        "\n",
        "        # Remove doubles\n",
        "        propositions = list(set(propositions))\n",
        "\n",
        "        if as_text:\n",
        "            return _convert_clauses_to_text(\n",
        "                propositions, inflect=inflect, capitalize=capitalize\n",
        "            )\n",
        "\n",
        "        return propositions\n",
        "\n",
        "\n",
        "def inflect_token(token, inflect):\n",
        "    if (\n",
        "        inflect\n",
        "        and token.pos_ == \"VERB\"\n",
        "        and \"AUX\" not in [tt.pos_ for tt in token.lefts]\n",
        "        # t is not preceded by an auxiliary verb (e.g. `the birds were ailing`)\n",
        "        and token.dep_ != \"pcomp\"\n",
        "    ):  # t `dreamed of becoming a dancer`\n",
        "        return str(token._.inflect(inflect))\n",
        "    else:\n",
        "        return str(token)\n",
        "\n",
        "\n",
        "def _convert_clauses_to_text(propositions, inflect, capitalize):\n",
        "    proposition_texts = []\n",
        "    for proposition in propositions:\n",
        "        span_texts = []\n",
        "        for span in proposition:\n",
        "\n",
        "            token_texts = []\n",
        "            for token in span:\n",
        "                token_texts.append(inflect_token(token, inflect))\n",
        "\n",
        "            span_texts.append(\" \".join(token_texts))\n",
        "        proposition_texts.append(\" \".join(span_texts))\n",
        "\n",
        "    if capitalize:  # Capitalize and add a full stop.\n",
        "        proposition_texts = [text.capitalize() + \".\" for text in proposition_texts]\n",
        "\n",
        "    return proposition_texts\n",
        "\n",
        "\n",
        "def _get_verb_matches(span):\n",
        "    # 1. Find verb phrases in the span\n",
        "    # (see mdmjsh answer here: https://stackoverflow.com/questions/47856247/extract-verb-phrases-using-spacy)\n",
        "\n",
        "    verb_matcher = Matcher(span.vocab)\n",
        "    verb_matcher.add(\n",
        "        \"Auxiliary verb phrase aux-verb\", None, [{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"}]\n",
        "    )\n",
        "    verb_matcher.add(\"Auxiliary verb phrase\", None, [{\"POS\": \"AUX\"}])\n",
        "    verb_matcher.add(\"Verb phrase\", None, [{\"POS\": \"VERB\"}])\n",
        "\n",
        "    return verb_matcher(span)\n",
        "\n",
        "\n",
        "def _get_verb_chunks(span):\n",
        "    matches = _get_verb_matches(span)\n",
        "\n",
        "    # Filter matches (e.g. do not have both \"has won\" and \"won\" in verbs)\n",
        "    verb_chunks = []\n",
        "    for match in [span[start:end] for _, start, end in matches]:\n",
        "        if match.root not in [vp.root for vp in verb_chunks]:\n",
        "            verb_chunks.append(match)\n",
        "    return verb_chunks\n",
        "\n",
        "\n",
        "def _get_subject(verb):\n",
        "    for c in verb.root.children:\n",
        "        if c.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
        "            subject = extract_span_from_entity(c)\n",
        "            return subject\n",
        "\n",
        "    root = verb.root\n",
        "    while root.dep_ in [\"conj\", \"cc\", \"advcl\", \"acl\", \"ccomp\", \"ROOT\"]:\n",
        "        for c in root.children:\n",
        "            if c.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
        "                subject = extract_span_from_entity(c)\n",
        "                return subject\n",
        "\n",
        "            if c.dep_ in [\"acl\", \"advcl\"]:\n",
        "                subject = find_verb_subject(c)\n",
        "                return extract_span_from_entity(subject) if subject else None\n",
        "\n",
        "        # Break cycles\n",
        "        if root == verb.root.head:\n",
        "            break\n",
        "        else:\n",
        "            root = verb.root.head\n",
        "\n",
        "    for c in root.children:\n",
        "        if c.dep_ in [\"nsubj\", \"nsubj:pass\", \"nsubjpass\"]:\n",
        "            subject = extract_span_from_entity(c)\n",
        "            return subject\n",
        "    return None\n",
        "\n",
        "\n",
        "def _find_matching_child(root, allowed_types):\n",
        "    for c in root.children:\n",
        "        if c.dep_ in allowed_types:\n",
        "            return extract_span_from_entity(c)\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_clauses(span):\n",
        "    clauses = []\n",
        "\n",
        "    verb_chunks = _get_verb_chunks(span)\n",
        "    for verb in verb_chunks:\n",
        "\n",
        "        subject = _get_subject(verb)\n",
        "        if not subject:\n",
        "            continue\n",
        "\n",
        "        # Check if there are phrases of the form, \"AE, a scientist of ...\"\n",
        "        # If so, add a new clause of the form:\n",
        "        # <AE, is, a scientist>\n",
        "        for c in subject.root.children:\n",
        "            if c.dep_ == \"appos\":\n",
        "                complement = extract_span_from_entity(c)\n",
        "                clause = Clause(subject=subject, complement=complement)\n",
        "                clauses.append(clause)\n",
        "\n",
        "        indirect_object = _find_matching_child(verb.root, [\"dative\"])\n",
        "        direct_object = _find_matching_child(verb.root, [\"dobj\"])\n",
        "        complement = _find_matching_child(\n",
        "            verb.root, [\"ccomp\", \"acomp\", \"xcomp\", \"attr\"]\n",
        "        )\n",
        "        adverbials = [\n",
        "            extract_span_from_entity(c)\n",
        "            for c in verb.root.children\n",
        "            if c.dep_ in (\"prep\", \"advmod\", \"agent\")\n",
        "        ]\n",
        "\n",
        "        clause = Clause(\n",
        "            subject=subject,\n",
        "            verb=verb,\n",
        "            indirect_object=indirect_object,\n",
        "            direct_object=direct_object,\n",
        "            complement=complement,\n",
        "            adverbials=adverbials,\n",
        "        )\n",
        "        clauses.append(clause)\n",
        "    return clauses\n",
        "\n",
        "\n",
        "def extract_clauses_doc(doc):\n",
        "    for sent in doc.sents:\n",
        "        clauses = extract_clauses(sent)\n",
        "        sent._.clauses = clauses\n",
        "        doc._.clauses += clauses\n",
        "    return doc\n",
        "\n",
        "\n",
        "def add_to_pipe(nlp):\n",
        "    nlp.add_pipe(extract_clauses_doc)\n",
        "\n",
        "\n",
        "def extract_span_from_entity(token):\n",
        "    ent_subtree = sorted([c for c in token.subtree], key=lambda x: x.i)\n",
        "    return Span(token.doc, start=ent_subtree[0].i, end=ent_subtree[-1].i + 1)\n",
        "\n",
        "\n",
        "def extract_span_from_entity_no_cc(token):\n",
        "    ent_subtree = sorted(\n",
        "        [token] + [c for c in token.children if c.dep_ not in [\"cc\", \"conj\", \"prep\"]],\n",
        "        key=lambda x: x.i,\n",
        "    )\n",
        "    return Span(token.doc, start=ent_subtree[0].i, end=ent_subtree[-1].i + 1)\n",
        "\n",
        "\n",
        "def extract_ccs_from_entity(token):\n",
        "    entities = [extract_span_from_entity_no_cc(token)]\n",
        "    for c in token.children:\n",
        "        if c.dep_ in [\"conj\", \"cc\"]:\n",
        "            entities += extract_ccs_from_entity(c)\n",
        "    return entities\n",
        "\n",
        "\n",
        "def extract_ccs_from_token_at_root(span):\n",
        "    if span is None:\n",
        "        return []\n",
        "    else:\n",
        "        return extract_ccs_from_token(span.root)\n",
        "\n",
        "\n",
        "def extract_ccs_from_token(token):\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]:\n",
        "        children = sorted(\n",
        "            [token]\n",
        "            + [\n",
        "                c\n",
        "                for c in token.children\n",
        "                if c.dep_ in [\"advmod\", \"amod\", \"det\", \"poss\", \"compound\"]\n",
        "            ],\n",
        "            key=lambda x: x.i,\n",
        "        )\n",
        "        entities = [Span(token.doc, start=children[0].i, end=children[-1].i + 1)]\n",
        "    else:\n",
        "        entities = [Span(token.doc, start=token.i, end=token.i + 1)]\n",
        "    for c in token.children:\n",
        "        if c.dep_ == \"conj\":\n",
        "            entities += extract_ccs_from_token(c)\n",
        "    return entities\n",
        "\n",
        "\n",
        "def find_verb_subject(v):\n",
        "    \"\"\"\n",
        "    Returns the nsubj, nsubjpass of the verb. If it does not exist and the root is a head,\n",
        "    find the subject of that verb instead.\n",
        "    \"\"\"\n",
        "    if v.dep_ in [\"nsubj\", \"nsubjpass\", \"nsubj:pass\"]:\n",
        "        return v\n",
        "    # guard against infinite recursion on root token\n",
        "    elif v.dep_ in [\"advcl\", \"acl\"] and v.head.dep_ != \"ROOT\":\n",
        "        return find_verb_subject(v.head)\n",
        "\n",
        "    for c in v.children:\n",
        "        if c.dep_ in [\"nsubj\", \"nsubjpass\", \"nsubj:pass\"]:\n",
        "            return c\n",
        "        elif c.dep_ in [\"advcl\", \"acl\"] and v.head.dep_ != \"ROOT\":\n",
        "            return find_verb_subject(v.head)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import spacy\n",
        "    from spacySent import spacyClause\n",
        "    nlp = spacy.load(spacy_param)\n",
        "    add_to_pipe(nlp)\n",
        "    data_c = []\n",
        "    for id, num in zip(spacyClause(dataFile).fulldataset(srs_param)['ID'],\n",
        "                       spacyClause(dataFile).fulldataset(srs_param)['Requirement Statement']):\n",
        "        doc = nlp(num)\n",
        "        sent_c = [clause.to_propositions(as_text=True, capitalize=True) for clause in doc._.clauses]\n",
        "        jml_clausa = len(sent_c)\n",
        "        data_c.append([id, num,  sent_c, jml_clausa])\n",
        "    clausy_df = pd.DataFrame(data_c, columns = ['ID','req', 'clause', 'jml_klausa'])\n",
        "    print(tabulate(clausy_df, headers = 'keys', tablefmt = 'psql'))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting clausySent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X81Noo_j20y",
        "cellView": "form",
        "outputId": "144143c2-8a12-4426-a69d-1f5dac86ee74"
      },
      "source": [
        "%%writefile spacySent.py\n",
        "\"\"\"\n",
        "Created on Th Oktober  7 18:07:24 2021\n",
        "@author: Rakha asyrofi\n",
        "spacy parameter\n",
        "\"\"\"\n",
        "\n",
        "#@title Modul2b Spacy parameter { vertical-output: true }\n",
        "spacy_param = 'en_core_web_sm' #@param {type:\"string\"}\n",
        "dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "srs_param = \"2005 - Grid 3D\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"]\n",
        "\n",
        "# dataFile = \"/content/drive/MyDrive/dataset/visualPartOf/partOf2005 - Grid 3D.xlsx\" #@param {type:\"string\"}\n",
        "# srs_param = \"tabel_partOf\" #@param {type:\"string\"}\n",
        "col_param = \"Requirement Statement\" #@param [\"Requirement Statement\", \"req\"]\n",
        "\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "class spacyClause:\n",
        "  def __init__(self, fileName= dataFile):\n",
        "      self.__data = fileName\n",
        "\n",
        "  def fulldataset(self, inputSRS= col_param):\n",
        "      xl = pd.ExcelFile(self.__data)\n",
        "      dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}[inputSRS]\n",
        "      return dfs\n",
        "\n",
        "  def preprocessing(self):\n",
        "      xl = pd.ExcelFile(self.__data)\n",
        "      for sh in xl.sheet_names:\n",
        "        df = xl.parse(sh)\n",
        "        print('Processing: [{}] ...'.format(sh))\n",
        "        print(df.head())\n",
        "\n",
        "  def find_root_of_sentence(self, doc):\n",
        "      root_token = None\n",
        "      for token in doc:\n",
        "          if (token.dep_ == \"ROOT\"):\n",
        "              root_token = token\n",
        "      return root_token\n",
        "\n",
        "  def find_other_verbs(self, doc, root_token):\n",
        "      other_verbs = []\n",
        "      for token in doc:\n",
        "          ancestors = list(token.ancestors)\n",
        "          if (token.pos_ == \"VERB\" and len(ancestors) == 1\\\n",
        "              and ancestors[0] == root_token):\n",
        "              other_verbs.append(token)\n",
        "      return other_verbs    \n",
        "\n",
        "  def get_clause_token_span_for_verb(self, verb, doc, all_verbs):\n",
        "      first_token_index = len(doc)\n",
        "      last_token_index = 0\n",
        "      this_verb_children = list(verb.children)\n",
        "      for child in this_verb_children:\n",
        "          if (child not in all_verbs):\n",
        "              if (child.i < first_token_index):\n",
        "                  first_token_index = child.i\n",
        "              if (child.i > last_token_index):\n",
        "                  last_token_index = child.i\n",
        "      return first_token_index, last_token_index\n",
        "\n",
        "  def extractData(self, doc):\n",
        "      root_token = spacyClause.find_root_of_sentence(self, doc)\n",
        "      other_verbs = spacyClause.find_other_verbs(self, doc, root_token)\n",
        "      all_verbs = [root_token] + other_verbs\n",
        "      token_spans = [spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs]   \n",
        "      sentence_clauses = [doc[token_span[0]:token_span[1]] for token_span in token_spans if (token_span[0] < token_span[1])]\n",
        "      sentence_clauses = sorted(sentence_clauses, key=lambda tup: tup[0])    \n",
        "      clauses_text = [clause.text for clause in sentence_clauses]\n",
        "      return clauses_text  \n",
        "\n",
        "  def main(self):\n",
        "      id_req = spacyClause.fulldataset(self, srs_param)['ID']\n",
        "      req = spacyClause.fulldataset(self, srs_param)[col_param]\n",
        "      dataSpacy = []\n",
        "      nlp = spacy.load(spacy_param)\n",
        "      for id, num in zip(id_req, req):\n",
        "          doc = nlp(num)\n",
        "          myClause = spacyClause.extractData(self, doc)\n",
        "          jml_clausa = len(myClause)\n",
        "          dataSpacy.append([id, num, myClause, jml_clausa])\n",
        "\n",
        "      spacy_df = pd.DataFrame(dataSpacy, columns = ['ID', 'req', 'clause', 'jml_clausa'])\n",
        "      print(tabulate(spacy_df, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "    spacyClause().main()\n",
        "\n",
        "  except OSError as err:\n",
        "    print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spacySent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky1HfEKUfi6A",
        "cellView": "form",
        "outputId": "0c4fb696-5bd0-4d0d-b648-d7a20f7e1c08"
      },
      "source": [
        "#@title Build Subclause { vertical-output: true }\n",
        "%%writefile subclause.py\n",
        "import itertools\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "# !git clone https://github.com/Christopher-Thornton/subclause.git\n",
        "\n",
        "def get_animate_words():\n",
        "    animate_words = set(line.strip() for line in open('/content/subclause/animacy/animate.unigrams.txt', 'r', encoding='utf8'))\n",
        "    animate_words.update({\"i\", \"me\", \"myself\", \"mine\", \"my\", \"we\", \"us\", \"ourself\", \"ourselves\", \"ours\", \"our\",\n",
        "                          \"you\", \"yourself\", \"yours\", \"your\", \"yourselves\", \"he\", \"him\", \"himself\", \"his\", \"she\",\n",
        "                          \"her\", \"herself\", \"hers\", \"her\", \"one\", \"oneself\", \"one's\", \"they\", \"them\", \"themself\",\n",
        "                          \"themselves\", \"theirs\", \"their\", \"they\", \"them\", \"'em\", \"themselves\", \"who\", \"whom\",\n",
        "                          \"whose\"})\n",
        "    return animate_words\n",
        "\n",
        "\n",
        "def get_inanimate_words():\n",
        "    inanimate_words = set(line.strip() for line in open('/content/subclause/animacy/inanimate.unigrams.txt', 'r', encoding='utf8'))\n",
        "    inanimate_words.update({\"it\", \"itself\", \"its\", \"where\", \"when\"})\n",
        "    return inanimate_words\n",
        "\n",
        "\n",
        "ANIMATE = get_animate_words()\n",
        "INANIMATE = get_inanimate_words()\n",
        "\n",
        "\n",
        "class SubClauseFinder:\n",
        "    def __init__(self):\n",
        "        # target relations of dependency parsing\n",
        "        self.TARGET_RELATIONS = {'relcl', 'advcl', 'ccomp', 'csubj', 'csubjpass', 'xcomp'}\n",
        "\n",
        "    def get_dependency_tree(self, root):\n",
        "        # SpaCy dependency parse doesnt return a tree, start from the root token and\n",
        "        # navigate down the tree via .children\n",
        "        dependency_tree = [root]\n",
        "        while sum([len(list(tok.children)) for tok in dependency_tree[-1]]) > 0:\n",
        "            dependency_tree.append(list(itertools.chain.from_iterable(\n",
        "                [list(tok.children) for tok in dependency_tree[-1]])))\n",
        "        dependency_tree = list(itertools.chain.from_iterable(dependency_tree))\n",
        "        return dependency_tree\n",
        "\n",
        "    def get_subclauses(self, annotated_sent):\n",
        "        root = [token for token in annotated_sent if token.dep_ == 'ROOT']\n",
        "        dependency_tree = self.get_dependency_tree(root)\n",
        "\n",
        "        # iterate the edges to find dependent clauses relations\n",
        "        subordinate_edges = []\n",
        "        for clause_root in dependency_tree:\n",
        "            if clause_root.dep_ in self.TARGET_RELATIONS:\n",
        "                subordinate_edges.append(clause_root)\n",
        "\n",
        "        subclauses = []\n",
        "        for clause_root in subordinate_edges:\n",
        "            clause_type = self.identify_clause_type(clause_root.dep_)\n",
        "            # extract information of specific clause type\n",
        "            if clause_type == 'RELATIVE':\n",
        "                clause = RelativeClause(annotated_sent, clause_root)\n",
        "            elif clause_type == 'ADJUNCT':\n",
        "                clause = AdjunctClause(annotated_sent, clause_root)\n",
        "            elif clause_type == 'COMPLEMENT':\n",
        "                clause = ComplementClause(annotated_sent, clause_root)\n",
        "            else:\n",
        "                raise ValueError\n",
        "            subclauses.append(clause)\n",
        "        return subclauses\n",
        "\n",
        "    def identify_clause_type(self, clause_root_dep):\n",
        "        if clause_root_dep == 'relcl':\n",
        "            return 'RELATIVE'\n",
        "        elif clause_root_dep == 'advcl':\n",
        "            return 'ADJUNCT'\n",
        "        elif clause_root_dep in {'ccomp', 'csubj', 'csubjpass', 'xcomp'}:\n",
        "            return 'COMPLEMENT'\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "\n",
        "class SubordinateClause:\n",
        "    # Abstract class for storing subordinate clause information.\n",
        "    # Actual subordinate clauses extend this class.\n",
        "    def __init__(self, annotated_sent, clause_root):\n",
        "        self.annotated_sent = annotated_sent\n",
        "        self.clause_root = clause_root\n",
        "        self.clause_root_dep = clause_root.dep_\n",
        "        # identify clause finiteness\n",
        "        self.is_finite = None\n",
        "        # type of subordinate clause\n",
        "        self.clause_type = None\n",
        "        self.clause_span = None\n",
        "        # subordinator is'mark' in adverbial clauses and complement clause but 'ref' in relative clause\n",
        "        self.subordinator = None\n",
        "        # level of embeddedness, main clause at level 0\n",
        "        self.embeddedness = None\n",
        "\n",
        "    def get_is_finite(self):\n",
        "        if self.is_finite is None:\n",
        "            self.identify_finiteness()\n",
        "        return self.is_finite\n",
        "\n",
        "    def get_clause_type(self):\n",
        "        if self.clause_type is None:\n",
        "            self.identify_clause_type()\n",
        "        return self.clause_type\n",
        "\n",
        "    def get_clause_span(self):\n",
        "        if self.clause_span is None:\n",
        "            self.set_clause_span()\n",
        "        return self.clause_span\n",
        "\n",
        "    def get_subordinator(self):\n",
        "        if self.subordinator is None:\n",
        "            self.identify_subordinator()\n",
        "        return self.subordinator\n",
        "\n",
        "    def get_embeddedness(self):\n",
        "        if self.embeddedness is None:\n",
        "            self.count_embeddedness()\n",
        "        return self.embeddedness\n",
        "\n",
        "    def referent_dependency(self, outedge):\n",
        "        \"\"\"\n",
        "        https://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf\n",
        "        A referent is the relation between a wh-complementizer in a relative clause and its referential head. In\n",
        "        Referent relations are represented as secondary dependencies because integrating them with\n",
        "        other dependencies breaks the single-head tree property (e.g., which would have multiple heads in Figure 28).\n",
        "        \"\"\"\n",
        "        # TODO: Not Implemented\n",
        "        return False\n",
        "\n",
        "    def identify_subordinator(self):\n",
        "        # for relative clauses, find the \"referent\"\n",
        "        if self.get_clause_type() == 'RELATIVE':\n",
        "            head_noun = self.clause_root.head\n",
        "            for child in head_noun.children:\n",
        "                if self.referent_dependency(child):\n",
        "                    self.subordinator = child\n",
        "        else:\n",
        "            # for adverbial and complement clauses, find the \"mark\"\n",
        "            for child in self.clause_root.children:\n",
        "                child_dep = child.dep_\n",
        "                if child_dep == 'mark':  # MARKER\n",
        "                    self.subordinator = child\n",
        "\n",
        "    def set_clause_span(self):\n",
        "        self.clause_span = self.make_span(self.clause_root)\n",
        "\n",
        "    def count_embeddedness(self):\n",
        "        sent_root = [token for token in self.annotated_sent if token.dep_ == 'ROOT'][0]\n",
        "        if sent_root.head.i == sent_root.i or sent_root.i == self.clause_root.i:\n",
        "            self.embeddedness = 1\n",
        "            return\n",
        "        # find number of edges to go from clause root to sent root\n",
        "        # Load spaCy's dependency tree into a networkx graph\n",
        "        edges = []\n",
        "        for token in self.annotated_sent:\n",
        "            for child in token.children:\n",
        "                edges.append(('{0}'.format(token.i),\n",
        "                              '{0}'.format(child.i)))\n",
        "        graph = nx.Graph(edges)\n",
        "        # Get the length and path\n",
        "        levels = nx.shortest_path_length(graph, source=str(self.clause_root.i), target=str(sent_root.i))\n",
        "        return levels\n",
        "\n",
        "    def identify_clause_type(self):\n",
        "        if self.clause_root_dep == 'relcl':\n",
        "            self.clause_type = 'RELATIVE'\n",
        "        elif self.clause_root_dep == 'advcl':\n",
        "            self.clause_type = 'ADJUNCT'\n",
        "        elif self.clause_root_dep in {'ccomp', 'csubj', 'csubjpass', 'xcomp'}:\n",
        "            self.clause_type = 'COMPLEMENT'\n",
        "\n",
        "    def identify_finiteness(self):\n",
        "        \"\"\"\n",
        "        check if the sub clause finite\n",
        "        Finite clauses are clauses that contain verbs which show tense. Otherwise they are nonfinite.\n",
        "        some examples:\n",
        "        I had something to eat [before leaving].\n",
        "        [After having spent six hours at the hospital], they eventually came home.\n",
        "        [Helped by local volunteers], staff at the museum have spent many years cataloguing photographs.\n",
        "        He left the party and went home, [not having anyone to talk to].\n",
        "        The person to ask [about going to New Zealand] is Beck.\n",
        "        You have to look at the picture really carefully [in order to see all the detail].\n",
        "        \"\"\"\n",
        "        # xcomp is nonfinite by definition\n",
        "        if self.clause_root_dep == 'xcomp':\n",
        "            self.is_finite = False\n",
        "            return\n",
        "        # the verb is the root of the clause\n",
        "        idx_word_before_verb = self.clause_root.i - 1\n",
        "        verb_pos = self.clause_root.pos_\n",
        "        if idx_word_before_verb < self.annotated_sent.start:\n",
        "            if verb_pos in {\"VBG\"  \"VBN\"}:\n",
        "                self.is_finite = False\n",
        "                return\n",
        "            else:\n",
        "                # not VBG or VBN, then finite\n",
        "                self.is_finite = True\n",
        "                return\n",
        "        wordBeforeVerb = self.annotated_sent[idx_word_before_verb - self.annotated_sent.start]\n",
        "        #  if the verb follows TO or a preposition, it is nonfinite\n",
        "        posWordBeforeVerb = wordBeforeVerb.pos_\n",
        "        if posWordBeforeVerb in {\"IN\", \"TO\"}:\n",
        "            self.is_finite = False\n",
        "            return\n",
        "        # if verb is gerund (VBG), it must have an aux, otherwise nonfinite\n",
        "        if verb_pos == \"VBG\":\n",
        "            hasAux = False\n",
        "            # check if there is aux\n",
        "            for child in self.clause_root.children_:  # childIterable(self.clause_root)\n",
        "                rel = child.dep_\n",
        "                if rel == \"aux\":\n",
        "                    hasAux = True\n",
        "            if not hasAux:\n",
        "                self.is_finite = False\n",
        "                return\n",
        "        # if verb is past participle (VBN), it must have aux/auxpass which is not VBGs, otherwise non-finite\n",
        "        if verb_pos == \"VBN\":\n",
        "            vbg_aux = False\n",
        "            # check if there is aux that is not in gerund form\n",
        "            for child in self.clause_root.children_:  # childIterable\n",
        "                if child.dep_ in {\"aux\" \"auxpass\"}:\n",
        "                    # get pos of aux\n",
        "                    aux = child  # child.getDependent()\n",
        "                    auxPOS = aux.pos_\n",
        "                    if auxPOS == \"VBG\":\n",
        "                        vbg_aux = True\n",
        "                    if vbg_aux:\n",
        "                        self.is_finite = False\n",
        "                        return\n",
        "        self.is_finite = True\n",
        "\n",
        "    def make_span(self, word):\n",
        "        i = word.i - self.annotated_sent.start\n",
        "        span = self.annotated_sent[\n",
        "               self.annotated_sent[i].left_edge.i - self.annotated_sent.start:\n",
        "               self.annotated_sent[i].right_edge.i + 1 - self.annotated_sent.start]\n",
        "\n",
        "        return span\n",
        "\n",
        "\n",
        "class RelativeClause(SubordinateClause):\n",
        "    def __init__(self, annotated_sent, clause_root):\n",
        "        super().__init__(annotated_sent, clause_root)\n",
        "        self.clause_type = \"RELATIVE\"\n",
        "        # further information related to relative clause\n",
        "        self.is_restrictive = None\n",
        "        # head noun\n",
        "        # TODO: Decide if noun chunking should be used\n",
        "        self.head_noun = self.clause_root.head\n",
        "        self.is_head_noun_animate = None\n",
        "        # head noun role in main clause\n",
        "        self.head_noun_role_in_main_clause = None\n",
        "        self.head_noun_role_in_sub_clause = None\n",
        "        # relative clauses's embeddedness is different from the other two types of clause\n",
        "        self.embeddedness = max(self.get_embeddedness() - 1, 1)\n",
        "\n",
        "    def get_head_noun(self):\n",
        "        return self.head_noun\n",
        "\n",
        "    def get_head_noun_animacy(self):\n",
        "        if self.get_head_noun() is not None:\n",
        "            if self.is_head_noun_animate is None:\n",
        "                self.set_head_noun_animacy()\n",
        "            return self.is_head_noun_animate\n",
        "\n",
        "    def get_head_noun_role_in_main_clause(self):\n",
        "        if self.get_head_noun() is not None:\n",
        "            if self.head_noun_role_in_main_clause is None:\n",
        "                self.set_head_noun_roles()\n",
        "            return self.head_noun_role_in_main_clause\n",
        "\n",
        "    def get_head_noun_role_in_sub_clause(self):\n",
        "        if self.get_head_noun() is not None:\n",
        "            if self.head_noun_role_in_sub_clause is None:\n",
        "                self.set_head_noun_roles()\n",
        "            return self.head_noun_role_in_sub_clause\n",
        "\n",
        "    def get_is_restrictive(self):\n",
        "        if self.is_restrictive is None:\n",
        "            self.set_restrictiveness()\n",
        "        return self.is_restrictive\n",
        "\n",
        "    def set_head_noun_animacy(self):\n",
        "        # TODO: use alternate method to detect animacy (Language Models)\n",
        "        if self.get_head_noun() in ANIMATE:\n",
        "            self.is_head_noun_animate = True\n",
        "        else:\n",
        "            self.is_head_noun_animate = False\n",
        "\n",
        "    def set_head_noun_roles(self):\n",
        "        # TODO: Check function\n",
        "        # https://www.brighthubeducation.com/english-homework-help/32754-the-functions-of-nouns-and-noun-phrases/\n",
        "        is_from_inside_rc = False\n",
        "        edge = self.get_head_noun()\n",
        "        relation = edge.dep_\n",
        "        head_idx = edge.head.i\n",
        "\n",
        "        # see if it is from inside or outside of the RC\n",
        "        span = self.get_clause_span()\n",
        "        if span.start <= head_idx <= span.end-1:\n",
        "            is_from_inside_rc = True\n",
        "\n",
        "        if relation in {'nsubj', 'nsubjpass'}:\n",
        "            self.set_role('SUBJECT', is_from_inside_rc)\n",
        "        elif relation == 'dobj':\n",
        "            self.set_role('DIRECT_OBJECT', is_from_inside_rc)\n",
        "        elif relation == 'pobj':  # 'iobj'\n",
        "            self.set_role('INDIRECT_OBJECT', is_from_inside_rc)\n",
        "        elif relation == 'nmod':\n",
        "            self.set_role('PREPOSITION_COMPLEMENT', is_from_inside_rc)\n",
        "        elif relation == 'appos':\n",
        "            self.set_role('APPOSITIVE', is_from_inside_rc)\n",
        "\n",
        "    def set_role(self, role, is_from_inside_rc):\n",
        "        if is_from_inside_rc:\n",
        "            self.head_noun_role_in_sub_clause = role\n",
        "        else:\n",
        "            self.head_noun_role_in_main_clause = role\n",
        "\n",
        "    def set_restrictiveness(self):\n",
        "        # if zero relativizer or \"that\", restrictive\n",
        "        subordinator = self.get_subordinator()\n",
        "        if subordinator is None or subordinator.text.lower() == \"that\":\n",
        "            self.is_restrictive = True\n",
        "            return\n",
        "\n",
        "        head_noun = self.get_head_noun()\n",
        "        if head_noun is not None:\n",
        "            # if the head noun is personal pronoun or proper noun(s), the clause is nonrestrictive\n",
        "            head_noun_pos = head_noun.pos_\n",
        "            if head_noun_pos in {\"NNP\", \"NNPS\", \"PRP\"}:\n",
        "                self.is_restrictive = False\n",
        "                return\n",
        "            # if the head noun is modified by an indefinite determiner like 'a', 'some', or 'any', restrictive\n",
        "            for child in head_noun.children:\n",
        "                relation = child.dep_\n",
        "                if relation == 'det':  # DETERMINER\n",
        "                    determiner = child.text.lower()\n",
        "                    if determiner in {\"a\", \"an\", \"some\", \"any\"}:\n",
        "                        self.is_restrictive = True\n",
        "                        return\n",
        "        self.is_restrictive = True\n",
        "\n",
        "\n",
        "class AdjunctClause(SubordinateClause):\n",
        "    # function of clause, e.g. temporal, modal, instrumental...\n",
        "    def __init__(self, annotated_sent, clause_root):\n",
        "        super().__init__(annotated_sent, clause_root)\n",
        "\n",
        "        self.clause_type = \"ADJUNCT\"\n",
        "\n",
        "        self.TIME_SUBORDINATORS = {\"when\", \"before\", \"after\", \"since\", \"while\", \"as\", \"till\", \"until\"}\n",
        "        self.PLACE_SUBORDINATORS = {\"where\", \"wherever\", \"anywhere\", \"everywhere\"}\n",
        "        self.CONDITION_SUBORDINATORS = {\"if\", \"unless\", \"lest\", \"provided\"}\n",
        "        self.REASON_SUBORDINATORS = {\"because\", \"since\", \"as\", \"given\"}\n",
        "        self.CONCESSION_SUBORDINATORS = {\"although\", \"though\"}\n",
        "        self.PURPOSE_SUBORDINATORS = {\"so\", \"to\"}\n",
        "        self.COMPARISON_SUBORDINATORS = {\"than\"}\n",
        "        self.MANNER_SUBORDINATORS = {\"like\", \"way\"}\n",
        "        self.RESULTS_SUBORDINATORS = {\"so\", \"such\"}\n",
        "\n",
        "        self.adjunct_function = None\n",
        "\n",
        "    def get_adjunct_function(self):\n",
        "        if self.adjunct_function is None:\n",
        "            self.assign_function()\n",
        "        return self.adjunct_function\n",
        "\n",
        "    def assign_function(self):\n",
        "        subordinator = self.get_subordinator()\n",
        "        if subordinator is None:\n",
        "            self.adjunct_function = None\n",
        "            return\n",
        "        subordinator = subordinator.text.lower()\n",
        "        if subordinator in self.TIME_SUBORDINATORS:\n",
        "            self.adjunct_function = 'TIME'\n",
        "        elif subordinator in self.PLACE_SUBORDINATORS:\n",
        "            self.adjunct_function = 'PLACE'\n",
        "        elif subordinator in self.CONDITION_SUBORDINATORS:\n",
        "            self.adjunct_function = 'CONDITION'\n",
        "        elif subordinator in self.REASON_SUBORDINATORS:\n",
        "            self.adjunct_function = 'REASON'\n",
        "        elif subordinator in self.CONCESSION_SUBORDINATORS:\n",
        "            self.adjunct_function = 'CONCESSION'\n",
        "        elif subordinator in self.PURPOSE_SUBORDINATORS:\n",
        "            self.adjunct_function = 'PURPOSE'\n",
        "        elif subordinator in self.COMPARISON_SUBORDINATORS:\n",
        "            self.adjunct_function = 'COMPARISION'\n",
        "        elif subordinator in self.MANNER_SUBORDINATORS:\n",
        "            self.adjunct_function = 'MANNER'\n",
        "        elif subordinator in self.RESULTS_SUBORDINATORS:\n",
        "            self.adjunct_function = 'RESULTS'\n",
        "        else:\n",
        "            self.adjunct_function = 'OTHER'\n",
        "\n",
        "\n",
        "class ComplementClause(SubordinateClause):\n",
        "    def __init__(self, annotated_sent, clause_root):\n",
        "        super().__init__(annotated_sent, clause_root)\n",
        "\n",
        "        self.clause_type = \"COMPLEMENT\"\n",
        "        # set complement type (subject or object)\n",
        "        self.complement_type = None\n",
        "\n",
        "    def get_complement_type(self):\n",
        "        if self.complement_type is None:\n",
        "            self.identify_complement_type()\n",
        "        return self.complement_type\n",
        "\n",
        "    def identify_complement_type(self):\n",
        "        # ccomp is always object complement by definition\n",
        "        if self.clause_root.dep_ == \"ccomp\":\n",
        "            self.complement_type = 'OBJECT_COMPLEMENT'\n",
        "            return\n",
        "        # check governor/head of edge.\n",
        "        # If it is outside the clause, it is an object complement, otherwise subject,\n",
        "        # because English is an SVO language\n",
        "        head_idx = self.clause_root.head.i\n",
        "\n",
        "        span = self.get_clause_span()\n",
        "        if span.start <= head_idx <= span.end-1:\n",
        "            self.complement_type = 'OBJECT_COMPLEMENT'\n",
        "        else:\n",
        "            self.complement_type = 'SUBJECT_COMPLEMENT'\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    subclausefinder = SubClauseFinder()\n",
        "\n",
        "    text = \"The door opened because the man pushed it. \" \\\n",
        "           \"I wondered whether the homework was necessary. \" \\\n",
        "           \"They will visit you before they go to the airport. \" \\\n",
        "           \"Before they go to the airport, they will visit you. \" \\\n",
        "           \"I went to the show that was very popular.\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        print('\\nSentence:', sent)\n",
        "        subclauses = subclausefinder.get_subclauses(sent)\n",
        "        for sc in subclauses:\n",
        "            print(\"\\tclause text:\", sc.get_clause_span())\n",
        "            print(\"\\tis finite:\", sc.get_is_finite())\n",
        "            print(\"\\tsubordinator:\", sc.get_subordinator())\n",
        "            print(\"\\tembeddedness:\", sc.get_embeddedness())\n",
        "            print(\"\\tclause type:\", sc.get_clause_type())\n",
        "            # for complement clauses\n",
        "            if sc.get_clause_type() == 'COMPLEMENT':\n",
        "                print(\"\\t\\tcomplement type:\", sc.get_complement_type())\n",
        "            # for adverbial clauses\n",
        "            elif sc.get_clause_type() == 'ADJUNCT':\n",
        "                print(\"\\t\\tadjunct function:\", sc.get_adjunct_function())\n",
        "            # for relative clauses\n",
        "            elif sc.get_clause_type() == 'RELATIVE':\n",
        "                print(\"\\t\\tis restrictive:\", sc.get_is_restrictive())\n",
        "                print(\"\\t\\thead noun:\", sc.get_head_noun())\n",
        "                print(\"\\t\\tis head noun animate:\", sc.get_head_noun_animacy())\n",
        "                print(\"\\t\\thead noun role in main clause:\", sc.get_head_noun_role_in_main_clause())\n",
        "                print(\"\\t\\thead noun role in subordinate clause:\", sc.get_head_noun_role_in_sub_clause())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing subclause.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig2ASmkKi4y9",
        "cellView": "form",
        "outputId": "09d6809e-d801-4811-d572-99f854eb58ea"
      },
      "source": [
        "%%writefile triplet.py\n",
        "\"\"\"\n",
        "Created on Th Oktober  7 18:07:24 2021\n",
        "@author: Rakha asyrofi\n",
        "triplet parameter\n",
        "\"\"\"\n",
        "\n",
        "#@title Modul2c: Triplet NLTK-Stanford Triplet Parameter { vertical-output: true }\n",
        "url_param = \"http://corenlp.run/\" #@param {type:\"string\"}\n",
        "dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "srs_param = \"2003 - Tachonet\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"]\n",
        "\n",
        "# dataFile = \"/content/drive/MyDrive/dataset/visualPartOf/partOf2005 - Grid 3D.xlsx\" #@param {type:\"string\"}\n",
        "# srs_param = \"tabel_partOf\" #@param {type:\"string\"}\n",
        "\n",
        "col_param = \"Requirement Statement\" #@param [\"Requirement Statement\", \"req\"]\n",
        "mode_param = \"result\" #@param [\"parse_tree\", \"spo\", \"result\"]\n",
        "\n",
        "import nltk, pandas as pd, numpy as np\n",
        "from nltk.tag.stanford import CoreNLPPOSTagger\n",
        "from nltk.tree import ParentedTree\n",
        "from tabulate import tabulate\n",
        "\n",
        "class extractNlp:\n",
        "  def __init__(self, coreUrl = url_param, fileName= dataFile):\n",
        "      self.__pos_tagger = CoreNLPPOSTagger(url= coreUrl)\n",
        "      self.__data = fileName\n",
        "\n",
        "  def fulldataset(self, inputSRS= 'req'):\n",
        "      xl = pd.ExcelFile(self.__data)\n",
        "      dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}[inputSRS]\n",
        "      return dfs\n",
        "\n",
        "  def preprocessing(self):\n",
        "      xl = pd.ExcelFile(self.__data)\n",
        "      for sh in xl.sheet_names:\n",
        "        df = xl.parse(sh)\n",
        "        print('Processing: [{}] ...'.format(sh))\n",
        "        print(df.head())\n",
        "\n",
        "  def triplet_extraction (self, input_sent, output=['parse_tree','spo','result']):\n",
        "      parse_tree, = ParentedTree.convert(list(self.__pos_tagger.parse(input_sent.split()))[0])\n",
        "\n",
        "      # Extract subject, predicate and object\n",
        "      subject = extractNlp.extract_subject(self, parse_tree)\n",
        "      predicate = extractNlp.extract_predicate(self, parse_tree)\n",
        "      objects = extractNlp.extract_object(self, parse_tree)\n",
        "      if 'parse_tree' in output:\n",
        "          print('---Parse Tree---')\n",
        "          parse_tree.pretty_print()\n",
        "      elif 'spo' in output:\n",
        "          return (\"subject:\\t{}\\npredicate:\\t{}\\nobject:\\t{}\".format(subject, predicate, objects))\n",
        "      elif 'result' in output:\n",
        "          return (' '.join([subject[0], predicate[0], objects[0]]))\n",
        "\n",
        "  def extract_subject (self, parse_tree):\n",
        "      # Extract the first noun found in NP_subtree\n",
        "      subject = []\n",
        "      for s in parse_tree.subtrees(lambda x: x.label() == 'NP'):\n",
        "          for t in s.subtrees(lambda y: y.label().startswith('NN')):\n",
        "              output = [t[0], extractNlp.extract_attr(self, t)]\n",
        "              # Avoid empty or repeated values\n",
        "              if output != [] and output not in subject:\n",
        "                  subject.append(output) \n",
        "      if len(subject) != 0: return subject[0] \n",
        "      else: return ['']\n",
        "\n",
        "  def extract_predicate (self, parse_tree):\n",
        "      # Extract the deepest(last) verb foybd ub VP_subtree\n",
        "      output, predicate = [],[]\n",
        "      for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
        "          for t in s.subtrees(lambda y: y.label().startswith('VB')):\n",
        "              output = [t[0], extractNlp.extract_attr(self, t)]\n",
        "              if output != [] and output not in predicate:    \n",
        "                  predicate.append(output)\n",
        "      if len(predicate) != 0: return predicate[-1]\n",
        "      else: return ['']\n",
        "\n",
        "  def extract_object (self, parse_tree):\n",
        "      # Extract the first noun or first adjective in NP, PP, ADP siblings of VP_subtree\n",
        "      objects, output, word = [],[],[]\n",
        "      for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
        "          for t in s.subtrees(lambda y: y.label() in ['NP','PP','ADP']):\n",
        "              if t.label() in ['NP','PP']:\n",
        "                  for u in t.subtrees(lambda z: z.label().startswith('NN')):\n",
        "                      word = u          \n",
        "              else:\n",
        "                  for u in t.subtrees(lambda z: z.label().startswith('JJ')):\n",
        "                      word = u\n",
        "              if len(word) != 0:\n",
        "                  output = [word[0], extractNlp.extract_attr(self, word)]\n",
        "              if output != [] and output not in objects:\n",
        "                  objects.append(output)\n",
        "      if len(objects) != 0: return objects[0]\n",
        "      else: return ['']\n",
        "\n",
        "  def extract_attr (self, word):\n",
        "      attrs = []     \n",
        "      # Search among the word's siblings\n",
        "      if word.label().startswith('JJ'):\n",
        "          for p in word.parent(): \n",
        "              if p.label() == 'RB':\n",
        "                  attrs.append(p[0])\n",
        "      elif word.label().startswith('NN'):\n",
        "          for p in word.parent():\n",
        "              if p.label() in ['DT','PRP$','POS','JJ','CD','ADJP','QP','NP']:\n",
        "                  attrs.append(p[0])\n",
        "      elif word.label().startswith('VB'):\n",
        "          for p in word.parent():\n",
        "              if p.label() == 'ADVP':\n",
        "                  attrs.append(p[0])\n",
        "      # Search among the word's uncles\n",
        "      if word.label().startswith('NN') or word.label().startswith('JJ'):\n",
        "          for p in word.parent().parent():\n",
        "              if p.label() == 'PP' and p != word.parent():\n",
        "                  attrs.append(' '.join(p.flatten()))\n",
        "      elif word.label().startswith('VB'):\n",
        "          for p in word.parent().parent():\n",
        "              if p.label().startswith('VB') and p != word.parent():\n",
        "                  attrs.append(' '.join(p.flatten()))\n",
        "      return attrs\n",
        "  \n",
        "  def __del__(self):\n",
        "      print(\"destructed\")\n",
        "\n",
        "  def main(self, output):\n",
        "      id_req = extractNlp.fulldataset(self, srs_param)['ID']\n",
        "      data_num = [num for num in extractNlp.fulldataset(self, srs_param)[col_param]]\n",
        "      data_triplet = [extractNlp.triplet_extraction(self, num, output= mode_param) \n",
        "                      for num in extractNlp.fulldataset(self, srs_param)[col_param]]\n",
        "      triplet_df = pd.DataFrame([data_num, data_triplet], index= ['origin', 'triplet'], columns= id_req).T\n",
        "      print(tabulate(triplet_df, headers = 'keys', tablefmt = 'psql'))\n",
        "      extractNlp.__del__(self)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "    extractNlp().main(mode_param)\n",
        "\n",
        "  except OSError as err:\n",
        "    print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing triplet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8bVxYQzISQI"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tOobyaGH5oB",
        "outputId": "e48b87a2-bd4a-4259-b3cd-aee6daaba94b"
      },
      "source": [
        "import nltk\n",
        "from nltk.tag.stanford import CoreNLPPOSTagger\n",
        "\n",
        "part1 = partOf() \n",
        "dataReq = part1.fulldataset(srs_param)\n",
        "pos_tagger = CoreNLPPOSTagger(url= \"http://corenlp.run\")\n",
        "a = dataReq['Requirement Statement'][7]\n",
        "sents = nltk.sent_tokenize(a)\n",
        "parse_tree, = ParentedTree.convert(list(pos_tagger.parse(a.split()))[0])\n",
        "tokens = [nltk.word_tokenize(num) for num in sents]\n",
        "tag = [pos_tagger.tag(num) for num in tokens]\n",
        "c = pos_tagger.parse(sents)\n",
        "parse_tree, = ParentedTree.convert(list(c))\n",
        "\n",
        "# tab1 = pd.DataFrame([a, sents], index= ['F08', 'K1']).T\n",
        "# print(tabulate(tab1, headers = 'keys', tablefmt = 'psql'))\n",
        "# tab2 = pd.DataFrame([sents, tokens], index= ['K1', 'tokenized_K1']).T\n",
        "# print(tabulate(tab2, headers = 'keys', tablefmt = 'psql'))\n",
        "# tab3 = pd.DataFrame([tokens, tag], index= ['tokenized_K1', 'Tagged_K1']).T\n",
        "# print(tabulate(tab3, headers = 'keys', tablefmt = 'psql'))\n",
        "# tab4 = pd.DataFrame([tag, parse_tree], index= ['Tagged_K1', 'Parse_K1']).T\n",
        "# print(tabulate(tab4, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "tab1 = pd.DataFrame([a, sents, tokens, tag, parse_tree], index = ['F07', 'K1', 'tokenized_K1', 'Tagged_K1', 'Parse_K1'])\n",
        "print(tabulate(tab1, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "# num = 'The product should allow the grid to be oriented by the user.' \n",
        "# num1 = 'Rotation functions should be employed'\n",
        "# num2 = 'zoom functions should be employed'\n",
        "# num3 = 'move functions should be employed'\n",
        "# sents = nltk.sent_tokenize(num)\n",
        "# tokens = [nltk.word_tokenize(num) for num in sents]\n",
        "# tag = [pos_tagger.tag(num) for num in tokens]\n",
        "# parse_tree, = ParentedTree.convert(list(pos_tagger.parse(num3.split()))[0])\n",
        "# print(parse_tree)\n",
        "\n",
        "# part3 = partOf(inputData= '/content/drive/MyDrive/dataset/dataset_2_grd.xlsx') \n",
        "# dataGrd = part3.fulldataset('stat')\n",
        "# dataTest = part3.fulldataset('test')\n",
        "# for idx, num in enumerate(dataGrd.Dataset):\n",
        "#   y_actual = dataTest.drop('Dataset', axis= 1).fillna(0).iloc[idx].values.astype(int) #define array of actual values\n",
        "#   y_predicted = dataGrd.drop('Dataset', axis= 1).iloc[idx].values.astype(int) #define array of predicted values\n",
        "#   nilai_akurasi = accuracy_score(y_true= y_actual, y_pred= y_predicted, normalize=True, sample_weight=None)\n",
        "#   nilai_recall = recall_score(y_true= y_actual, y_pred= y_predicted, average= 'macro')\n",
        "#   nilai_presisi = precision_score(y_true= y_actual, y_pred= y_predicted, average= 'macro')\n",
        "#   print(\"Data_{}\\nakurasi {}\\n recall {}\\n presisi {}\\n\".format(num, nilai_akurasi, nilai_recall, nilai_presisi))\n",
        "#   print(classification_report(y_true= y_actual, y_pred= y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructed_called\n",
            "+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|              | 0                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| F07          | The product should allow the grid to be oriented by the user. Rotation, zoom and move functions should be employed                                                                                                                                                                                                                                                           |\n",
            "| K1           | ['The product should allow the grid to be oriented by the user.', 'Rotation, zoom and move functions should be employed']                                                                                                                                                                                                                                                    |\n",
            "| tokenized_K1 | [['The', 'product', 'should', 'allow', 'the', 'grid', 'to', 'be', 'oriented', 'by', 'the', 'user', '.'], ['Rotation', ',', 'zoom', 'and', 'move', 'functions', 'should', 'be', 'employed']]                                                                                                                                                                                  |\n",
            "| Tagged_K1    | [[('The', 'DT'), ('product', 'NN'), ('should', 'MD'), ('allow', 'VB'), ('the', 'DT'), ('grid', 'NN'), ('to', 'TO'), ('be', 'VB'), ('oriented', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('user', 'NN'), ('.', '.')], [('Rotation', 'NN'), (',', ','), ('zoom', 'NN'), ('and', 'CC'), ('move', 'VB'), ('functions', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('employed', 'VBN')]] |\n",
            "| Parse_K1     | (ROOT                                                                                                                                                                                                                                                                                                                                                                        |\n",
            "|              |   (S                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|              |     (NP (DT The) (NN product))                                                                                                                                                                                                                                                                                                                                               |\n",
            "|              |     (VP                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|              |       (MD should)                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|              |       (VP                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|              |         (VB allow)                                                                                                                                                                                                                                                                                                                                                           |\n",
            "|              |         (NP (DT the) (NN grid))                                                                                                                                                                                                                                                                                                                                              |\n",
            "|              |         (S                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "|              |           (VP                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|              |             (TO to)                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|              |             (VP                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|              |               (VB be)                                                                                                                                                                                                                                                                                                                                                        |\n",
            "|              |               (ADJP                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|              |                 (VBN oriented)                                                                                                                                                                                                                                                                                                                                               |\n",
            "|              |                 (PP (IN by) (NP (DT the) (NN user)))))))))                                                                                                                                                                                                                                                                                                                   |\n",
            "|              |     (. .)))                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}