{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modul_partof.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oaihAadDQxbs",
        "jEubXuvy11cz",
        "OQ9rg4RMSLzN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqclWxczkBRI",
        "outputId": "33f49e84-520f-4f23-cb11-681631993db5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38G-s9c_sw8"
      },
      "source": [
        "!pip install -U pywsd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4BGyFj1AJY2"
      },
      "source": [
        "!pip install -U wn==0.0.23"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iNF7HpYSUqS"
      },
      "source": [
        "!pip install XlsxWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaihAadDQxbs"
      },
      "source": [
        "# partOf: wsd measurement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZkYnKBCjfKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0d4b18-4eaa-4c4a-be0d-6f62cb68e73d"
      },
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pywsd.cosine import cosine_similarity\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "stemming = PorterStemmer()\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "lem = WordNetLemmatizer()\n",
        "class wsd_partof:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def fulldataset(self, dataFile, inputSRS):\n",
        "      xl = pd.ExcelFile(dataFile)\n",
        "      dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}\n",
        "      kalimat = dfs[inputSRS]\n",
        "      kalimat_semua = kalimat.head(len(kalimat))\n",
        "      return kalimat_semua\n",
        "\n",
        "  def preprocessing(self, dataFile):\n",
        "    xl = pd.ExcelFile(dataFile)\n",
        "    for sh in xl.sheet_names:\n",
        "      df = xl.parse(sh)\n",
        "      print('Processing: [{}] ...'.format(sh))\n",
        "      print(df.head())\n",
        "\n",
        "  # cleaning text\n",
        "  def apply_cleaning_function_to_list(self, X):\n",
        "      cleaned_X = []\n",
        "      for element in X:\n",
        "          cleaned_X.append(wsd_partof.clean_text(self, raw_text= element))\n",
        "      return cleaned_X\n",
        "\n",
        "  def clean_text(self, raw_text):\n",
        "      text = raw_text.lower()\n",
        "      tokens = word_tokenize(text)\n",
        "      token_words = [w for w in tokens if w.isalpha()]\n",
        "      lemma_words = [lem.lemmatize(w) for w in token_words]\n",
        "      meaningful_words = [w for w in lemma_words if not w in stops]\n",
        "      joined_words = ( \" \".join(meaningful_words))\n",
        "      return joined_words    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      myWsd_partof = wsd_partof()\n",
        "      # myWsd_partof.preprocessing(dataFile= r'/content/drive/MyDrive/dataset/dataset_2_split.xlsx')\n",
        "\n",
        "      file1 = r'/content/drive/MyDrive/dataset/dataset_2.xlsx'\n",
        "      dataSRS =  '2005 - Grid 3D'\n",
        "      a = myWsd_partof.fulldataset(dataFile= file1, inputSRS= dataSRS)\n",
        "      list_req1 = list(a['Requirement Statement'])\n",
        "      id_req1 = list(a['ID'])\n",
        "      cleaned1 = myWsd_partof.apply_cleaning_function_to_list(X= list_req1)\n",
        "\n",
        "      file2 = r'/content/drive/MyDrive/dataset/dataset_2_split.xlsx'\n",
        "      b = myWsd_partof.fulldataset(dataFile= file2, inputSRS= dataSRS)\n",
        "      list_req2 = list(b['Requirement Statement'])\n",
        "      id_req2 = list(b['ID'])\n",
        "      cleaned2 = myWsd_partof.apply_cleaning_function_to_list(X= list_req2)\n",
        "\n",
        "      hasil_wsd = []\n",
        "      for num in cleaned1:\n",
        "        text = [cosine_similarity(num, angka) for angka in cleaned2]\n",
        "        hasil_wsd.append(text)\n",
        "\n",
        "      data_raw = pd.DataFrame(hasil_wsd, index= id_req1, columns= id_req2)\n",
        "      print(\"Hasil pengukuran semantik antar kebutuhan atomik dan non atomik {}\".format(dataSRS))\n",
        "      print(tabulate(data_raw, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "      # thresholding\n",
        "      threshold = 0.1\n",
        "      d = data_raw.values >= threshold\n",
        "      d1 = pd.DataFrame(d, index= id_req1, columns= id_req2)\n",
        "      mask = d1.isin([True])\n",
        "      d2 = d1.where(mask, other= 0)\n",
        "      mask2 = d1.isin([False])\n",
        "      d3 = d2.where(mask2, other= 1)\n",
        "      print(\"\\nHasil ukur semantik diatas threshold {}\".format(threshold))\n",
        "      print(tabulate(d3, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "      file3 = r'/content/drive/MyDrive/dataset/wsd/wsd_groundtruth.xlsx'\n",
        "      dataGT = 'grid3d_eval'\n",
        "      b3 = fulldataset(data= file3, inputSRS= dataGT)\n",
        "      b3 = b3.drop(['Index'], axis= 1)\n",
        "      b3.index= d3.index\n",
        "      print(\"\\nData Hasil Ground Truth {}\".format(dataGT))\n",
        "      print(tabulate(b3, headers = 'keys', tablefmt = 'psql'))  \n",
        "\n",
        "      y_actual = d3.values.astype(int)\n",
        "      y_predicted = b3.values.astype(int)\n",
        "      print(\"akurasi\", metrics.accuracy_score(y_true= d3_list, y_pred= b3_list))\n",
        "      print(\"presion\", metrics.precision_score(y_true= y_actual, y_pred= y_predicted, average= 'macro'))\n",
        "      print(\"recall\", metrics.recall_score(y_true= y_actual, y_pred= y_predicted, average= 'macro'))\n",
        "      print(\"metrics {}\".format(metrics.classification_report(y_true= y_actual, y_pred= y_predicted)))       \n",
        "\n",
        "  except OSError as err:\n",
        "    print(\"OS error: {0}\".format(err))\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil pengukuran semantik antar kebutuhan atomik dan non atomik 2005 - Grid 3D\n",
            "+------+----------+----------+-----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "|      |      F01 |      F02 |       F03 |      F04 |      F05 |      F06 |       F07 |     F08a |     F08b |     F08c |     F08d |     F09a |     F09b |     F09c |     F09d |     F09e |    NF01a |    NF01b |     NF02 |    NF03a |    NF03b |    NF03c |    NF04a |    NF04b |    NF05a |    NF05b |    NF06a |    NF06b |    NF06c |     NF07 |     NF08 |\n",
            "|------+----------+----------+-----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 1        | 0.125    | 0.319801  | 0.474342 | 0.392232 | 0.392232 | 0.223607  | 0.158114 | 0        | 0        | 0        | 0.375    | 0.353553 | 0.25     | 0        | 0.316228 | 0.176777 | 0.176777 | 0.235702 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 0        | 0.288675 | 0        |\n",
            "| F02  | 0.125    | 1        | 0.319801  | 0.158114 | 0.196116 | 0.196116 | 0         | 0.158114 | 0        | 0        | 0        | 0.375    | 0.353553 | 0.25     | 0        | 0.158114 | 0        | 0        | 0        | 0.408248 | 0.204124 | 0.158114 | 0        | 0        | 0        | 0        | 0        | 0.176777 | 0        | 0.144338 | 0        |\n",
            "| F03  | 0.319801 | 0.319801 | 1         | 0.40452  | 0.501745 | 0.501745 | 0.0953463 | 0        | 0        | 0        | 0        | 0.639602 | 0.603023 | 0.426401 | 0        | 0.40452  | 0.150756 | 0.150756 | 0.201008 | 0        | 0        | 0        | 0.213201 | 0        | 0        | 0        | 0        | 0.603023 | 0        | 0.369274 | 0        |\n",
            "| F04  | 0.474342 | 0.158114 | 0.40452   | 1        | 0.496139 | 0.496139 | 0.282843  | 0.2      | 0        | 0        | 0        | 0.474342 | 0.447214 | 0.316228 | 0        | 0.4      | 0.223607 | 0.223607 | 0.298142 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.447214 | 0        | 0.365148 | 0        |\n",
            "| F05  | 0.392232 | 0.196116 | 0.501745  | 0.496139 | 1        | 0.692308 | 0.263117  | 0        | 0        | 0        | 0        | 0.686406 | 0.5547   | 0.49029  | 0        | 0.496139 | 0.27735  | 0.27735  | 0.3698   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.5547   | 0        | 0.452911 | 0        |\n",
            "| F06  | 0.392232 | 0.196116 | 0.501745  | 0.496139 | 0.692308 | 1        | 0.175412  | 0        | 0        | 0        | 0        | 0.588348 | 0.5547   | 0.392232 | 0        | 0.496139 | 0.27735  | 0.27735  | 0.3698   | 0        | 0        | 0        | 0.196116 | 0        | 0.196116 | 0.138675 | 0        | 0.5547   | 0        | 0.452911 | 0        |\n",
            "| F07  | 0.223607 | 0        | 0.0953463 | 0.282843 | 0.263117 | 0.175412 | 1         | 0.282843 | 0        | 0        | 0        | 0.223607 | 0.105409 | 0.111803 | 0        | 0.141421 | 0.158114 | 0.158114 | 0.210819 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.158114 | 0        | 0.129099 | 0        |\n",
            "| F08  | 0.111803 | 0.111803 | 0         | 0.141421 | 0        | 0        | 0.2       | 0.707107 | 0.547723 | 0.547723 | 0.547723 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.282843 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.1      |\n",
            "| F09  | 0.412568 | 0.257855 | 0.571739  | 0.521862 | 0.728202 | 0.647291 | 0.230633  | 0        | 0        | 0        | 0        | 0.773566 | 0.826568 | 0.773566 | 0.145865 | 0.717561 | 0.29173  | 0.29173  | 0.388973 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.58346  | 0.145865 | 0.476393 | 0        |\n",
            "| NF01 | 0.158114 | 0        | 0.13484   | 0.2      | 0.248069 | 0.248069 | 0.141421  | 0        | 0        | 0        | 0        | 0.158114 | 0.149071 | 0.158114 | 0        | 0.2      | 0.894427 | 0.894427 | 0.298142 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.223607 | 0        | 0.182574 | 0.141421 |\n",
            "| NF02 | 0.235702 | 0        | 0.201008  | 0.298142 | 0.3698   | 0.3698   | 0.210819  | 0        | 0        | 0        | 0        | 0.235702 | 0.222222 | 0.235702 | 0        | 0.298142 | 0.333333 | 0.333333 | 1        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.333333 | 0        | 0.272166 | 0        |\n",
            "| NF03 | 0        | 0.33541  | 0         | 0        | 0        | 0        | 0         | 0.282843 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.547723 | 0.547723 | 0.707107 | 0        | 0.119523 | 0        | 0        | 0        | 0        | 0        | 0        | 0        |\n",
            "| NF04 | 0        | 0        | 0.100504  | 0        | 0        | 0.09245  | 0         | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.19245  | 0        | 0        | 0.471405 | 0.881917 | 0.235702 | 0.166667 | 0        | 0        | 0        | 0        | 0        |\n",
            "| NF05 | 0        | 0        | 0         | 0        | 0        | 0.124035 | 0         | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.316228 | 0        | 0.632456 | 0.894427 | 0        | 0        | 0        | 0        | 0        |\n",
            "| NF06 | 0.235702 | 0.117851 | 0.402015  | 0.298142 | 0.3698   | 0.3698   | 0.105409  | 0        | 0        | 0        | 0        | 0.353553 | 0.444444 | 0.235702 | 0        | 0.298142 | 0.166667 | 0.166667 | 0.222222 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.666667 | 0.666667 | 0.333333 | 0.408248 | 0        |\n",
            "| NF07 | 0.288675 | 0.144338 | 0.369274  | 0.365148 | 0.452911 | 0.452911 | 0.129099  | 0        | 0        | 0        | 0        | 0.433013 | 0.408248 | 0.288675 | 0        | 0.365148 | 0.204124 | 0.204124 | 0.272166 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.204124 | 0.408248 | 0        | 1        | 0        |\n",
            "| NF08 | 0        | 0        | 0         | 0        | 0        | 0        | 0         | 0.141421 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.158114 | 0.158114 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 1        |\n",
            "+------+----------+----------+-----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "\n",
            "Hasil ukur semantik diatas threshold 0.1\n",
            "+------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+\n",
            "|      |   F01 |   F02 |   F03 |   F04 |   F05 |   F06 |   F07 |   F08a |   F08b |   F08c |   F08d |   F09a |   F09b |   F09c |   F09d |   F09e |   NF01a |   NF01b |   NF02 |   NF03a |   NF03b |   NF03c |   NF04a |   NF04b |   NF05a |   NF05b |   NF06a |   NF06b |   NF06c |   NF07 |   NF08 |\n",
            "|------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------|\n",
            "| F01  |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      1 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F02  |     1 |     1 |     1 |     1 |     1 |     1 |     0 |      1 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       0 |       0 |      0 |       1 |       1 |       1 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F03  |     1 |     1 |     1 |     1 |     1 |     1 |     0 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       1 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F04  |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      1 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F05  |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F06  |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       1 |       0 |       1 |       1 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F07  |     1 |     0 |     0 |     1 |     1 |     1 |     1 |      1 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| F08  |     1 |     1 |     0 |     1 |     0 |     0 |     1 |      1 |      1 |      1 |      1 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F09  |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      1 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       1 |      1 |      0 |\n",
            "| NF01 |     1 |     0 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      1 |\n",
            "| NF02 |     1 |     0 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       0 |      1 |      0 |\n",
            "| NF03 |     0 |     1 |     0 |     0 |     0 |     0 |     0 |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       1 |       1 |       1 |       0 |       1 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF04 |     0 |     0 |     1 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       1 |       0 |       0 |       1 |       1 |       1 |       1 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF05 |     0 |     0 |     0 |     0 |     0 |     1 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       1 |       0 |       1 |       1 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF06 |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       1 |       1 |      1 |      0 |\n",
            "| NF07 |     1 |     1 |     1 |     1 |     1 |     1 |     1 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      0 |      1 |       1 |       1 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       1 |       0 |      1 |      0 |\n",
            "| NF08 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       1 |       1 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      1 |\n",
            "+------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+\n",
            "\n",
            "Data Hasil Ground Truth grid3d_eval\n",
            "+------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+\n",
            "|      |   F01 |   F02 |   F03 |   F04 |   F05 |   F06 |   F07 |   F08a |   F08b |   F08c |   F08d |   F09a |   F09b |   F09c |   F09d |   F09e |   NF01a |   NF01b |   NF02 |   NF03a |   NF03b |   NF03c |   NF04a |   NF04b |   NF05a |   NF05b |   NF06a |   NF06b |   NF06c |   NF07 |   NF08 |\n",
            "|------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------|\n",
            "| F01  |     1 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F02  |     0 |     1 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F03  |     0 |     0 |     1 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F04  |     0 |     0 |     0 |     1 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F05  |     0 |     0 |     0 |     0 |     1 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F06  |     0 |     0 |     0 |     0 |     0 |     1 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F07  |     0 |     0 |     0 |     0 |     0 |     0 |     1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F08  |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      1 |      1 |      1 |      1 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| F09  |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      1 |      1 |      1 |      1 |      1 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF01 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       1 |       1 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF02 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF03 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       1 |       1 |       1 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF04 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       1 |       1 |       0 |       0 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF05 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       1 |       1 |       0 |       0 |       0 |      0 |      0 |\n",
            "| NF06 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       1 |       1 |       1 |      0 |      0 |\n",
            "| NF07 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      1 |      0 |\n",
            "| NF08 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |       0 |       0 |      0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |       0 |      0 |      1 |\n",
            "+------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+\n",
            "akurasi 0.0\n",
            "presion 1.0\n",
            "recall 0.3084845262264617\n",
            "metrics               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.08      0.14        13\n",
            "           1       1.00      0.09      0.17        11\n",
            "           2       1.00      0.08      0.15        12\n",
            "           3       1.00      0.08      0.14        13\n",
            "           4       1.00      0.08      0.15        12\n",
            "           5       1.00      0.08      0.14        13\n",
            "           6       1.00      0.09      0.17        11\n",
            "           7       1.00      0.14      0.25         7\n",
            "           8       1.00      1.00      1.00         1\n",
            "           9       1.00      1.00      1.00         1\n",
            "          10       1.00      1.00      1.00         1\n",
            "          11       1.00      0.08      0.15        12\n",
            "          12       1.00      0.08      0.15        12\n",
            "          13       1.00      0.08      0.15        12\n",
            "          14       1.00      1.00      1.00         1\n",
            "          15       1.00      0.08      0.15        12\n",
            "          16       1.00      0.08      0.15        12\n",
            "          17       1.00      0.08      0.15        12\n",
            "          18       1.00      0.09      0.17        11\n",
            "          19       1.00      0.33      0.50         3\n",
            "          20       1.00      0.50      0.67         2\n",
            "          21       1.00      0.33      0.50         3\n",
            "          22       1.00      0.25      0.40         4\n",
            "          23       1.00      0.50      0.67         2\n",
            "          24       1.00      0.33      0.50         3\n",
            "          25       1.00      0.33      0.50         3\n",
            "          26       1.00      0.50      0.67         2\n",
            "          27       1.00      0.08      0.15        12\n",
            "          28       1.00      0.50      0.67         2\n",
            "          29       1.00      0.08      0.15        12\n",
            "          30       1.00      0.50      0.67         2\n",
            "\n",
            "   micro avg       1.00      0.14      0.24       229\n",
            "   macro avg       1.00      0.31      0.40       229\n",
            "weighted avg       1.00      0.14      0.22       229\n",
            " samples avg       1.00      0.19      0.28       229\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJOVvDWSRLZ_"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_dataset' : data_raw, \n",
        "          'tabel_threshold' : d3,\n",
        "          'tabel_groundtruth' : b3,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/wsd/data_wsd.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEubXuvy11cz"
      },
      "source": [
        "# manual\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSZuAmKzSN4J",
        "outputId": "a182437b-e7ce-4bcc-8b9a-6b3e39ece647"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "#define array of actual values\n",
        "y_actual = [\n",
        "1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\n",
        "0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\n",
        "]\n",
        "\n",
        "#define array of predicted values\n",
        "y_predicted = [\n",
        "1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t1,\t0,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\n",
        "0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\n",
        "]\n",
        "\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true= y_actual, y_pred= y_predicted).ravel()\n",
        "\n",
        "print(\"false positif : \",fp)\n",
        "print(\"false negative : \",fn)\n",
        "print(\"true positive : \", tp) \n",
        "print(\"true negative : \", tn)\n",
        "\n",
        "print(\"akurasi\", metrics.accuracy_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"recall\", metrics.recall_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"presion\", metrics.precision_score(y_true= y_actual, y_pred= y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positif :  94\n",
            "false negative :  261\n",
            "true positive :  17\n",
            "true negative :  2958\n",
            "akurasi 0.8933933933933934\n",
            "recall 0.06115107913669065\n",
            "presion 0.15315315315315314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7fKphMkSDQT"
      },
      "source": [
        "d3.reset_index(drop=True, inplace=True)\n",
        "d3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "HyiEUiyyssuo",
        "outputId": "683478af-f6cc-4be0-ddb8-3a8a2885efa5"
      },
      "source": [
        "from pywsd import disambiguate\n",
        "from pywsd.similarity import similarity_by_path\n",
        "\n",
        "file1 = r'/content/drive/MyDrive/dataset/dataset_2.xlsx'\n",
        "dataSRS =  '2004 - colorcast'\n",
        "a = fulldataset(data= file1, inputSRS= dataSRS)\n",
        "list_req1 = list(a['Requirement Statement'])\n",
        "id_req1 = list(a['ID'])\n",
        "cleaned1 = apply_cleaning_function_to_list(list_req1)\n",
        "\n",
        "file2 = r'/content/drive/MyDrive/dataset/dataset_2_split.xlsx'\n",
        "b = fulldataset(data= file2, inputSRS= dataSRS)\n",
        "list_req2 = list(b['Requirement Statement'])\n",
        "id_req2 = list(b['ID'])\n",
        "cleaned2 = apply_cleaning_function_to_list(list_req2)\n",
        "\n",
        "word1 = [disambiguate(x) for x in cleaned1 if x is not None]\n",
        "word1_synset = [[n[1] for n in y if n[1] is not None] for y in word1]\n",
        "word1_kata = [[n[0] for n in y if n[0] is not None] for y in word1]\n",
        "\n",
        "word2 = [disambiguate(x) for x in cleaned2 if x is not None]\n",
        "word2_synset = [[n[1] for n in y if n[1] is not None] for y in word2]\n",
        "word2_kata = [[n[0] for n in y if n[1] is not None] for y in word2]\n",
        "\n",
        "id1 = 7\n",
        "id2 = 19\n",
        "data_list = []\n",
        "for idx, num in zip(word1_kata[id1], word1_synset[id1]):\n",
        "  a = [(similarity_by_path(num, angka, option= \"wup\")) for idy, angka in enumerate(word2_synset[id2])]\n",
        "  data_list.append(a)\n",
        "\n",
        "df_sem = pd.DataFrame(data_list, index= word1_synset[id1], columns= word2_synset[id2])\n",
        "df_sem\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Synset('performance.n.04')</th>\n",
              "      <th>Synset('requirement.n.01')</th>\n",
              "      <th>Synset('transparently.r.02')</th>\n",
              "      <th>Synset('specify.v.03')</th>\n",
              "      <th>Synset('waiter.n.01')</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Synset('coloring_material.n.01')</th>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('search.n.05')</th>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('versatile.s.01')</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('solicitation.n.02')</th>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('specify.v.03')</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.153846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('rudiment.n.01')</th>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('paint.n.01')</th>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.421053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('work.v.05')</th>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('time.n.06')</th>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Synset('waiter.n.01')</th>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.909091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Synset('performance.n.04')  ...  Synset('waiter.n.01')\n",
              "Synset('coloring_material.n.01')                    0.266667  ...               0.285714\n",
              "Synset('search.n.05')                               0.555556  ...               0.111111\n",
              "Synset('versatile.s.01')                            0.000000  ...               0.000000\n",
              "Synset('solicitation.n.02')                         0.285714  ...               0.142857\n",
              "Synset('specify.v.03')                              0.166667  ...               0.153846\n",
              "Synset('rudiment.n.01')                             0.315789  ...               0.105263\n",
              "Synset('paint.n.01')                                0.250000  ...               0.421053\n",
              "Synset('work.v.05')                                 0.153846  ...               0.142857\n",
              "Synset('time.n.06')                                 0.533333  ...               0.133333\n",
              "Synset('waiter.n.01')                               0.125000  ...               0.909091\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 350
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "GjIvmmKO3BH3",
        "outputId": "452060b2-bb52-45c7-fb52-f5fe84084639"
      },
      "source": [
        "import numpy as np\n",
        "A=np.array(data_list)\n",
        "B=np.array(data_list)\n",
        "\n",
        "def csm(A,B,corr):\n",
        "    if corr:\n",
        "        B=B-B.mean(axis=1)[:,np.newaxis]\n",
        "        A=A-A.mean(axis=1)[:,np.newaxis]\n",
        "    num=np.dot(A,B.T)\n",
        "    p1=np.sqrt(np.sum(A**2,axis=1))[:,np.newaxis]\n",
        "    p2=np.sqrt(np.sum(B**2,axis=1))[np.newaxis,:]\n",
        "    return num/(p1*p2)\n",
        "hasil = (csm(A,B,True))\n",
        "pd.DataFrame(hasil, )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.631727</td>\n",
              "      <td>-0.017972</td>\n",
              "      <td>0.831888</td>\n",
              "      <td>-0.907633</td>\n",
              "      <td>0.727712</td>\n",
              "      <td>0.906964</td>\n",
              "      <td>-0.900464</td>\n",
              "      <td>0.668611</td>\n",
              "      <td>0.516050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.631727</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.271781</td>\n",
              "      <td>0.899129</td>\n",
              "      <td>-0.692403</td>\n",
              "      <td>0.986795</td>\n",
              "      <td>0.313025</td>\n",
              "      <td>-0.688082</td>\n",
              "      <td>0.996851</td>\n",
              "      <td>-0.247068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.017972</td>\n",
              "      <td>-0.271781</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.080640</td>\n",
              "      <td>0.435796</td>\n",
              "      <td>-0.164138</td>\n",
              "      <td>-0.142796</td>\n",
              "      <td>0.450967</td>\n",
              "      <td>-0.216747</td>\n",
              "      <td>-0.165967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.831888</td>\n",
              "      <td>0.899129</td>\n",
              "      <td>0.080640</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.722245</td>\n",
              "      <td>0.956656</td>\n",
              "      <td>0.522777</td>\n",
              "      <td>-0.711986</td>\n",
              "      <td>0.927411</td>\n",
              "      <td>-0.046018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.907633</td>\n",
              "      <td>-0.692403</td>\n",
              "      <td>0.435796</td>\n",
              "      <td>-0.722245</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.732829</td>\n",
              "      <td>-0.870899</td>\n",
              "      <td>0.999844</td>\n",
              "      <td>-0.702320</td>\n",
              "      <td>-0.522877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.727712</td>\n",
              "      <td>0.986795</td>\n",
              "      <td>-0.164138</td>\n",
              "      <td>0.956656</td>\n",
              "      <td>-0.732829</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.410333</td>\n",
              "      <td>-0.726457</td>\n",
              "      <td>0.995763</td>\n",
              "      <td>-0.164180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.906964</td>\n",
              "      <td>0.313025</td>\n",
              "      <td>-0.142796</td>\n",
              "      <td>0.522777</td>\n",
              "      <td>-0.870899</td>\n",
              "      <td>0.410333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.868415</td>\n",
              "      <td>0.345110</td>\n",
              "      <td>0.826187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.900464</td>\n",
              "      <td>-0.688082</td>\n",
              "      <td>0.450967</td>\n",
              "      <td>-0.711986</td>\n",
              "      <td>0.999844</td>\n",
              "      <td>-0.726457</td>\n",
              "      <td>-0.868415</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.696956</td>\n",
              "      <td>-0.525939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.668611</td>\n",
              "      <td>0.996851</td>\n",
              "      <td>-0.216747</td>\n",
              "      <td>0.927411</td>\n",
              "      <td>-0.702320</td>\n",
              "      <td>0.995763</td>\n",
              "      <td>0.345110</td>\n",
              "      <td>-0.696956</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.224859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.516050</td>\n",
              "      <td>-0.247068</td>\n",
              "      <td>-0.165967</td>\n",
              "      <td>-0.046018</td>\n",
              "      <td>-0.522877</td>\n",
              "      <td>-0.164180</td>\n",
              "      <td>0.826187</td>\n",
              "      <td>-0.525939</td>\n",
              "      <td>-0.224859</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...         7         8         9\n",
              "0  1.000000  0.631727 -0.017972  ... -0.900464  0.668611  0.516050\n",
              "1  0.631727  1.000000 -0.271781  ... -0.688082  0.996851 -0.247068\n",
              "2 -0.017972 -0.271781  1.000000  ...  0.450967 -0.216747 -0.165967\n",
              "3  0.831888  0.899129  0.080640  ... -0.711986  0.927411 -0.046018\n",
              "4 -0.907633 -0.692403  0.435796  ...  0.999844 -0.702320 -0.522877\n",
              "5  0.727712  0.986795 -0.164138  ... -0.726457  0.995763 -0.164180\n",
              "6  0.906964  0.313025 -0.142796  ... -0.868415  0.345110  0.826187\n",
              "7 -0.900464 -0.688082  0.450967  ...  1.000000 -0.696956 -0.525939\n",
              "8  0.668611  0.996851 -0.216747  ... -0.696956  1.000000 -0.224859\n",
              "9  0.516050 -0.247068 -0.165967  ... -0.525939 -0.224859  1.000000\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWkxNoj5v8UP",
        "outputId": "e36d331d-7ea8-4fd9-dc26-ee5068cc0e8b"
      },
      "source": [
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "WORD = re.compile(r\"\\w+\")\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
        "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def text_to_vector(text):\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "\n",
        "text1 = \"This is a foo bar sentence .\"\n",
        "text2 = \"This sentence is similar to a foo bar sentence .\"\n",
        "\n",
        "vector1 = text_to_vector(text1)\n",
        "vector2 = text_to_vector(text2)\n",
        "print(vector1)\n",
        "print(vector2)\n",
        "cosine = get_cosine(vector1, vector2)\n",
        "\n",
        "print(\"Cosine:\", cosine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'This': 1, 'is': 1, 'a': 1, 'foo': 1, 'bar': 1, 'sentence': 1})\n",
            "Counter({'sentence': 2, 'This': 1, 'is': 1, 'similar': 1, 'to': 1, 'a': 1, 'foo': 1, 'bar': 1})\n",
            "Cosine: 0.8616404368553293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzd7Jjdnef_p",
        "outputId": "ae92c2a6-32ea-4fd5-dea3-08f2d28ae6c8"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "#define array of actual values\n",
        "y_actual = [\n",
        "\n",
        "]\n",
        "\n",
        "#define array of predicted values\n",
        "y_predicted = [\n",
        "1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n",
        "]\n",
        "\n",
        "\n",
        "# tn, fp, fn, tp = confusion_matrix(predict_train,y_train).ravel()\n",
        "tn, fp, fn, tp = confusion_matrix(y_predicted,y_actual).ravel()\n",
        "\n",
        "print(\"True Negatives : \",tn)\n",
        "print(\"False Positives : \",fp)\n",
        "print(\"False Negatives : \", fn) \n",
        "print(\"True Positives : \", tp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Negatives :  1566\n",
            "False Positives :  289\n",
            "False Negatives :  39\n",
            "True Positives :  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd_zQOPG2KWv"
      },
      "source": [
        "list1= set(['product', 'plot', 'data', 'point', 'scientific', 'correct', 'manner', 'grid', 'axis', 'label', 'correct', 'accord', 'input', 'data', 'file',\n",
        "'data', 'point', 'colour', 'accord', 'cluster', 'number', 'contain', 'file', 'single', 'click', 'mouse', 'data', 'point', 'bring', 'name', 'mouse', 'data',\n",
        "'point', 'cause', 'application', 'display', 'detail', 'product', 'allow', 'multiple', 'point', 'click', 'name', 'display', 'product', 'allow', 'grid', 'orient', \n",
        "'user', 'rotation', 'zoom', 'move', 'function', 'employ', 'data' , 'file', 'contain', 'name', 'point', 'parameter', 'plot', 'single', 'designate', 'colour', \n",
        "'attribute', 'used', 'comparison', 'description', 'point', 'large', 'enough', 'see', 'select', 'point', 'big', 'distort', 'overall', 'pattern', 'spread',\n",
        "'axis', 'clearly', 'label', 'easily', 'recognise', 'grid', 'orient', 'different', 'position', 'application', 'colour', 'screen', 'shot', 'print',\n",
        "'clear', 'black', 'white', 'background', 'application', 'intuitive', 'require', 'specialist', 'training', 'program', 'start', 'within', 'second', 'depends', \n",
        "'number', 'data', 'point', 'plotted', 'interaction', 'data', 'point', 'delay', 'long', 'second', 'response', 'change', 'orientation', 'fast', 'enough',\n",
        "'avoid', 'interrupt', 'user', 'flow'])\n",
        "list2 = set(['product', 'allow', 'grid', 'orient', 'user', 'rotation', 'function', 'employed', 'zoom', 'function', 'employed', 'move', 'function', 'employed', 'data',\n",
        "'file', 'contain', 'name', 'data', 'point', 'data', 'file', 'contain', 'parameter', 'data', 'point', 'plot', 'data', 'file', 'contain', 'single', 'parameter',\n",
        "'designate', 'colour', 'point', 'attribute', 'used', 'comparison', 'description', 'data', 'point', 'point', 'large', 'enough', 'see', 'point', 'large', 'enough', \n",
        "'select', 'axis', 'clearly', 'labelled', 'axis', 'easily', 'recognised', 'grid', 'oriented', 'different', 'position', 'application', 'coloured', 'screen',\n",
        "'shot', 'printed', 'clearly', 'black', 'white', 'background', 'application', 'intuitive', 'application', 'require', 'specialist', 'training', 'program', 'start',\n",
        "'within', 'second', 'depends', 'number', 'data', 'point', 'plot'])\n",
        "from pywsd import disambiguate\n",
        "from pywsd.similarity import similarity_by_path\n",
        "\n",
        "word1 = [disambiguate(x) for x in list1]\n",
        "word1_synset = [[n[1] for n in y] for y in word1]\n",
        "word1_kata = [[n[0] for n in y] for y in word1]\n",
        "word2 = [disambiguate(x) for x in list2]\n",
        "word2_synset = [[n[1] for n in y] for y in word2]\n",
        "word2_kata = [[n[0] for n in y] for y in word2]\n",
        "\n",
        "data_list= []\n",
        "for idx, num in zip(word1_kata, word1_synset):\n",
        "  a = [similarity_by_path(num[0], angka[0], option= \"wup\") for idy, angka in zip(word2_kata, word2_synset)]\n",
        "  data_list.append(a)\n",
        "\n",
        "df_list = pd.DataFrame(data_list, index= list1, columns= list2)\n",
        "df_list['product']['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ9rg4RMSLzN"
      },
      "source": [
        "# Another method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyQP_cmYUArF"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "\n",
        "def convert_tag(tag):\n",
        "    \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
        "    \n",
        "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
        "    try:\n",
        "        return tag_dict[tag[0]]\n",
        "    except KeyError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def doc_to_synsets(doc):\n",
        "    \"\"\"\n",
        "    Returns a list of synsets in document.\n",
        "\n",
        "    Tokenizes and tags the words in the document doc.\n",
        "    Then finds the first synset for each word/tag combination.\n",
        "    If a synset is not found for that combination it is skipped.\n",
        "\n",
        "    Args:\n",
        "        doc: string to be converted\n",
        "\n",
        "    Returns:\n",
        "        list of synsets\n",
        "\n",
        "    Example:\n",
        "        doc_to_synsets('Fish are nvqjp friends.')\n",
        "        Out: [Synset('fish.n.01'), Synset('be.v.01'), Synset('friend.n.01')]\n",
        "    \"\"\"\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    pos = nltk.pos_tag(tokens)\n",
        "    tags = [tag[1] for tag in pos]\n",
        "    wntag = [convert_tag(tag) for tag in tags]\n",
        "    ans = list(zip(tokens,wntag))\n",
        "    sets = [wn.synsets(x,y) for x,y in ans]\n",
        "    final = [val[0] for val in sets if len(val) > 0]\n",
        "    \n",
        "    return final\n",
        "\n",
        "\n",
        "def similarity_score(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculate the normalized similarity score of s1 onto s2\n",
        "\n",
        "    For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
        "    Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
        "    number of largest similarity values found.\n",
        "\n",
        "    Args:\n",
        "        s1, s2: list of synsets from doc_to_synsets\n",
        "\n",
        "    Returns:\n",
        "        normalized similarity score of s1 onto s2\n",
        "\n",
        "    Example:\n",
        "        synsets1 = doc_to_synsets('I like cats')\n",
        "        synsets2 = doc_to_synsets('I like dogs')\n",
        "        similarity_score(synsets1, synsets2)\n",
        "        Out: 0.73333333333333339\n",
        "    \"\"\"\n",
        "    s =[]\n",
        "    for i1 in s1:\n",
        "        r = []\n",
        "        scores = [x for x in [i1.path_similarity(i2) for i2 in s2] if x is not None]\n",
        "        if scores:\n",
        "            s.append(max(scores))\n",
        "    return sum(s)/len(s)\n",
        "\n",
        "\n",
        "def document_path_similarity(doc1, doc2):\n",
        "    \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
        "\n",
        "    synsets1 = doc_to_synsets(doc1)\n",
        "    synsets2 = doc_to_synsets(doc2)\n",
        "\n",
        "    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TTvPzyKUCpf",
        "outputId": "72b590ac-61b8-48fb-fd76-75306a0a25ad"
      },
      "source": [
        "doc_to_synsets('Fish are nvqjp friends.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('fish.n.01'), Synset('be.v.01'), Synset('friend.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CrETK83UQKx",
        "outputId": "85fd3cfa-31bc-4a7a-eb82-29b8d3d1ed24"
      },
      "source": [
        "synsets1 = doc_to_synsets('I like cats')\n",
        "synsets2 = doc_to_synsets('I like dogs')\n",
        "similarity_score(synsets1, synsets2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlSGL6jaUnR5",
        "outputId": "c9987286-cfa0-4bcb-e2da-e089f474906f"
      },
      "source": [
        "doc1 = \"I like you\"\n",
        "doc2 = \"I love you\"\n",
        "document_path_similarity(doc1, doc2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWHqlZYGXT-p",
        "outputId": "eb242b50-9df0-41ab-f989-d78e8f34eb3c"
      },
      "source": [
        "def test_document_path_similarity():\n",
        "    doc1 = 'This is a function to test document_path_similarity.'\n",
        "    doc2 = 'Use this function to see if your code in doc_to_synsets \\\n",
        "    and similarity_score is correct!'\n",
        "    return document_path_similarity(doc1, doc2)\n",
        "test_document_path_similarity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.554265873015873"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRaLdClXXZx5"
      },
      "source": [
        "def most_similar_docs():\n",
        "    paraphrases = cleaned1\n",
        "    paraphrases['similarity_score'] = paraphrases.apply(lambda x:document_path_similarity(x['D1'], x['D2']), axis=1)\n",
        "    return (paraphrases.sort_values('similarity_score', ascending=False).iloc[0]['D1'], paraphrases.sort_values('similarity_score', ascending=False).iloc[0]['D2'], paraphrases.sort_values('similarity_score', ascending=False).iloc[0]['similarity_score'])\n",
        "most_similar_docs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l4hCW-EXmRk"
      },
      "source": [
        "def label_accuracy():\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    paraphrases['similarity_score'] = paraphrases.apply(lambda x:document_path_similarity(x['D1'], x['D2']), axis=1)\n",
        "    paraphrases['predicted'] = np.where(paraphrases['similarity_score'] > 0.75, 1, 0)\n",
        "    \n",
        "    return accuracy_score(paraphrases['Quality'], paraphrases['predicted'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}