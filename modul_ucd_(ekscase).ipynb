{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modul_ucd_(ekscase).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3LbIYvnsxNiA",
        "RkW-XQc8nYfJ",
        "QcODID7GGsG5"
      ],
      "authorship_tag": "ABX9TyOYL8ROTStAvJi8C6KkUPCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyrofist/Extraction-Requirement/blob/main/modul_ucd_(ekscase).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-dPqTpoUkl",
        "outputId": "932a10b4-7f77-449b-d3f2-ab63b09ce759"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')\n",
        "%cd /content/mydrive"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mydrive\n",
            "/content/mydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pbz2pC4L1si"
      },
      "source": [
        "!pip install -U pywsd\n",
        "!pip install XlsxWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWlrIX8MAH2"
      },
      "source": [
        "!pip install -U wn==0.0.23"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LbIYvnsxNiA"
      },
      "source": [
        "# Modul1: xmlparser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ADmce0anTnm",
        "outputId": "1d98084b-7adb-44ba-ee09-69b0b43e8767"
      },
      "source": [
        "# function\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# template class xmlparser\n",
        "class xmlParser:\n",
        "\n",
        "    # inisialisasi\n",
        "    def __init__(self, filename= '/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/IRCI_Researcher.xmi', \n",
        "                 tipe_xmi= '{http://schema.omg.org/spec/XMI/2.1}type',\n",
        "                 id_xmi= '{http://schema.omg.org/spec/XMI/2.1}id'):\n",
        "    \tself.namaFile = filename\n",
        "    \tself.xmi_type = tipe_xmi\n",
        "    \tself.xmi_id = id_xmi\n",
        "\n",
        "    def data_root(self):\n",
        "        tree = ET.parse(self.namaFile)\n",
        "        root = tree.getroot()\n",
        "        return root\n",
        "\n",
        "    #fungsi parse tree elemen\n",
        "    def elemenTreeParse(self):\n",
        "      try: \n",
        "        elemenTag = [elem.tag for elem in xmlParser.data_root(self).iter()]\n",
        "        elemenAtribut = [elem.attrib for elem in root.iter()]\n",
        "        tabelElemen = pd.DataFrame([elemenTag, elemenAtribut], index=['Berdasarkan Tag', 'Berdsarkan Atribut']).T\n",
        "        return tabelElemen\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    # fungsi pencarian elemen\n",
        "    def cariTreeElemen(self, elemen):\n",
        "      try:\n",
        "        pencarian = [num.findall(elemen) for num in xmlParser.data_root(self).iter()]\n",
        "        return pencarian\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    #fungsi list elemen\n",
        "    def listElemen(self, elemen):\n",
        "      try:\n",
        "        listElemen = [berdasarkanOwnEnd.attrib for berdasarkanOwnEnd in xmlParser.data_root(self).iter(elemen)]\n",
        "        tabelElement = pd.DataFrame(listElemen)\n",
        "        return tabelElement\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    # fungsi mencari table spesifik\n",
        "    def tableElemenSpesifik(self, elemen= 'packagedElement', kolom1= 'name'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        berdasarkanPackagedELement = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(elemen)]\n",
        "        for num in berdasarkanPackagedELement:\n",
        "          a1 = num[kolom1]\n",
        "          c1 = num[self.xmi_type]\n",
        "          d1 = num[self.xmi_id]\n",
        "          hasil.append([a1, c1, d1])\n",
        "        cleanPackagedELement = pd.DataFrame(hasil, columns=[kolom1, self.xmi_type, self.xmi_id])\n",
        "        return cleanPackagedELement\n",
        "        \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    # fungsi mencari string\n",
        "    def doString(self):\n",
        "      try:\n",
        "        print(ET.tostring(xmlParser.data_root(self), encoding='utf8').decode('utf8'))\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def dataPaketElemen(self, category = 'packagedElement'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        berdasarkanPackagedELement = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in berdasarkanPackagedELement:\n",
        "          a1 = num[self.xmi_id]\n",
        "          b1 = num['name']\n",
        "          d1 = num[self.xmi_type]\n",
        "          hasil.append([a1, b1, d1])\n",
        "\n",
        "        paketElemen = pd.DataFrame(hasil, columns=['id', 'name', 'type'])\n",
        "        return paketElemen\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def dataExtend(self, category = 'extend'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        berdasarkanExtend = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in berdasarkanExtend:\n",
        "          a1 = num[self.xmi_id]\n",
        "          b1 = num[self.xmi_type]\n",
        "          c1 = num['extendedCase']\n",
        "          d1 = paketElemen[paketElemen['id'] == c1].iloc[0]['name']\n",
        "          e1 = num['extension']\n",
        "          f1 = paketElemen[paketElemen['id'] == e1].iloc[0]['name']\n",
        "          hasil.append([a1, b1, c1, d1, e1, f1])\n",
        "          \n",
        "        extendTable = pd.DataFrame(hasil, columns=['id', 'type', 'source', 'sourceName', 'destination', 'destinationName'])\n",
        "        return extendTable\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def dataInclude(self, category = 'include'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        byinclude = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in byinclude:\n",
        "          a1 = num['{http://schema.omg.org/spec/XMI/2.1}id']\n",
        "          b1 = num['{http://schema.omg.org/spec/XMI/2.1}type']\n",
        "          c1 = num['includingCase']\n",
        "          d1 = paketElemen[paketElemen['id'] == c1].iloc[0]['name']\n",
        "          e1 = num['addition']\n",
        "          f1 = paketElemen[paketElemen['id'] == e1].iloc[0]['name']\n",
        "          hasil.append([a1, b1, c1, d1, e1, f1])\n",
        "        includeTable = pd.DataFrame(hasil, columns= ['id', 'tipe', 'include', 'includeName', 'addition', 'additionName'])\n",
        "        return includeTable        \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def dataOwnedEnd(self, category = 'ownedEnd'):\n",
        "      try:\n",
        "        # berdasarkan ownedEnd\n",
        "        hasil = []\n",
        "        berdasarkanOwnedEnd = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        berdasarkanOwnedEnd\n",
        "        for num in berdasarkanOwnedEnd:\n",
        "          a1 = num['type']\n",
        "          b1 = num[self.xmi_id]\n",
        "          c1 = num[self.xmi_type]\n",
        "          d1 = paketElemen[paketElemen['id'] == a1].iloc[0]['name']\n",
        "          hasil.append([a1, b1, c1, d1])\n",
        "          \n",
        "        ownedEndTable = pd.DataFrame(hasil, columns=['id_data', 'id_property', 'type_property', 'id_name'])\n",
        "        return ownedEndTable\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def dataOwnedMember(self, category = 'ownedMember'):\n",
        "      try:\n",
        "        # berdasarkan UML Model\n",
        "        hasilNum = []\n",
        "        berdasarkanOwnedMember = [packagedElement for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in berdasarkanOwnedMember:\n",
        "          a = num.attrib[self.xmi_id]\n",
        "          b = num.attrib[self.xmi_type]\n",
        "          for index, angka in enumerate(num.iter('ownedEnd')):\n",
        "            if index == 0:\n",
        "              c = paketElemen[paketElemen['id'] == angka.attrib['type']].iloc[0]['name']\n",
        "            else:\n",
        "              d = paketElemen[paketElemen['id'] == angka.attrib['type']].iloc[0]['name']\n",
        "          hasilNum.append([a, b, c, d])\n",
        "\n",
        "        ownedMemberTable = pd.DataFrame(hasilNum, columns=['id', 'type_property', 'actor', 'usecase'])\n",
        "        return ownedMemberTable  \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def __del__(self):\n",
        "        print ('Destructor called.')    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      # myXmlParser = xmlParser(filename= '/content/mydrive/MyDrive/dataset/IRCI_V2/topic/IRCI_Topic.xmi')\n",
        "      # myXmlParser = xmlParser(filename= '/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/IRCI_Researcher.xmi')\n",
        "      # myXmlParser = xmlParser(filename= '/content/mydrive/MyDrive/dataset/RAnalyzer/RAnalyzer-DependencyUC/rAnalyzerUC.xmi')\n",
        "      myXmlParser = xmlParser()\n",
        "      paketElemen = myXmlParser.dataPaketElemen()\n",
        "      extendTable = myXmlParser.dataExtend()\n",
        "      ownedEndTable = myXmlParser.dataOwnedEnd()\n",
        "      ownedMemberTable = myXmlParser.dataOwnedMember()\n",
        "  \n",
        "      \"\"\"# Modul 1\n",
        "      Parsing file xmi menjadi tabel2 (daftar aktor, daftar use case, dan relasi antara actor use case dan antar use case)\n",
        "      \"\"\"\n",
        "      print(\"actorTable\")\n",
        "      actorTable = paketElemen[paketElemen['type'] == 'uml:Actor']\n",
        "      print(tabulate(actorTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\nuseCaseTable\")\n",
        "      useCaseTable = paketElemen[paketElemen['type'] == 'uml:UseCase']\n",
        "      print(tabulate(useCaseTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\nextendTable\")\n",
        "      print(tabulate(extendTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "\n",
        "      print(\"\\nassociationTable\")\n",
        "      print(tabulate(ownedMemberTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\npropertyTable\")\n",
        "      print(tabulate(ownedEndTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      #untuk extend - data researcher dan topic\n",
        "      hasilAktor = []\n",
        "      hasilDestinasi = []\n",
        "\n",
        "      for idx, num in enumerate(extendTable.sourceName):\n",
        "        c = ownedMemberTable[ownedMemberTable['usecase'] == extendTable.sourceName[idx]]\n",
        "        if len(c) > 0:\n",
        "          for aktor in c.actor:\n",
        "            hasilAktor.append(aktor)\n",
        "            hasilDestinasi.append(extendTable.destinationName[idx])\n",
        "        else:\n",
        "          temp = 2\n",
        "          d = ownedMemberTable[ownedMemberTable['usecase'] == extendTable.sourceName[idx-temp]]\n",
        "          for dAktor in d.actor:\n",
        "            hasilAktor.append(dAktor)\n",
        "            hasilDestinasi.append(extendTable.destinationName[idx])\n",
        "\n",
        "      df_a = pd.DataFrame([hasilAktor, hasilDestinasi], index= ['actor', 'action']).T\n",
        "      df_a['actor'] = df_a.groupby(['action'])['actor'].transform(lambda x: ';'.join(x))\n",
        "      df_a = df_a[['action','actor']].drop_duplicates()\n",
        "      df_a['actor'][0] = set(df_a['actor'][0].split(\";\")) # fungsi ini digunakan untuk menyempurnakan format\n",
        "      df_a['actor'][0] = \";\".join(df_a['actor'][0])\n",
        "      ownedMemberTable.rename(columns = {'usecase':'action'}, inplace = True)\n",
        "      dt_b = pd.concat([df_a, ownedMemberTable])\n",
        "      dt_actor_action = dt_b.drop(['id', 'type_property'], axis= 1)\n",
        "      dt_actor_action['actor'] = dt_actor_action.groupby(['action'])['actor'].transform(lambda x: ';'.join(x))\n",
        "      dt_actor_action = dt_actor_action[['action','actor']].drop_duplicates()\n",
        "      print(\"\\nactorActionTable\")\n",
        "      print(tabulate(dt_actor_action, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # print(\"\\nincludeTable\")\n",
        "      # print(tabulate(includeTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # # untuk include  data ranalyzer\n",
        "      # hasilAktor = []\n",
        "      # hasilDestinasi = []\n",
        "      # for idy, angka in enumerate(includeTable.includeName):\n",
        "      #   f = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy]]\n",
        "      #   if len(f) > 0:\n",
        "      #     for aktor in f.actor:\n",
        "      #       hasilAktor.append(aktor)\n",
        "      #       hasilDestinasi.append(includeTable.additionName[idy])\n",
        "      #   else:\n",
        "      #     tempY = 2\n",
        "      #     g = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy-tempY]]\n",
        "      #     for dAktor in g.actor:\n",
        "      #       hasilAktor.append(dAktor)\n",
        "      #       hasilDestinasi.append(includeTable.additionName[idy])\n",
        "\n",
        "      # df_a = pd.DataFrame([hasilAktor, hasilDestinasi], index= ['actor', 'action']).T\n",
        "      # df_a['actor'] = df_a.groupby(['action'])['actor'].transform(lambda x: ';'.join(x))\n",
        "      # df_a = df_a[['action','actor']].drop_duplicates()\n",
        "      # df_a['actor'][0] = set(df_a['actor'][0].split(\";\")) # fungsi ini digunakan untuk menyempurnakan format\n",
        "      # df_a['actor'][0] = \";\".join(df_a['actor'][0])\n",
        "      # ownedMemberTable.rename(columns = {'usecase':'action'}, inplace = True)\n",
        "      # dt_b = pd.concat([df_a, ownedMemberTable])\n",
        "      # dt_actor_action = dt_b.drop(['id', 'type_property'], axis= 1)\n",
        "      # print(\"\\nactorActionTable\")\n",
        "      # print(tabulate(dt_actor_action, headers = 'keys', tablefmt = 'psql'))      \n",
        "\n",
        "      myXmlParser.__del__()\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actorTable\n",
            "+----+----------------------+-----------+-----------+\n",
            "|    | id                   | name      | type      |\n",
            "|----+----------------------+-----------+-----------|\n",
            "|  1 | AAAAAAF4mi7mgDh1TSM= | Submitter | uml:Actor |\n",
            "|  4 | AAAAAAF4mi99ijj8BCY= | Viewer    | uml:Actor |\n",
            "+----+----------------------+-----------+-----------+\n",
            "\n",
            "useCaseTable\n",
            "+----+----------------------+------------------------+-------------+\n",
            "|    | id                   | name                   | type        |\n",
            "|----+----------------------+------------------------+-------------|\n",
            "|  2 | AAAAAAF4mi8PyDifi1c= | insertMetadata         | uml:UseCase |\n",
            "|  3 | AAAAAAF4mi8ydjjN568= | searchResearcher       | uml:UseCase |\n",
            "|  5 | AAAAAAF4mjIw1zvIN8o= | searchArticle          | uml:UseCase |\n",
            "|  6 | AAAAAAF4mjJ07jyH6c4= | viewNextResult         | uml:UseCase |\n",
            "|  7 | AAAAAAF4mjPvZT9ass0= | orderByRelevancy       | uml:UseCase |\n",
            "|  8 | AAAAAAF4mjRfaEAQ9os= | orderByScore           | uml:UseCase |\n",
            "|  9 | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher | uml:UseCase |\n",
            "| 10 | AAAAAAF4mjURoUGOMMc= | editProfile            | uml:UseCase |\n",
            "| 11 | AAAAAAF4mjVMjEPi+Lg= | removeArticle          | uml:UseCase |\n",
            "+----+----------------------+------------------------+-------------+\n",
            "\n",
            "extendTable\n",
            "+----+----------------------+------------+----------------------+------------------------+----------------------+------------------------+\n",
            "|    | id                   | type       | source               | sourceName             | destination          | destinationName        |\n",
            "|----+----------------------+------------+----------------------+------------------------+----------------------+------------------------|\n",
            "|  0 | AAAAAAF4mjK8bj1PVZQ= | uml:Extend | AAAAAAF4mi8PyDifi1c= | insertMetadata         | AAAAAAF4mjIw1zvIN8o= | searchArticle          |\n",
            "|  1 | AAAAAAF4mjMMwD4V6+M= | uml:Extend | AAAAAAF4mi8PyDifi1c= | insertMetadata         | AAAAAAF4mjIw1zvIN8o= | searchArticle          |\n",
            "|  2 | AAAAAAF4mjNY8D6c2V4= | uml:Extend | AAAAAAF4mjIw1zvIN8o= | searchArticle          | AAAAAAF4mjJ07jyH6c4= | viewNextResult         |\n",
            "|  3 | AAAAAAF4mjXE8EYDP8I= | uml:Extend | AAAAAAF4mi8ydjjN568= | searchResearcher       | AAAAAAF4mjPvZT9ass0= | orderByRelevancy       |\n",
            "|  4 | AAAAAAF4mjXqS0Z4De0= | uml:Extend | AAAAAAF4mi8ydjjN568= | searchResearcher       | AAAAAAF4mjRfaEAQ9os= | orderByScore           |\n",
            "|  5 | AAAAAAF4mjZD3EjDQ6w= | uml:Extend | AAAAAAF4mi8ydjjN568= | searchResearcher       | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher |\n",
            "|  6 | AAAAAAF4mjcNv0mI9L0= | uml:Extend | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher | AAAAAAF4mjURoUGOMMc= | editProfile            |\n",
            "|  7 | AAAAAAF4mjbxIUlKgbk= | uml:Extend | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher | AAAAAAF4mjVMjEPi+Lg= | removeArticle          |\n",
            "+----+----------------------+------------+----------------------+------------------------+----------------------+------------------------+\n",
            "\n",
            "associationTable\n",
            "+----+----------------------+-----------------+-----------+------------------+\n",
            "|    | id                   | type_property   | actor     | usecase          |\n",
            "|----+----------------------+-----------------+-----------+------------------|\n",
            "|  0 | AAAAAAF4mjEd2znxW70= | uml:Association | Submitter | insertMetadata   |\n",
            "|  1 | AAAAAAF4mjFZVDo6ugo= | uml:Association | Submitter | searchResearcher |\n",
            "|  2 | AAAAAAF4mjGFvTqMa1Y= | uml:Association | Viewer    | searchResearcher |\n",
            "+----+----------------------+-----------------+-----------+------------------+\n",
            "\n",
            "propertyTable\n",
            "+----+----------------------+----------------------+-----------------+------------------+\n",
            "|    | id_data              | id_property          | type_property   | id_name          |\n",
            "|----+----------------------+----------------------+-----------------+------------------|\n",
            "|  0 | AAAAAAF4mi7mgDh1TSM= | AAAAAAF4mjEd2znyn70= | uml:Property    | Submitter        |\n",
            "|  1 | AAAAAAF4mi8PyDifi1c= | AAAAAAF4mjEd2znz9JY= | uml:Property    | insertMetadata   |\n",
            "|  2 | AAAAAAF4mi7mgDh1TSM= | AAAAAAF4mjFZVDo7nEA= | uml:Property    | Submitter        |\n",
            "|  3 | AAAAAAF4mi8ydjjN568= | AAAAAAF4mjFZVDo8Jp8= | uml:Property    | searchResearcher |\n",
            "|  4 | AAAAAAF4mi99ijj8BCY= | AAAAAAF4mjGFvTqNVrk= | uml:Property    | Viewer           |\n",
            "|  5 | AAAAAAF4mi8ydjjN568= | AAAAAAF4mjGFvTqOglI= | uml:Property    | searchResearcher |\n",
            "+----+----------------------+----------------------+-----------------+------------------+\n",
            "\n",
            "actorActionTable\n",
            "+----+------------------------+------------------+\n",
            "|    | action                 | actor            |\n",
            "|----+------------------------+------------------|\n",
            "|  0 | searchArticle          | Submitter        |\n",
            "|  2 | viewNextResult         | Submitter        |\n",
            "|  3 | orderByRelevancy       | Submitter;Viewer |\n",
            "|  5 | orderByScore           | Submitter;Viewer |\n",
            "|  7 | viewDetailOfResearcher | Submitter;Viewer |\n",
            "|  9 | editProfile            | Submitter;Viewer |\n",
            "| 11 | removeArticle          | Submitter;Viewer |\n",
            "|  0 | insertMetadata         | Submitter        |\n",
            "|  1 | searchResearcher       | Submitter;Viewer |\n",
            "+----+------------------------+------------------+\n",
            "Destructor called.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xesxursq2LCd"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_aktor' : actorTable, \n",
        "          'tabel_usecase' : useCaseTable,\n",
        "          'tabel_relasi' : extendTable,\n",
        "          'tabel_asosisasi' : ownedMemberTable,\n",
        "          'tabel_properti' : ownedEndTable,\n",
        "          'tabel_aktor_action' : df_a,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/data_xmi.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D_Q4akInoIY"
      },
      "source": [
        "# Modul2: parsing aksi dan aktor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXOTKqbSnlnT",
        "outputId": "6a3a12b7-bf66-4baa-b394-559dfa50db46"
      },
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "# template class parsingRequirement\n",
        "class parsingRequirement:\n",
        "\n",
        "    # inisialisasi\n",
        "    def __init__(self, filename):\n",
        "    \tself.namaFile = filename\n",
        "      \n",
        "    #fungsi parse tree elemen\n",
        "    def membacaCSV(self):\n",
        "      try: \n",
        "        modul_pembacaan = pd.read_csv(self.namaFile, delimiter= ',')\n",
        "        return modul_pembacaan\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    # cleaning text\n",
        "    def apply_cleaning_function_to_list(self, X):\n",
        "      try:\n",
        "        cleaned_X = []\n",
        "        for element in X:\n",
        "            cleaned_X.append(parsingRequirement.clean_text(self, raw_text= element))\n",
        "        return cleaned_X\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def clean_text(self, raw_text):\n",
        "      try:\n",
        "\n",
        "        # using the .is_stop flag\n",
        "        nlp = English()\n",
        "        tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "        tokens = tokenizer(raw_text)\n",
        "        lemma_list = [token.lemma_.lower() for token in tokens if token.is_stop is False and token.is_punct is False and token.is_alpha is True]\n",
        "        joined_words = ( \" \".join(lemma_list))\n",
        "        return joined_words\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def data_raw(self, data): # get data raw\n",
        "        data_num2 = []\n",
        "        for num in data.fillna(\"empty\"):\n",
        "          for num1 in num.split(\";\"):\n",
        "            for num2 in num1.split(\".\"):\n",
        "              if 'Submitter' in num2:\n",
        "                data_num2.append(num2)\n",
        "              if 'Viewer' in num2:\n",
        "                data_num2.append(num2)\n",
        "              elif 'system' in num2:\n",
        "                data_num2.append(num2)\n",
        "              elif 'actor' in num2:\n",
        "                data_num2.append(num2)\n",
        "              elif 'empty' in num2:\n",
        "                data_num2.append(num2)\n",
        "        return data_num2        \n",
        "\n",
        "    def aksi_aktor(self, data): # get data aksi dan aktor\n",
        "      try:\n",
        "        nlp = English()\n",
        "        tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "        tokens = tokenizer(data)\n",
        "        a = [token.text for token in tokens]\n",
        "        b = [x for x in a if x == 'submitter' or x == 'viewer' or x == 'system' or x == 'actor']  \n",
        "        b1 = \";\".join(b)\n",
        "        b1 = b1.replace(\"actor\", \"submitter; viewer\")\n",
        "        c = [x for x in a if x != 'submitter' and x != 'viewer' and x != 'system' and x != 'actor']  \n",
        "        c1 = \" \".join(c)\n",
        "        return b1, c1        \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "        print ('Destructor called.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "\n",
        "    # parsing functional\n",
        "    MyParsingRequirement = parsingRequirement(filename= \"/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/freqs_researcher.txt\")\n",
        "    freqs = MyParsingRequirement.membacaCSV()\n",
        "    data_freqs = MyParsingRequirement.data_raw(freqs.requirement)\n",
        "\n",
        "    # pembersihan data\n",
        "    freq_requirement = freqs.requirement\n",
        "    id_freq_requirement = freqs.id\n",
        "    text_to_clean_freq = list(freq_requirement)\n",
        "    cleaned_freq = MyParsingRequirement.apply_cleaning_function_to_list(text_to_clean_freq)\n",
        "\n",
        "    data_aktor = []\n",
        "    data_aksi = []\n",
        "    for num in cleaned_freq:\n",
        "      dt_aksi_aktor = MyParsingRequirement.aksi_aktor(num)\n",
        "      data_aktor.append(dt_aksi_aktor[0])\n",
        "      data_aksi.append(dt_aksi_aktor[1])\n",
        "\n",
        "    freqs['aksi'] = data_aksi\n",
        "    freqs['aktor'] = data_aktor\n",
        "    print(\"\\nfreqs\")\n",
        "    print(tabulate(freqs, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "    # parsing ucd1\n",
        "    MyParsingRequirement = parsingRequirement(filename= \"/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/ucs/insert_metadata.txt\")\n",
        "    ucd1 = MyParsingRequirement.membacaCSV()\n",
        "    data_ucd1 = MyParsingRequirement.data_raw(ucd1.flowOfEvents)\n",
        "\n",
        "    list_index= [(\"data{}\".format(idx)) for idx, num in enumerate(data_ucd1)]\n",
        "    data_list = pd.DataFrame(data_ucd1, index= list_index)\n",
        "    data_list = data_list.drop(index= \"data5\").reset_index().drop(labels= ['index'], axis= 1)\n",
        "    ucd1['aksi'] = data_list\n",
        "\n",
        "    ucd1_req = ucd1.aksi\n",
        "    id_ucd1_req = ucd1.id\n",
        "    text_to_clean_ucd1 = list(ucd1_req)\n",
        "    cleaned1_ucd = MyParsingRequirement.apply_cleaning_function_to_list(text_to_clean_ucd1)\n",
        "\n",
        "    data1_aktor = []\n",
        "    data1_aksi = []\n",
        "    for num in cleaned1_ucd:\n",
        "      dt_aksi_aktor = MyParsingRequirement.aksi_aktor(num)\n",
        "      data1_aktor.append(dt_aksi_aktor[0])\n",
        "      data1_aksi.append(dt_aksi_aktor[1])\n",
        "\n",
        "    ucd1['aksi'] = data1_aksi\n",
        "    ucd1['aktor'] = data1_aktor\n",
        "    print(\"\\nucd1\")\n",
        "    print(tabulate(ucd1, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "    # parsing ucd2\n",
        "    MyParsingRequirement = parsingRequirement(filename= \"/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/ucs/search_researcher.txt\")\n",
        "    ucd2 = MyParsingRequirement.membacaCSV()\n",
        "    data_ucd2 = MyParsingRequirement.data_raw(ucd2.flowOfEvents)\n",
        "\n",
        "    list2_index= [(\"data{}\".format(idx)) for idx, num in enumerate(data_ucd2)]\n",
        "    data2_list = pd.DataFrame(data_ucd2, index= list2_index)\n",
        "    data2_list = data2_list.reset_index().drop(labels= ['index'], axis= 1)\n",
        "    ucd2['aksi'] = data2_list\n",
        "\n",
        "    ucd2_req = ucd2.aksi\n",
        "    id_ucd2_req = ucd2.id\n",
        "    text_to_clean_ = list(ucd2_req)\n",
        "    cleaned2_ucd = MyParsingRequirement.apply_cleaning_function_to_list(text_to_clean_)\n",
        "\n",
        "    data2_aktor = []\n",
        "    data2_aksi = []\n",
        "    for num in cleaned2_ucd:\n",
        "      dt_aksi_aktor = MyParsingRequirement.aksi_aktor(num)\n",
        "      data2_aktor.append(dt_aksi_aktor[0])\n",
        "      data2_aksi.append(dt_aksi_aktor[1])\n",
        "\n",
        "    ucd2['aksi'] = data2_aksi\n",
        "    ucd2['aktor'] = data2_aktor\n",
        "    print(\"\\nucd2\")\n",
        "    print(tabulate(ucd2, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructor called.\n",
            "\n",
            "freqs\n",
            "+----+------+---------------------------------------------------------------------------+--------------------------------------+------------------+\n",
            "|    | id   | requirement                                                               | aksi                                 | aktor            |\n",
            "|----+------+---------------------------------------------------------------------------+--------------------------------------+------------------|\n",
            "|  0 | F01  | Submitter can insert metadata of article                                  | insert metadata article              | submitter        |\n",
            "|  1 | F02  | Submitter or Viewer can search researchers which are relevant to keyword  | search researchers relevant keyword  | submitter;viewer |\n",
            "|  2 | F03  | Submitter or Viewer can sort the researchers start from the highest score | sort researchers start highest score | submitter;viewer |\n",
            "|  3 | F04  | Submitter or Viewer can sort the article starts from the relevancy        | sort article starts relevancy        | submitter;viewer |\n",
            "|  4 | F05  | Submitter or viewer can view the detail of a researcher profile           | view detail researcher profile       | submitter;viewer |\n",
            "|  5 | F06  | Submitter can remove an article from his/her profile                      | remove article profile               | submitter        |\n",
            "|  6 | F07  | Submitter can edit his/ her profile                                       | edit profile                         | submitter        |\n",
            "|  7 | F08  | The system can show a progress bar                                        | progress bar                         | system           |\n",
            "+----+------+---------------------------------------------------------------------------+--------------------------------------+------------------+\n",
            "Destructor called.\n",
            "\n",
            "ucd1\n",
            "+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-----------+\n",
            "|    | id                | usecase        | flowOfEvents                                                                                                        | aksi                                               | aktor     |\n",
            "|----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-----------|\n",
            "|  0 | basic_flow        | nan            | nan                                                                                                                 |                                                    |           |\n",
            "|  1 | 1                 | insertMetadata | In the main page; Submitter clicks on insert metadata menu.                                                         | clicks insert metadata menu                        | submitter |\n",
            "|  2 | 2                 | insertMetadata | The system shows a form for inserting metadata.                                                                     | shows form inserting metadata                      | system    |\n",
            "|  3 | 3                 | insertMetadata | Submitter fills in the metadata field with metadata of article(s); then clicks on submit button.                    | fills metadata field metadata                      | submitter |\n",
            "|  4 | 4                 | insertMetadata | The system shows the progress information. After it is done; the system shows a message: “Submission is completed.” | shows progress information                         | system    |\n",
            "|  5 | 5                 | insertMetadata | The system returns to the main page.                                                                                | returns main page                                  | system    |\n",
            "|  6 | alternative_flows | nan            | nan                                                                                                                 |                                                    |           |\n",
            "|  7 | extensionPoints   | nan            | nan                                                                                                                 |                                                    |           |\n",
            "|  8 | 3b                | searchArticle  | Submitter can add metadata by Search an existing Article based on a keyword.                                        | add metadata search existing article based keyword | submitter |\n",
            "|  9 | 3b.1              | searchArticle  | Submitter enter a keyword and clicks Search Article button.                                                         | enter keyword clicks search article button         | submitter |\n",
            "| 10 | 3b.2              | searchArticle  | The system shows the progress information. After it is done it shows a result (a list of relevant articles).        | shows progress information                         | system    |\n",
            "| 11 | 3b.4              | searchArticle  | Submitter click an Add button on a relevant article.                                                                | click add button relevant article                  | submitter |\n",
            "| 12 | 3b.5              | searchArticle  | The system shows the metadata of the relevant article.                                                              | shows metadata relevant article                    | system    |\n",
            "| 13 | 3b.2a             | viewNextResult | The Submitter can View to the Nextpage of the Result.                                                               | view nextpage result                               | submitter |\n",
            "| 14 | 4b.2a.1           | viewNextResult | The Submitter clicks on Next button to move to the next page.                                                       | clicks button page                                 | submitter |\n",
            "| 15 | 4b.2a.2           | viewNextResult | The system shows the next result.                                                                                   | shows result                                       | system    |\n",
            "+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-----------+\n",
            "Destructor called.\n",
            "\n",
            "ucd2\n",
            "+----+------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+-------------------+\n",
            "|    | id               | usecase              | flowOfEvents                                                                                                                   | aksi                                                                      | aktor             |\n",
            "|----+------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+-------------------|\n",
            "|  0 | basic_flow       | nan                  | nan                                                                                                                            |                                                                           |                   |\n",
            "|  1 | 1                | searchResearcher     | The actor enters a keyword on the search field and clicks on the Search Button.                                                | enters keyword search field clicks search button                          | submitter; viewer |\n",
            "|  2 | 2                | searchResearcher     | The system shows a progress bar.                                                                                               | shows progress bar                                                        | system            |\n",
            "|  3 | 3                | searchResearcher     | After it has finished searching; the system views the list of researchers which are relevant to the keyword being entered.     | views list researchers relevant keyword entered                           | system            |\n",
            "|  4 | alternativeFlow  | nan                  | nan                                                                                                                            |                                                                           |                   |\n",
            "|  5 | extension_points | nan                  | nan                                                                                                                            |                                                                           |                   |\n",
            "|  6 | 3a               | orderByRelevancy     | The actor can change the view of search result based on Order by Relevancy of the researchers to the given keyword.            | change view search result based order relevancy researchers given keyword | submitter; viewer |\n",
            "|  7 | 3a.1             | orderByRelevancy     | The actor clicks on Relevant menu                                                                                              | clicks relevant menu                                                      | submitter; viewer |\n",
            "|  8 | 3a.2             | orderByRelevancy     | The system refreshes the view and order the researchers based on their relevancy.                                              | refreshes view order researchers based relevancy                          | system            |\n",
            "|  9 | 3b               | orderByScore         | The actor can change the view of articles based on Order by Score of researchers.                                              | change view articles based order score researchers                        | submitter; viewer |\n",
            "| 10 | 3b.1             | orderByScore         | The actor clicks on Score menu                                                                                                 | clicks score menu                                                         | submitter; viewer |\n",
            "| 11 | 3b.2             | orderByScore         | The system refreshes the view and order the researchers based on their scores.                                                 | refreshes view order researchers based scores                             | system            |\n",
            "| 12 | 3c               | viewDetailResearcher | The actor can View Detail Researcher of profile.                                                                               | view detail researcher profile                                            | submitter; viewer |\n",
            "| 13 | 3c.1             | viewDetailResearcher | The Submitter clicks on the name of the researcher.                                                                            | clicks researcher                                                         | submitter         |\n",
            "| 14 | 3c.2             | viewDetailResearcher | The system shows the profile and list of his/ her published articles.                                                          | shows profile list published articles                                     | system            |\n",
            "| 15 | 3d.2a            | removeArticle        | The Submitter remove his Article.                                                                                              | remove article                                                            | submitter         |\n",
            "| 16 | 3d.2a.1          | removeArticle        | The Submitter clicks on Remove button of the article.                                                                          | clicks remove button article                                              | submitter         |\n",
            "| 17 | 3d.2a.2          | removeArticle        | The system reshows the profile and list of his/ her published articles.                                                        | reshows profile list published articles                                   | system            |\n",
            "| 18 | 3d.2b            | editProfile          | The Submitter Edit his/ her Profile.                                                                                           | edit profile                                                              | submitter         |\n",
            "| 19 | 3d.2b.1          | editProfile          | The Submitter clicks on Edit Profile button.                                                                                   | clicks edit profile button                                                | submitter         |\n",
            "| 20 | 3d.2b.2          | editProfile          | The system shows the profile form.                                                                                             | shows profile form                                                        | system            |\n",
            "| 21 | 3d.2b.2          | editProfile          | The Submitter edit a field; and click Save button.                                                                             | edit field                                                                | submitter         |\n",
            "| 22 | 3d.2b.2          | editProfile          | The system shows a progress bar. After it is finished; the system reshows the profile and list of his/ her published articles. | shows progress bar                                                        | system            |\n",
            "+----+------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+-------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC7M0Qd26sf9"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_freqs' : freqs, \n",
        "          'tabel_ucd1' : ucd1,\n",
        "          'tabel_ucd2' : ucd2,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/data_aksi_aktor.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkW-XQc8nYfJ"
      },
      "source": [
        "# Modul3: pencarian relasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYR7YfJnvkXT",
        "outputId": "331902c2-cb8a-4cd5-832d-9c49d36e6458"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETzqGKt_nXVU",
        "outputId": "511e766a-dc53-4c99-bede-a3317aad0fb3"
      },
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pywsd import disambiguate\n",
        "from pywsd.similarity import max_similarity as maxsim\n",
        "from pywsd.cosine import cosine_similarity\n",
        "\n",
        "\n",
        "# template class ucdReq\n",
        "class ucdReq:\n",
        "\n",
        "  #inicsialisasi\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def useCaseWSDStopwords(self, keyword, id_keyword):\n",
        "    word_stopwords = [disambiguate(x) for x in self.keyword]\n",
        "    b = [len(word_tokenize(num)) for num in self.keyword]\n",
        "    c = max(b)\n",
        "    list_kolom = [\"data{}\".format(x) for x in range(0,c)]\n",
        "    word_synset_stopwords = [[n[1] for n in y] for y in word_stopwords]\n",
        "    hasilUcd_stopwords = pd.DataFrame(word_synset_stopwords, index= self.id_keyword, columns= list_kolom)\n",
        "    return hasilUcd_stopwords\n",
        "\n",
        "  #PengukuranUCD\n",
        "  def useCaseMeasurement(self, keyword1, keyword2, id1, id2):\n",
        "    hasil_wsd = []\n",
        "    for num in keyword1:\n",
        "      text = [cosine_similarity(num, angka) for angka in keyword2]\n",
        "      hasil_wsd.append(text)\n",
        "    df = pd.DataFrame(hasil_wsd, index= id1, columns= id2)\n",
        "    return df\n",
        "\n",
        "  def change_case(self, word):\n",
        "      return ''.join([' '+i.lower() if i.isupper()\n",
        "          else i for i in word]).lstrip(' ')\n",
        "\n",
        "  def __del__(self):\n",
        "    print ('Destructor called.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      MyucdReq = ucdReq()\n",
        "      ucd1= ucd1.dropna()\n",
        "      tbl_1 = MyucdReq.useCaseMeasurement(keyword1= freqs.aksi, keyword2=ucd1.aksi , id1= freqs.id, id2= ucd1.usecase)\n",
        "      tbl_1.rename(columns = {'insertMetadata':'UC01', 'searchArticle':'UC03', 'viewNextResult':'UC04'}, inplace = True)\n",
        "      print(\"\\nData Pengukuran antara functional dan ucd1 (txt)\")\n",
        "      print(tabulate(tbl_1, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      ucd2= ucd2.dropna()\n",
        "      tbl_2 = MyucdReq.useCaseMeasurement(keyword1= freqs.aksi, keyword2=ucd2.aksi , id1= freqs.id, id2= ucd2.usecase)\n",
        "      tbl_2.rename(columns = {'searchResearcher':'UC02', 'orderByRelevancy':'UC05', 'orderByScore':'UC06', \n",
        "                              'viewDetailResearcher':'UC07', 'removeArticle':'UC09', 'editProfile':'UC08' }, inplace = True)\n",
        "      print(\"\\nData Pengukuran antara functional dan ucd2 (txt)\")\n",
        "      print(tabulate(tbl_2, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_3 = pd.concat([tbl_1, tbl_2], axis= 1)\n",
        "      tbl_3['uc01'] = tbl_3.UC01.values.max(1)\n",
        "      tbl_3['uc02'] = tbl_3.UC02.values.max(1)\n",
        "      tbl_3['uc03'] = tbl_3.UC03.values.max(1)\n",
        "      tbl_3['uc04'] = tbl_3.UC04.values.max(1)\n",
        "      tbl_3['uc05'] = tbl_3.UC05.values.max(1)\n",
        "      tbl_3['uc06'] = tbl_3.UC06.values.max(1)\n",
        "      tbl_3['uc07'] = tbl_3.UC07.values.max(1)\n",
        "      tbl_3['uc08'] = tbl_3.UC08.values.max(1)\n",
        "      tbl_3['uc09'] = tbl_3.UC09.values.max(1)\n",
        "      tbl_3filter = tbl_3.drop(['UC01','UC02', 'UC03', 'UC04', 'UC05', 'UC06', 'UC07', 'UC08', 'UC09'], axis= 1)\n",
        "      print(\"\\nData filter pengukuran maksmimum (txt)\")\n",
        "      print(tabulate(tbl_3filter, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      threshold = 0.6\n",
        "      d = tbl_3filter.values >= threshold\n",
        "      d1 = pd.DataFrame(d, index= tbl_3filter.index, columns= tbl_3filter.columns)\n",
        "      mask = d1.isin([True])\n",
        "      d2 = d1.where(mask, other= 0)\n",
        "      mask2 = d1.isin([False])\n",
        "      tbl_4 = d2.where(mask2, other= 1)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (txt)\")\n",
        "      print(tabulate(tbl_4, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # Driver code\n",
        "      data_ucd = []\n",
        "      for num in useCaseTable.name:\n",
        "        data_ucd.append(MyucdReq.change_case(num))\n",
        "      tbl_1x = MyucdReq.useCaseMeasurement(keyword1= freqs.aksi, keyword2=data_ucd , id1= freqs.id, id2= useCaseTable.name)\n",
        "      tbl_1x.rename(columns = {'insertMetadata':'uc01', 'searchArticle':'uc03', 'viewNextResult':'uc04', \n",
        "                               'searchResearcher':'uc02', 'orderByRelevancy':'uc05', 'orderByScore':'uc06', \n",
        "                              'viewDetailOfResearcher':'uc07', 'removeArticle':'uc09', 'editProfile':'uc08' }, inplace = True)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\")\n",
        "      print(tabulate(tbl_1x, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      threshold = 0.6\n",
        "      dt = tbl_1x.values >= threshold\n",
        "      dt1 = pd.DataFrame(dt, index= tbl_1x.index, columns= tbl_1x.columns)\n",
        "      mask = dt1.isin([True])\n",
        "      dt2 = dt1.where(mask, other= 0)\n",
        "      mask2 = dt2.isin([False])\n",
        "      tbl_5 = dt2.where(mask2, other= 1)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\")\n",
        "      print(tabulate(tbl_5, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      list_usecase = ['uc01', 'uc02', 'uc03', 'uc04', 'uc05', 'uc06', 'uc07', 'uc08', 'uc09']\n",
        "      tbl_6 = tbl_4.merge(tbl_5, how= 'inner', left_index= True, right_index= True, on= list_usecase)\n",
        "      print(\"\\nData hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi)\")\n",
        "      print(tabulate(tbl_6, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      MyucdReq.__del__()\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructor called.\n",
            "\n",
            "Data Pengukuran antara functional dan ucd1 (txt)\n",
            "+------+---------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+--------+--------+\n",
            "| id   |    UC01 |     UC01 |     UC01 |     UC01 |   UC01 |     UC03 |     UC03 |     UC03 |     UC03 |     UC03 |     UC04 |   UC04 |   UC04 |\n",
            "|------+---------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+--------+--------|\n",
            "| F01  | 0.57735 | 0.288675 | 0.471405 | 0        |      0 | 0.436436 | 0.235702 | 0        | 0.258199 | 0.57735  | 0        |      0 |      0 |\n",
            "| F02  | 0       | 0        | 0        | 0        |      0 | 0.377964 | 0.408248 | 0        | 0.223607 | 0.25     | 0        |      0 |      0 |\n",
            "| F03  | 0       | 0        | 0        | 0        |      0 | 0        | 0        | 0        | 0        | 0        | 0        |      0 |      0 |\n",
            "| F04  | 0       | 0        | 0        | 0        |      0 | 0.188982 | 0.204124 | 0        | 0.223607 | 0.25     | 0        |      0 |      0 |\n",
            "| F05  | 0       | 0        | 0        | 0        |      0 | 0        | 0        | 0        | 0        | 0        | 0.288675 |      0 |      0 |\n",
            "| F06  | 0       | 0        | 0        | 0        |      0 | 0.218218 | 0.235702 | 0        | 0.258199 | 0.288675 | 0        |      0 |      0 |\n",
            "| F07  | 0       | 0        | 0        | 0        |      0 | 0        | 0        | 0        | 0        | 0        | 0        |      0 |      0 |\n",
            "| F08  | 0       | 0        | 0        | 0.408248 |      0 | 0        | 0        | 0.408248 | 0        | 0        | 0        |      0 |      0 |\n",
            "+------+---------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+--------+--------+\n",
            "\n",
            "Data Pengukuran antara functional dan ucd2 (txt)\n",
            "+------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------+\n",
            "| id   |   UC02 |     UC02 |     UC02 |     UC05 |     UC05 |     UC05 |     UC06 |     UC06 |     UC06 |     UC07 |     UC07 |     UC07 |     UC09 |     UC09 |     UC09 |     UC08 |     UC08 |     UC08 |   UC08 |     UC08 |\n",
            "|------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------|\n",
            "| F01  |    0   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.408248 | 0.288675 | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F02  |    0.5 | 0        | 0.612372 | 0.474342 | 0.288675 | 0.204124 | 0.188982 | 0        | 0.204124 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F03  |    0   | 0        | 0.182574 | 0.141421 | 0        | 0.182574 | 0.338062 | 0.258199 | 0.182574 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F04  |    0   | 0        | 0        | 0.158114 | 0        | 0.204124 | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 0.25     | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F05  |    0   | 0        | 0        | 0.158114 | 0        | 0.204124 | 0.188982 | 0        | 0.204124 | 1        | 0.353553 | 0.223607 | 0        | 0        | 0.223607 | 0.353553 | 0.25     | 0.288675 |    0   | 0        |\n",
            "| F06  |    0   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.288675 | 0        | 0.258199 | 0.816497 | 0.57735  | 0.258199 | 0.408248 | 0.288675 | 0.333333 |    0   | 0        |\n",
            "| F07  |    0   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 0        | 0.316228 | 0        | 0        | 0.316228 | 1        | 0.707107 | 0.408248 |    0.5 | 0        |\n",
            "| F08  |    0   | 0.816497 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        |    0   | 0.816497 |\n",
            "+------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------+\n",
            "\n",
            "Data filter pengukuran maksmimum (txt)\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| id   |     uc01 |     uc02 |     uc03 |     uc04 |     uc05 |     uc06 |     uc07 |     uc08 |     uc09 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 0.57735  | 0        | 0.57735  | 0        | 0        | 0        | 0        | 0        | 0.408248 |\n",
            "| F02  | 0        | 0.612372 | 0.408248 | 0        | 0.474342 | 0.204124 | 0        | 0        | 0        |\n",
            "| F03  | 0        | 0.182574 | 0        | 0        | 0.182574 | 0.338062 | 0        | 0        | 0        |\n",
            "| F04  | 0        | 0        | 0.25     | 0        | 0.204124 | 0        | 0        | 0        | 0.353553 |\n",
            "| F05  | 0        | 0        | 0        | 0.288675 | 0.204124 | 0.204124 | 1        | 0.353553 | 0.223607 |\n",
            "| F06  | 0        | 0        | 0.288675 | 0        | 0        | 0        | 0.288675 | 0.408248 | 0.816497 |\n",
            "| F07  | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 1        | 0.316228 |\n",
            "| F08  | 0.408248 | 0.816497 | 0.408248 | 0        | 0        | 0        | 0        | 0.816497 | 0        |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (txt)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "| F08  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\n",
            "+------+----------+----------+----------+----------+----------+----------+--------+----------+----------+\n",
            "| id   |     uc01 |     uc02 |     uc03 |     uc04 |     uc05 |     uc06 |   uc07 |     uc08 |     uc09 |\n",
            "|------+----------+----------+----------+----------+----------+----------+--------+----------+----------|\n",
            "| F01  | 0.816497 | 0        | 0.408248 | 0        | 0        | 0        |   0    | 0        | 0.408248 |\n",
            "| F02  | 0        | 0.353553 | 0.353553 | 0        | 0        | 0        |   0    | 0        | 0        |\n",
            "| F03  | 0        | 0        | 0        | 0        | 0        | 0.258199 |   0    | 0        | 0        |\n",
            "| F04  | 0        | 0        | 0.353553 | 0        | 0.288675 | 0        |   0    | 0        | 0.353553 |\n",
            "| F05  | 0        | 0.353553 | 0        | 0.288675 | 0        | 0        |   0.75 | 0.353553 | 0        |\n",
            "| F06  | 0        | 0        | 0.408248 | 0        | 0        | 0        |   0    | 0.408248 | 0.816497 |\n",
            "| F07  | 0        | 0        | 0        | 0        | 0        | 0        |   0    | 1        | 0        |\n",
            "| F08  | 0        | 0        | 0        | 0        | 0        | 0        |   0    | 0        | 0        |\n",
            "+------+----------+----------+----------+----------+----------+----------+--------+----------+----------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "| F08  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "Data hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "| F08  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "Destructor called.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3XyBiqc8UnU"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_freqs_ucd1'    : tbl_1, \n",
        "          'tabel_freqs_ucd2'    : tbl_2,\n",
        "          'tabel_filter'        : tbl_3filter,\n",
        "          'tabel_relasi_txt'    : tbl_4,\n",
        "          'tabel_relasi_xmi'    : tbl_5,\n",
        "          'tabel_join_txt_xmi'  : tbl_6,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/data_relasi.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcODID7GGsG5"
      },
      "source": [
        "# Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYFzOjD-8Xgy",
        "outputId": "5f3bffbe-21bb-42f3-b321-95f10ecb98fe"
      },
      "source": [
        "threshold = 0.1\n",
        "d = df_filter.values >= threshold\n",
        "d1 = pd.DataFrame(d, index= df_filter.index, columns= df_filter.columns)\n",
        "mask = d1.isin([True])\n",
        "d2 = d1.where(mask, other= 0)\n",
        "mask2 = d1.isin([False])\n",
        "tbl_4 = d2.where(mask2, other= 1)\n",
        "tbl_4.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
              "       [0, 1, 1, 0, 1, 1, 1, 0, 0],\n",
              "       [0, 1, 0, 0, 1, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 0, 1, 0, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 0, 0, 1, 1, 0]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1OyJmar_eB1",
        "outputId": "397eec68-b021-4c89-9db6-3ab804f7b8e9"
      },
      "source": [
        "def fulldataset(data, inputSRS):\n",
        "    xl = pd.ExcelFile(data)\n",
        "    dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}\n",
        "    kalimat = dfs[inputSRS]\n",
        "    kalimat_semua = kalimat.head(len(kalimat))\n",
        "    return kalimat_semua\n",
        "\n",
        "file3 = r'/content/mydrive/MyDrive/dataset/data_relasi.xlsx'\n",
        "dataGT = 'ground_truth'\n",
        "tbl_grd = fulldataset(data= file3, inputSRS= dataGT)\n",
        "tbl_grd = tbl_grd.drop(['index'], axis= 1)\n",
        "tbl_grd.index= d3.index\n",
        "tbl_grd.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioZB3zawCN3t",
        "outputId": "4ab5d547-c970-4c52-cb97-a954d9c54bba"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tbl_4_list = tbl_4.values.astype(int)\n",
        "tbl_grd_list = tbl_grd.values.astype(int)\n",
        "y_actual = tbl_4_list\n",
        "y_predicted = tbl_grd_list\n",
        "print(\"akurasi\", metrics.accuracy_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"presion\", metrics.precision_score(y_true= y_actual, y_pred= y_predicted, average= 'macro'))\n",
        "print(\"recall\", metrics.recall_score(y_true= y_actual, y_pred= y_predicted, average= 'macro'))\n",
        "print(\"metrics {}\".format(metrics.classification_report(y_true= y_actual, y_pred= y_predicted)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "akurasi 0.0\n",
            "presion 0.8888888888888888\n",
            "recall 0.24351851851851855\n",
            "metrics               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67         4\n",
            "           1       1.00      0.50      0.67         4\n",
            "           2       1.00      0.20      0.33         5\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       1.00      0.25      0.40         4\n",
            "           5       1.00      0.17      0.29         6\n",
            "           6       1.00      0.12      0.22         8\n",
            "           7       1.00      0.25      0.40         4\n",
            "           8       1.00      0.20      0.33         5\n",
            "\n",
            "   micro avg       1.00      0.24      0.38        42\n",
            "   macro avg       0.89      0.24      0.37        42\n",
            "weighted avg       0.95      0.24      0.37        42\n",
            " samples avg       1.00      0.25      0.39        42\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8IdWfjfGKsZ",
        "outputId": "d8fd2012-4e72-4a4b-8964-7cf4bd3435f8"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "#define array of actual values\n",
        "y_actual = [\n",
        "1,0,1,0,0,0,0,0,1,\n",
        "0,1,1,0,1,0,0,0,0,\n",
        "0,0,0,0,0,1,0,0,0,\n",
        "0,0,0,0,0,0,0,0,1,\n",
        "0,1,0,0,1,1,1,1,0,\n",
        "0,0,0,0,0,0,1,1,1,\n",
        "0,0,0,0,0,0,1,1,1,\n",
        "1,1,1,1,0,0,0,1,0,\n",
        "]\n",
        "\n",
        "#define array of predicted values\n",
        "y_predicted = [\n",
        "1,0,0,0,0,0,0,0,0,\n",
        "0,1,1,0,0,0,0,0,0,\n",
        "0,0,0,0,0,1,0,0,0,\n",
        "0,0,0,0,1,0,0,0,0,\n",
        "0,0,0,0,0,0,1,0,0,\n",
        "0,0,0,0,0,0,0,0,1,\n",
        "0,0,0,0,0,0,0,1,0,\n",
        "1,1,0,0,0,0,0,0,0,\n",
        "]\n",
        "\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true= y_actual, y_pred= y_predicted).ravel()\n",
        "\n",
        "print(\"false positif : \",fp)\n",
        "print(\"false negative : \",fn)\n",
        "print(\"true positive : \", tp) \n",
        "print(\"true negative : \", tn)\n",
        "\n",
        "print(\"akurasi\", metrics.accuracy_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"recall\", metrics.recall_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"presion\", metrics.precision_score(y_true= y_actual, y_pred= y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positif :  1\n",
            "false negative :  15\n",
            "true positive :  9\n",
            "true negative :  47\n",
            "akurasi 0.7777777777777778\n",
            "recall 0.375\n",
            "presion 0.9\n"
          ]
        }
      ]
    }
  ]
}