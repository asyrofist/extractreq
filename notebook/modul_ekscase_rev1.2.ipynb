{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modul_ucd_(ekscase).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1D_Q4akInoIY",
        "VY_5dBVUw2rx",
        "KCr2_xlVhi2R",
        "RkW-XQc8nYfJ",
        "XLDgm2smg3SP",
        "QcODID7GGsG5",
        "v21nTXqOs1oU"
      ],
      "authorship_tag": "ABX9TyPYtkXRt4a7BmF9n1Vt2wEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyrofist/Extraction-Requirement/blob/main/modul_ucd_(ekscase).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-dPqTpoUkl",
        "outputId": "a0db092e-5d31-4701-870e-8fd95dd27822"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')\n",
        "%cd /content/mydrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/mydrive; to attempt to forcibly remount, call drive.mount(\"/content/mydrive\", force_remount=True).\n",
            "/content/mydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pbz2pC4L1si"
      },
      "source": [
        "!pip install -U pywsd==1.1.0\n",
        "# !pip install XlsxWriter\n",
        "# !pip install -U wn==0.0.23"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHe6v5jzrBU5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LbIYvnsxNiA"
      },
      "source": [
        "# Modul1: xmlparser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ADmce0anTnm",
        "outputId": "0b6ac773-64cb-4203-bcf8-f5d5de32e25b"
      },
      "source": [
        "# function\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# template class xmlparser\n",
        "class xmlParser:\n",
        "\n",
        "    # inisialisasi\n",
        "    def __init__(self, filename= '/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/IRCI_Researcher.xmi', \n",
        "                 tipe_xmi= '{http://schema.omg.org/spec/XMI/2.1}type',\n",
        "                 id_xmi= '{http://schema.omg.org/spec/XMI/2.1}id'):\n",
        "    \tself.__namaFile = filename\n",
        "    \tself.__xmi_type = tipe_xmi\n",
        "    \tself.__xmi_id = id_xmi\n",
        "\n",
        "    def data_root(self):\n",
        "        tree = ET.parse(self.__namaFile)\n",
        "        root = tree.getroot()\n",
        "        return root\n",
        "\n",
        "    #fungsi parse tree elemen\n",
        "    def elemenTreeParse(self):\n",
        "      try: \n",
        "        elemenTag = [elem.tag for elem in xmlParser.data_root(self).iter()]\n",
        "        elemenAtribut = [elem.attrib for elem in root.iter()]\n",
        "        tabelElemen = pd.DataFrame([elemenTag, elemenAtribut], index=['Berdasarkan Tag', 'Berdsarkan Atribut']).T\n",
        "        return tabelElemen\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    # fungsi pencarian elemen\n",
        "    def cariTreeElemen(self, elemen):\n",
        "      try:\n",
        "        pencarian = [num.findall(elemen) for num in xmlParser.data_root(self).iter()]\n",
        "        return pencarian\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    #fungsi list elemen\n",
        "    def listElemen(self, elemen):\n",
        "      try:\n",
        "        listElemen = [berdasarkanOwnEnd.attrib for berdasarkanOwnEnd in xmlParser.data_root(self).iter(elemen)]\n",
        "        tabelElement = pd.DataFrame(listElemen)\n",
        "        return tabelElement\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    # fungsi mencari table spesifik\n",
        "    def tableElemenSpesifik(self, elemen= 'packagedElement', kolom1= 'name'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        berdasarkanPackagedELement = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(elemen)]\n",
        "        for num in berdasarkanPackagedELement:\n",
        "          a1 = num[kolom1]\n",
        "          c1 = num[self.__xmi_type]\n",
        "          d1 = num[self.__xmi_id]\n",
        "          hasil.append([a1, c1, d1])\n",
        "        cleanPackagedELement = pd.DataFrame(hasil, columns=[kolom1, self.__xmi_type, self.__xmi_id])\n",
        "        return cleanPackagedELement\n",
        "        \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    # fungsi mencari string\n",
        "    def doString(self):\n",
        "      try:\n",
        "        print(ET.tostring(xmlParser.data_root(self), encoding='utf8').decode('utf8'))\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def dataPaketElemen(self, category = 'packagedElement'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        berdasarkanPackagedELement = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in berdasarkanPackagedELement:\n",
        "          a1 = num[self.__xmi_id]\n",
        "          b1 = num['name']\n",
        "          d1 = num[self.__xmi_type]\n",
        "          hasil.append([a1, b1, d1])\n",
        "\n",
        "        paketElemen = pd.DataFrame(hasil, columns=['id', 'name', 'type'])\n",
        "        return paketElemen\n",
        "\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def dataExtend(self, category = 'extend'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        berdasarkanExtend = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in berdasarkanExtend:\n",
        "          a1 = num[self.__xmi_id]\n",
        "          b1 = num[self.__xmi_type]\n",
        "          c1 = num['extendedCase']\n",
        "          d1 = paketElemen[paketElemen['id'] == c1].iloc[0]['name']\n",
        "          e1 = num['extension']\n",
        "          f1 = paketElemen[paketElemen['id'] == e1].iloc[0]['name']\n",
        "          hasil.append([a1, b1, c1, d1, e1, f1])\n",
        "          \n",
        "        extendTable = pd.DataFrame(hasil, columns=['id', 'type', 'source', 'sourceName', 'destination', 'destinationName'])\n",
        "        return extendTable\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def dataInclude(self, category = 'include'):\n",
        "      try:\n",
        "        hasil = []\n",
        "        byinclude = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in byinclude:\n",
        "          a1 = num['{http://schema.omg.org/spec/XMI/2.1}id']\n",
        "          b1 = num['{http://schema.omg.org/spec/XMI/2.1}type']\n",
        "          c1 = num['includingCase']\n",
        "          d1 = paketElemen[paketElemen['id'] == c1].iloc[0]['name']\n",
        "          e1 = num['addition']\n",
        "          f1 = paketElemen[paketElemen['id'] == e1].iloc[0]['name']\n",
        "          hasil.append([a1, b1, c1, d1, e1, f1])\n",
        "        includeTable = pd.DataFrame(hasil, columns= ['id', 'tipe', 'include', 'includeName', 'addition', 'additionName'])\n",
        "        return includeTable        \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def dataOwnedEnd(self, category = 'ownedEnd'):\n",
        "      try:\n",
        "        # berdasarkan ownedEnd\n",
        "        hasil = []\n",
        "        berdasarkanOwnedEnd = [packagedElement.attrib for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        berdasarkanOwnedEnd\n",
        "        for num in berdasarkanOwnedEnd:\n",
        "          a1 = num['type']\n",
        "          b1 = num[self.__xmi_id]\n",
        "          c1 = num[self.__xmi_type]\n",
        "          d1 = paketElemen[paketElemen['id'] == a1].iloc[0]['name']\n",
        "          hasil.append([a1, b1, c1, d1])\n",
        "          \n",
        "        ownedEndTable = pd.DataFrame(hasil, columns=['id_data', 'id_property', 'type_property', 'id_name'])\n",
        "        return ownedEndTable\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def dataOwnedMember(self, category = 'ownedMember'):\n",
        "      try:\n",
        "        # berdasarkan UML Model\n",
        "        hasilNum = []\n",
        "        berdasarkanOwnedMember = [packagedElement for packagedElement in xmlParser.data_root(self).iter(category)]\n",
        "        for num in berdasarkanOwnedMember:\n",
        "          a = num.attrib[self.__xmi_id]\n",
        "          b = num.attrib[self.__xmi_type]\n",
        "          for index, angka in enumerate(num.iter('ownedEnd')):\n",
        "            if index == 0:\n",
        "              c = paketElemen[paketElemen['id'] == angka.attrib['type']].iloc[0]['name']\n",
        "            else:\n",
        "              d = paketElemen[paketElemen['id'] == angka.attrib['type']].iloc[0]['name']\n",
        "          hasilNum.append([a, b, c, d])\n",
        "\n",
        "        ownedMemberTable = pd.DataFrame(hasilNum, columns=['id', 'type_property', 'actor', 'usecase'])\n",
        "        return ownedMemberTable  \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def __del__(self):\n",
        "        print ('Destructor called.')    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      # myXmlParser = xmlParser(filename= '/content/mydrive/MyDrive/dataset/IRCI_V2/topic/IRCI_Topic.xmi')\n",
        "      # myXmlParser = xmlParser(filename= '/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/IRCI_Researcher.xmi')\n",
        "      # myXmlParser = xmlParser(filename= '/content/mydrive/MyDrive/dataset/RAnalyzer/RAnalyzer-DependencyUC/rAnalyzerUC.xmi')\n",
        "      myXmlParser = xmlParser()\n",
        "      paketElemen = myXmlParser.dataPaketElemen()\n",
        "      extendTable = myXmlParser.dataExtend()\n",
        "      ownedEndTable = myXmlParser.dataOwnedEnd()\n",
        "      ownedMemberTable = myXmlParser.dataOwnedMember()\n",
        "  \n",
        "      \"\"\"# Modul 1\n",
        "      Parsing file xmi menjadi tabel2 (daftar aktor, daftar use case, dan relasi antara actor use case dan antar use case)\n",
        "      \"\"\"\n",
        "      print(\"actorTable\")\n",
        "      actorTable = paketElemen[paketElemen['type'] == 'uml:Actor']\n",
        "      print(tabulate(actorTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\nuseCaseTable\")\n",
        "      useCaseTable = paketElemen[paketElemen['type'] == 'uml:UseCase']\n",
        "      print(tabulate(useCaseTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\nextendTable\")\n",
        "      print(tabulate(extendTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "\n",
        "      print(\"\\nassociationTable\")\n",
        "      print(tabulate(ownedMemberTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # print(\"\\npropertyTable\")\n",
        "      # print(tabulate(ownedEndTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      #untuk extend - data researcher dan topic\n",
        "      hasilAktor = []\n",
        "      hasilDestinasi = []\n",
        "\n",
        "      for idx, num in enumerate(extendTable.sourceName):\n",
        "        c = ownedMemberTable[ownedMemberTable['usecase'] == extendTable.sourceName[idx]]\n",
        "        if len(c) > 0:\n",
        "          for aktor in c.actor:\n",
        "            hasilAktor.append(aktor)\n",
        "            hasilDestinasi.append(extendTable.destinationName[idx])\n",
        "        else:\n",
        "          temp = 2\n",
        "          d = ownedMemberTable[ownedMemberTable['usecase'] == extendTable.sourceName[idx-temp]]\n",
        "          for dAktor in d.actor:\n",
        "            hasilAktor.append(dAktor)\n",
        "            hasilDestinasi.append(extendTable.destinationName[idx])\n",
        "\n",
        "      df_a = pd.DataFrame([hasilAktor, hasilDestinasi], index= ['actor', 'action']).T\n",
        "      df_a['actor'] = df_a.groupby(['action'])['actor'].transform(lambda x: ';'.join(x))\n",
        "      df_a = df_a[['action','actor']].drop_duplicates()\n",
        "      df_a['actor'][0] = set(df_a['actor'][0].split(\";\")) # fungsi ini digunakan untuk menyempurnakan format\n",
        "      df_a['actor'][0] = \";\".join(df_a['actor'][0])\n",
        "      ownedMemberTable.rename(columns = {'usecase':'action'}, inplace = True)\n",
        "      dt_b = pd.concat([df_a, ownedMemberTable])\n",
        "      dt_actor_action = dt_b.drop(['id', 'type_property'], axis= 1)\n",
        "      dt_actor_action['actor'] = dt_actor_action.groupby(['action'])['actor'].transform(lambda x: ';'.join(x))\n",
        "      dt_actor_action = dt_actor_action[['action','actor']].drop_duplicates()\n",
        "      print(\"\\nactorActionTable\")\n",
        "      print(tabulate(dt_actor_action, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # print(\"\\nincludeTable\")\n",
        "      # print(tabulate(includeTable, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # # untuk include  data ranalyzer\n",
        "      # hasilAktor = []\n",
        "      # hasilDestinasi = []\n",
        "      # for idy, angka in enumerate(includeTable.includeName):\n",
        "      #   f = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy]]\n",
        "      #   if len(f) > 0:\n",
        "      #     for aktor in f.actor:\n",
        "      #       hasilAktor.append(aktor)\n",
        "      #       hasilDestinasi.append(includeTable.additionName[idy])\n",
        "      #   else:\n",
        "      #     tempY = 2\n",
        "      #     g = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy-tempY]]\n",
        "      #     for dAktor in g.actor:\n",
        "      #       hasilAktor.append(dAktor)\n",
        "      #       hasilDestinasi.append(includeTable.additionName[idy])\n",
        "\n",
        "      # df_a = pd.DataFrame([hasilAktor, hasilDestinasi], index= ['actor', 'action']).T\n",
        "      # df_a['actor'] = df_a.groupby(['action'])['actor'].transform(lambda x: ';'.join(x))\n",
        "      # df_a = df_a[['action','actor']].drop_duplicates()\n",
        "      # df_a['actor'][0] = set(df_a['actor'][0].split(\";\")) # fungsi ini digunakan untuk menyempurnakan format\n",
        "      # df_a['actor'][0] = \";\".join(df_a['actor'][0])\n",
        "      # ownedMemberTable.rename(columns = {'usecase':'action'}, inplace = True)\n",
        "      # dt_b = pd.concat([df_a, ownedMemberTable])\n",
        "      # dt_actor_action = dt_b.drop(['id', 'type_property'], axis= 1)\n",
        "      # print(\"\\nactorActionTable\")\n",
        "      # print(tabulate(dt_actor_action, headers = 'keys', tablefmt = 'psql'))      \n",
        "\n",
        "      myXmlParser.__del__()\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructor called.\n",
            "actorTable\n",
            "+----+----------------------+-----------+-----------+\n",
            "|    | id                   | name      | type      |\n",
            "|----+----------------------+-----------+-----------|\n",
            "|  1 | AAAAAAF4mi7mgDh1TSM= | Submitter | uml:Actor |\n",
            "|  4 | AAAAAAF4mi99ijj8BCY= | Viewer    | uml:Actor |\n",
            "+----+----------------------+-----------+-----------+\n",
            "\n",
            "useCaseTable\n",
            "+----+----------------------+------------------------+-------------+\n",
            "|    | id                   | name                   | type        |\n",
            "|----+----------------------+------------------------+-------------|\n",
            "|  2 | AAAAAAF4mi8PyDifi1c= | insertMetadata         | uml:UseCase |\n",
            "|  3 | AAAAAAF4mi8ydjjN568= | searchResearcher       | uml:UseCase |\n",
            "|  5 | AAAAAAF4mjIw1zvIN8o= | searchArticle          | uml:UseCase |\n",
            "|  6 | AAAAAAF4mjJ07jyH6c4= | viewNextResult         | uml:UseCase |\n",
            "|  7 | AAAAAAF4mjPvZT9ass0= | orderByRelevancy       | uml:UseCase |\n",
            "|  8 | AAAAAAF4mjRfaEAQ9os= | orderByScore           | uml:UseCase |\n",
            "|  9 | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher | uml:UseCase |\n",
            "| 10 | AAAAAAF4mjURoUGOMMc= | editProfile            | uml:UseCase |\n",
            "| 11 | AAAAAAF4mjVMjEPi+Lg= | removeArticle          | uml:UseCase |\n",
            "+----+----------------------+------------------------+-------------+\n",
            "\n",
            "extendTable\n",
            "+----+----------------------+------------+----------------------+------------------------+----------------------+------------------------+\n",
            "|    | id                   | type       | source               | sourceName             | destination          | destinationName        |\n",
            "|----+----------------------+------------+----------------------+------------------------+----------------------+------------------------|\n",
            "|  0 | AAAAAAF4mjK8bj1PVZQ= | uml:Extend | AAAAAAF4mi8PyDifi1c= | insertMetadata         | AAAAAAF4mjIw1zvIN8o= | searchArticle          |\n",
            "|  1 | AAAAAAF4mjMMwD4V6+M= | uml:Extend | AAAAAAF4mi8PyDifi1c= | insertMetadata         | AAAAAAF4mjIw1zvIN8o= | searchArticle          |\n",
            "|  2 | AAAAAAF4mjNY8D6c2V4= | uml:Extend | AAAAAAF4mjIw1zvIN8o= | searchArticle          | AAAAAAF4mjJ07jyH6c4= | viewNextResult         |\n",
            "|  3 | AAAAAAF4mjXE8EYDP8I= | uml:Extend | AAAAAAF4mi8ydjjN568= | searchResearcher       | AAAAAAF4mjPvZT9ass0= | orderByRelevancy       |\n",
            "|  4 | AAAAAAF4mjXqS0Z4De0= | uml:Extend | AAAAAAF4mi8ydjjN568= | searchResearcher       | AAAAAAF4mjRfaEAQ9os= | orderByScore           |\n",
            "|  5 | AAAAAAF4mjZD3EjDQ6w= | uml:Extend | AAAAAAF4mi8ydjjN568= | searchResearcher       | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher |\n",
            "|  6 | AAAAAAF4mjcNv0mI9L0= | uml:Extend | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher | AAAAAAF4mjURoUGOMMc= | editProfile            |\n",
            "|  7 | AAAAAAF4mjbxIUlKgbk= | uml:Extend | AAAAAAF4mjS6/EDYTk8= | viewDetailOfResearcher | AAAAAAF4mjVMjEPi+Lg= | removeArticle          |\n",
            "+----+----------------------+------------+----------------------+------------------------+----------------------+------------------------+\n",
            "\n",
            "associationTable\n",
            "+----+----------------------+-----------------+-----------+------------------+\n",
            "|    | id                   | type_property   | actor     | usecase          |\n",
            "|----+----------------------+-----------------+-----------+------------------|\n",
            "|  0 | AAAAAAF4mjEd2znxW70= | uml:Association | Submitter | insertMetadata   |\n",
            "|  1 | AAAAAAF4mjFZVDo6ugo= | uml:Association | Submitter | searchResearcher |\n",
            "|  2 | AAAAAAF4mjGFvTqMa1Y= | uml:Association | Viewer    | searchResearcher |\n",
            "+----+----------------------+-----------------+-----------+------------------+\n",
            "\n",
            "actorActionTable\n",
            "+----+------------------------+------------------+\n",
            "|    | action                 | actor            |\n",
            "|----+------------------------+------------------|\n",
            "|  0 | searchArticle          | Submitter        |\n",
            "|  2 | viewNextResult         | Submitter        |\n",
            "|  3 | orderByRelevancy       | Submitter;Viewer |\n",
            "|  5 | orderByScore           | Submitter;Viewer |\n",
            "|  7 | viewDetailOfResearcher | Submitter;Viewer |\n",
            "|  9 | editProfile            | Submitter;Viewer |\n",
            "| 11 | removeArticle          | Submitter;Viewer |\n",
            "|  0 | insertMetadata         | Submitter        |\n",
            "|  1 | searchResearcher       | Submitter;Viewer |\n",
            "+----+------------------------+------------------+\n",
            "Destructor called.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xesxursq2LCd"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_aktor' : actorTable, \n",
        "          'tabel_usecase' : useCaseTable,\n",
        "          'tabel_relasi' : extendTable,\n",
        "          'tabel_asosisasi' : ownedMemberTable,\n",
        "          'tabel_properti' : ownedEndTable,\n",
        "          'tabel_aktor_action' : df_a,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/data_xmi.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D_Q4akInoIY"
      },
      "source": [
        "# Modul2: parsing aksi dan aktor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXOTKqbSnlnT",
        "outputId": "f193d4c6-35ac-46c6-f39a-f82c9d2f1102"
      },
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# template class parsingRequirement\n",
        "class parsingRequirement:\n",
        "\n",
        "    # inisialisasi\n",
        "    def __init__(self, filename):\n",
        "    \t  self.__namaFile = filename\n",
        "      \n",
        "    #fungsi parse tree elemen\n",
        "    def membacaCSV(self):\n",
        "        dt_csv = pd.read_csv(self.__namaFile, delimiter= ',')\n",
        "        return dt_csv\n",
        "\n",
        "    # cleaning text\n",
        "    def apply_cleaning_function_to_list(self, X):\n",
        "        dt_parse =  [parsingRequirement.clean_text(self, raw_text= element) for element in X]\n",
        "        return dt_parse\n",
        "        \n",
        "    def clean_text(self, raw_text):\n",
        "      try:\n",
        "        nlp = English()\n",
        "        tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "        tokens = tokenizer(raw_text)\n",
        "        lemma_list = [token.lemma_.lower() for token in tokens \n",
        "                      if token.is_stop is False and token.is_punct is False and token.is_alpha is True]\n",
        "        joined_words = ( \" \".join(lemma_list))\n",
        "        return joined_words\n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "    def data_raw(self, data_raw): # get data raw\n",
        "        dt =  [num2 for num in data_raw.fillna('empty') for num1 in num.split(';') for num2 in num1.split('.') \n",
        "              if 'Submitter' in num2 or 'Viewer' in num2 or 'system' in num2 or 'actor' in num2 or 'empty' in num2]\n",
        "        return dt\n",
        "\n",
        "    def aksi_aktor(self, data): # get data aksi dan aktor\n",
        "      try:\n",
        "        nlp = English()\n",
        "        tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "        tokens = tokenizer(data)\n",
        "        a = [token.text.lower() for token in tokens]\n",
        "        b = [x for x in a if x == 'submitter' or x == 'viewer' or x == 'system' or x == 'actor']  \n",
        "        b1 = \";\".join(b)\n",
        "        b1 = b1.replace(\"actor\", \"submitter; viewer\")\n",
        "        c = [x for x in a if x != 'submitter' and x != 'viewer' and x != 'system' and x != 'actor']  \n",
        "        c1 = \" \".join(c)\n",
        "        return b1, c1        \n",
        "      except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "        print ('Destructor called.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "\n",
        "    # parsing functional\n",
        "    MyParsingRequirement = parsingRequirement(filename= \"/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/freqs_researcher.txt\")\n",
        "    freqs = MyParsingRequirement.membacaCSV()\n",
        "    data_freqs = MyParsingRequirement.data_raw(freqs.requirement)\n",
        "\n",
        "    cleaned_freq = MyParsingRequirement.apply_cleaning_function_to_list(list(freqs.requirement))\n",
        "    freqs['aksi'] = [MyParsingRequirement.aksi_aktor(num)[1] for num in cleaned_freq]\n",
        "    freqs['aktor'] = [MyParsingRequirement.aksi_aktor(num)[0] for num in cleaned_freq]\n",
        "    print(\"\\nfreqs\")\n",
        "    print(tabulate(freqs, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "    # parsing ucd1\n",
        "    MyParsingRequirement = parsingRequirement(filename= \"/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/ucs/insert_metadata.txt\")\n",
        "    ucd1 = MyParsingRequirement.membacaCSV()\n",
        "    data_ucd1 = MyParsingRequirement.data_raw(ucd1.flowOfEvents)\n",
        "\n",
        "    list_index= [(\"data{}\".format(idx)) for idx, num in enumerate(data_ucd1)]\n",
        "    data_list = pd.DataFrame(data_ucd1, index= list_index)\n",
        "    data_list = data_list.drop(index= \"data5\").reset_index().drop(labels= ['index'], axis= 1)\n",
        "    # data_list = data_list.reset_index().drop(labels= ['index'], axis= 1)\n",
        "    ucd1['aksi'] = data_list\n",
        "\n",
        "    cleaned1_ucd = MyParsingRequirement.apply_cleaning_function_to_list(list(ucd1.aksi))\n",
        "    ucd1['aksi'] = [MyParsingRequirement.aksi_aktor(num)[1] for num in cleaned1_ucd]\n",
        "    ucd1['aktor'] = [MyParsingRequirement.aksi_aktor(num)[0] for num in cleaned1_ucd]\n",
        "    print(\"\\nucd1\")\n",
        "    print(tabulate(ucd1, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "    # parsing ucd2\n",
        "    MyParsingRequirement = parsingRequirement(filename= \"/content/mydrive/MyDrive/dataset/IRCI_V2/researcher/ucs/search_researcher.txt\")\n",
        "    ucd2 = MyParsingRequirement.membacaCSV()\n",
        "    data_ucd2 = MyParsingRequirement.data_raw(ucd2.flowOfEvents)\n",
        "\n",
        "    list2_index= [(\"data{}\".format(idx)) for idx, num in enumerate(data_ucd2)]\n",
        "    data2_list = pd.DataFrame(data_ucd2, index= list2_index)\n",
        "    data2_list = data2_list.reset_index().drop(labels= ['index'], axis= 1)\n",
        "    ucd2['aksi'] = data2_list\n",
        "\n",
        "    cleaned2_ucd = MyParsingRequirement.apply_cleaning_function_to_list(list(ucd2.aksi))\n",
        "    ucd2['aksi'] = [MyParsingRequirement.aksi_aktor(num)[1] for num in cleaned2_ucd]\n",
        "    ucd2['aktor'] = [MyParsingRequirement.aksi_aktor(num)[0] for num in cleaned2_ucd]\n",
        "    print(\"\\nucd2\")\n",
        "    print(tabulate(ucd2, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructor called.\n",
            "\n",
            "freqs\n",
            "+----+------+---------------------------------------------------------------------------+--------------------------------------+------------------+\n",
            "|    | id   | requirement                                                               | aksi                                 | aktor            |\n",
            "|----+------+---------------------------------------------------------------------------+--------------------------------------+------------------|\n",
            "|  0 | F01  | Submitter can insert metadata of article                                  | insert metadata article              | submitter        |\n",
            "|  1 | F02  | Submitter or Viewer can search researchers which are relevant to keyword  | search researchers relevant keyword  | submitter;viewer |\n",
            "|  2 | F03  | Submitter or Viewer can sort the researchers start from the highest score | sort researchers start highest score | submitter;viewer |\n",
            "|  3 | F04  | Submitter or Viewer can sort the article starts from the relevancy        | sort article starts relevancy        | submitter;viewer |\n",
            "|  4 | F05  | Submitter or viewer can view the detail of a researcher profile           | view detail researcher profile       | submitter;viewer |\n",
            "|  5 | F06  | Submitter can remove an article from his/her profile                      | remove article profile               | submitter        |\n",
            "|  6 | F07  | Submitter can edit his/ her profile                                       | edit profile                         | submitter        |\n",
            "|  7 | F08  | The system can show a progress bar                                        | progress bar                         | system           |\n",
            "+----+------+---------------------------------------------------------------------------+--------------------------------------+------------------+\n",
            "Destructor called.\n",
            "\n",
            "ucd1\n",
            "+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-----------+\n",
            "|    | id                | usecase        | flowOfEvents                                                                                                        | aksi                                               | aktor     |\n",
            "|----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-----------|\n",
            "|  0 | basic_flow        | nan            | nan                                                                                                                 |                                                    |           |\n",
            "|  1 | 1                 | insertMetadata | In the main page; Submitter clicks on insert metadata menu.                                                         | clicks insert metadata menu                        | submitter |\n",
            "|  2 | 2                 | insertMetadata | The system shows a form for inserting metadata.                                                                     | shows form inserting metadata                      | system    |\n",
            "|  3 | 3                 | insertMetadata | Submitter fills in the metadata field with metadata of article(s); then clicks on submit button.                    | fills metadata field metadata                      | submitter |\n",
            "|  4 | 4                 | insertMetadata | The system shows the progress information. After it is done; the system shows a message: “Submission is completed.” | shows progress information                         | system    |\n",
            "|  5 | 5                 | insertMetadata | The system returns to the main page.                                                                                | returns main page                                  | system    |\n",
            "|  6 | alternative_flows | nan            | nan                                                                                                                 |                                                    |           |\n",
            "|  7 | extensionPoints   | nan            | nan                                                                                                                 |                                                    |           |\n",
            "|  8 | 3b                | searchArticle  | Submitter can add metadata by Search an existing Article based on a keyword.                                        | add metadata search existing article based keyword | submitter |\n",
            "|  9 | 3b.1              | searchArticle  | Submitter enter a keyword and clicks Search Article button.                                                         | enter keyword clicks search article button         | submitter |\n",
            "| 10 | 3b.2              | searchArticle  | The system shows the progress information. After it is done it shows a result (a list of relevant articles).        | shows progress information                         | system    |\n",
            "| 11 | 3b.4              | searchArticle  | Submitter click an Add button on a relevant article.                                                                | click add button relevant article                  | submitter |\n",
            "| 12 | 3b.5              | searchArticle  | The system shows the metadata of the relevant article.                                                              | shows metadata relevant article                    | system    |\n",
            "| 13 | 3b.2a             | viewNextResult | The Submitter can View to the Nextpage of the Result.                                                               | view nextpage result                               | submitter |\n",
            "| 14 | 4b.2a.1           | viewNextResult | The Submitter clicks on Next button to move to the next page.                                                       | clicks button page                                 | submitter |\n",
            "| 15 | 4b.2a.2           | viewNextResult | The system shows the next result.                                                                                   | shows result                                       | system    |\n",
            "+----+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-----------+\n",
            "Destructor called.\n",
            "\n",
            "ucd2\n",
            "+----+------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+-------------------+\n",
            "|    | id               | usecase              | flowOfEvents                                                                                                                   | aksi                                                                      | aktor             |\n",
            "|----+------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+-------------------|\n",
            "|  0 | basic_flow       | nan                  | nan                                                                                                                            |                                                                           |                   |\n",
            "|  1 | 1                | searchResearcher     | The actor enters a keyword on the search field and clicks on the Search Button.                                                | enters keyword search field clicks search button                          | submitter; viewer |\n",
            "|  2 | 2                | searchResearcher     | The system shows a progress bar.                                                                                               | shows progress bar                                                        | system            |\n",
            "|  3 | 3                | searchResearcher     | After it has finished searching; the system views the list of researchers which are relevant to the keyword being entered.     | views list researchers relevant keyword entered                           | system            |\n",
            "|  4 | alternativeFlow  | nan                  | nan                                                                                                                            |                                                                           |                   |\n",
            "|  5 | extension_points | nan                  | nan                                                                                                                            |                                                                           |                   |\n",
            "|  6 | 3a               | orderByRelevancy     | The actor can change the view of search result based on Order by Relevancy of the researchers to the given keyword.            | change view search result based order relevancy researchers given keyword | submitter; viewer |\n",
            "|  7 | 3a.1             | orderByRelevancy     | The actor clicks on Relevant menu                                                                                              | clicks relevant menu                                                      | submitter; viewer |\n",
            "|  8 | 3a.2             | orderByRelevancy     | The system refreshes the view and order the researchers based on their relevancy.                                              | refreshes view order researchers based relevancy                          | system            |\n",
            "|  9 | 3b               | orderByScore         | The actor can change the view of articles based on Order by Score of researchers.                                              | change view articles based order score researchers                        | submitter; viewer |\n",
            "| 10 | 3b.1             | orderByScore         | The actor clicks on Score menu                                                                                                 | clicks score menu                                                         | submitter; viewer |\n",
            "| 11 | 3b.2             | orderByScore         | The system refreshes the view and order the researchers based on their scores.                                                 | refreshes view order researchers based scores                             | system            |\n",
            "| 12 | 3c               | viewDetailResearcher | The actor can View Detail Researcher of profile.                                                                               | view detail researcher profile                                            | submitter; viewer |\n",
            "| 13 | 3c.1             | viewDetailResearcher | The Submitter clicks on the name of the researcher.                                                                            | clicks researcher                                                         | submitter         |\n",
            "| 14 | 3c.2             | viewDetailResearcher | The system shows the profile and list of his/ her published articles.                                                          | shows profile list published articles                                     | system            |\n",
            "| 15 | 3d.2a            | removeArticle        | The Submitter remove his Article.                                                                                              | remove article                                                            | submitter         |\n",
            "| 16 | 3d.2a.1          | removeArticle        | The Submitter clicks on Remove button of the article.                                                                          | clicks remove button article                                              | submitter         |\n",
            "| 17 | 3d.2a.2          | removeArticle        | The system reshows the profile and list of his/ her published articles.                                                        | reshows profile list published articles                                   | system            |\n",
            "| 18 | 3d.2b            | editProfile          | The Submitter Edit his/ her Profile.                                                                                           | edit profile                                                              | submitter         |\n",
            "| 19 | 3d.2b.1          | editProfile          | The Submitter clicks on Edit Profile button.                                                                                   | clicks edit profile button                                                | submitter         |\n",
            "| 20 | 3d.2b.2          | editProfile          | The system shows the profile form.                                                                                             | shows profile form                                                        | system            |\n",
            "| 21 | 3d.2b.2          | editProfile          | The Submitter edit a field; and click Save button.                                                                             | edit field                                                                | submitter         |\n",
            "| 22 | 3d.2b.2          | editProfile          | The system shows a progress bar. After it is finished; the system reshows the profile and list of his/ her published articles. | shows progress bar                                                        | system            |\n",
            "+----+------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+-------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Fd8bI9ECtL",
        "outputId": "74d05a01-1d8f-47b1-efde-f38b7fa8872a"
      },
      "source": [
        "data_raw = ucd1.flowOfEvents\n",
        "data_num2 = []\n",
        "for num in data_raw.fillna(\"empty\"):\n",
        "  for num1 in num.split(\";\"):\n",
        "    for num2 in num1.split(\".\"):\n",
        "      if 'Submitter' in num2 or 'Viewer' in num2 or 'system' in num2 or 'actor' in num2 or 'empty' in num2:\n",
        "        data_num2.append(num2)\n",
        "data_num2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['empty',\n",
              " ' Submitter clicks on insert metadata menu',\n",
              " 'The system shows a form for inserting metadata',\n",
              " 'Submitter fills in the metadata field with metadata of article(s)',\n",
              " 'The system shows the progress information',\n",
              " ' the system shows a message: “Submission is completed',\n",
              " 'The system returns to the main page',\n",
              " 'empty',\n",
              " 'empty',\n",
              " 'Submitter can add metadata by Search an existing Article based on a keyword',\n",
              " 'Submitter enter a keyword and clicks Search Article button',\n",
              " 'The system shows the progress information',\n",
              " 'Submitter click an Add button on a relevant article',\n",
              " 'The system shows the metadata of the relevant article',\n",
              " 'The Submitter can View to the Nextpage of the Result',\n",
              " 'The Submitter clicks on Next button to move to the next page',\n",
              " 'The system shows the next result']"
            ]
          },
          "metadata": {},
          "execution_count": 637
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC7M0Qd26sf9"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_freqs' : freqs, \n",
        "          'tabel_ucd1' : ucd1,\n",
        "          'tabel_ucd2' : ucd2,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/data_aksi_aktor.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY_5dBVUw2rx"
      },
      "source": [
        "# Modul3: Pencarian Relasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Iwfs9jcWyj",
        "outputId": "e5935ea6-6e1c-48f1-83c2-897c18396172"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from spacy.lang.en import English\n",
        "from pywsd import disambiguate\n",
        "from pywsd.similarity import similarity_by_path\n",
        "from tabulate import tabulate\n",
        "\n",
        "# template class ucdReq\n",
        "class ucdReq:\n",
        "\n",
        "  #inicsialisasi\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def synset_word(self, data): # pencarian synset\n",
        "    try:\n",
        "      nlp = English()\n",
        "      tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "      num_token = [token.text for token in tokenizer(data)]\n",
        "      word = [disambiguate(x) for x in num_token]\n",
        "      wordsynset = [[n[1] for n in y if n[1] is not None] for y in word]\n",
        "      final = [val[0] for val in wordsynset if len(val) > 0]\n",
        "      return final\n",
        "    except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "  def wsd_greedy(self, s1, s2): # kombinasi algoritma wsd dan greedy\n",
        "    try:\n",
        "      scores = [[x for x in [similarity_by_path(i1, i2, option= 'wup') for i2 in s2] if x is not None] for i1 in s1]\n",
        "      flt_scores = [val for val in scores if len(val) > 0]\n",
        "      list1 = [np.max(num) for num in flt_scores]\n",
        "      dt = (sorted(list1, reverse=True))\n",
        "      dt_value = dt[:len(dt)-1]\n",
        "      kalkulasi = 2 * sum(dt_value) / ((len(flt_scores)) + (len(flt_scores[0])))\n",
        "      return kalkulasi\n",
        "    except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "  def similaritas_doc(self, doc1, doc2): # pencarian kesmaaan dokumen\n",
        "    try:\n",
        "      synsets1 = ucdReq.synset_word(self, doc1)\n",
        "      synsets2 = ucdReq.synset_word(self, doc2)\n",
        "      return (ucdReq.wsd_greedy(self, synsets1, synsets2) + ucdReq.wsd_greedy(self, synsets2, synsets1)) / 2       \n",
        "    except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "  def ucdMeasurement(self, keyword1, keyword2): #pengukuran\n",
        "    try:\n",
        "      hasil_wsd = [[ucdReq.similaritas_doc(self, num, angka) for angka in keyword2] for num in keyword1]\n",
        "      return pd.DataFrame(hasil_wsd)\n",
        "    except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "\n",
        "  def change_case(self, word):\n",
        "      return ''.join([' '+i.lower() if i.isupper() else i for i in word]).lstrip(' ')\n",
        "\n",
        "  def thresholdvalue(self, threshold, data):\n",
        "    try:\n",
        "      dt = data.values >= threshold\n",
        "      d1 = pd.DataFrame(dt, index= data.index, columns= data.columns)\n",
        "      mask = d1.isin([True])\n",
        "      d2 = d1.where(mask, other= 0)\n",
        "      mask2 = d1.isin([False])\n",
        "      return d2.where(mask2, other= 1)\n",
        "    except OSError as err:\n",
        "        print(\"OS error: {0}\".format(err))\n",
        "\n",
        "  def __del__(self):\n",
        "    print ('Destructor called.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      MyucdReq = ucdReq()\n",
        "      tbl_1 = MyucdReq.ucdMeasurement(freqs.aksi, ucd1.dropna().aksi)\n",
        "      tbl_1.columns = ucd1.dropna().usecase\n",
        "      tbl_1.index = freqs.id\n",
        "      tbl_1.rename(columns = {'insertMetadata':'UC01', 'searchArticle':'UC03', 'viewNextResult':'UC04'}, inplace = True)\n",
        "      print(\"\\nData Pengukuran antara functional dan ucd1 (txt)\")\n",
        "      print(tabulate(tbl_1, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_2 = MyucdReq.ucdMeasurement(freqs.aksi, ucd2.dropna().aksi)\n",
        "      tbl_2.columns = ucd2.dropna().usecase\n",
        "      tbl_2.index = freqs.id\n",
        "      tbl_2.rename(columns = {'searchResearcher':'UC02', 'orderByRelevancy':'UC05', 'orderByScore':'UC06', \n",
        "                              'viewDetailResearcher':'UC07', 'removeArticle':'UC09', 'editProfile':'UC08' }, inplace = True)\n",
        "      print(\"\\nData Pengukuran antara functional dan ucd2 (txt)\")\n",
        "      print(tabulate(tbl_2, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_3 = pd.concat([tbl_1, tbl_2], axis= 1)\n",
        "      tbl_3['uc01'] = tbl_3.UC01.values.max(1)\n",
        "      tbl_3['uc02'] = tbl_3.UC02.values.max(1)\n",
        "      tbl_3['uc03'] = tbl_3.UC03.values.max(1)\n",
        "      tbl_3['uc04'] = tbl_3.UC04.values.max(1)\n",
        "      tbl_3['uc05'] = tbl_3.UC05.values.max(1)\n",
        "      tbl_3['uc06'] = tbl_3.UC06.values.max(1)\n",
        "      tbl_3['uc07'] = tbl_3.UC07.values.max(1)\n",
        "      tbl_3['uc08'] = tbl_3.UC08.values.max(1)\n",
        "      tbl_3['uc09'] = tbl_3.UC09.values.max(1)\n",
        "      tbl_3filter = tbl_3.drop(['UC01','UC02', 'UC03', 'UC04', 'UC05', 'UC06', 'UC07', 'UC08', 'UC09'], axis= 1)\n",
        "      print(\"\\nData filter pengukuran maksmimum antara functional terhadap ucd1 dan ucd2 (txt)\")\n",
        "      print(tabulate(tbl_3filter, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_4 = MyucdReq.thresholdvalue(0.4, tbl_3filter)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (txt)\")\n",
        "      print(tabulate(tbl_4, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # xmi code\n",
        "      data_ucd = [MyucdReq.change_case(num) for num in useCaseTable.name]\n",
        "      tbl_1x = MyucdReq.ucdMeasurement(freqs.aksi, data_ucd)\n",
        "      tbl_1x.index = freqs.id\n",
        "      tbl_1x.columns = useCaseTable.name\n",
        "      tbl_1x.rename(columns = {'insertMetadata':'uc01', 'searchArticle':'uc03', 'viewNextResult':'uc04', \n",
        "                               'searchResearcher':'uc02', 'orderByRelevancy':'uc05', 'orderByScore':'uc06', \n",
        "                              'viewDetailOfResearcher':'uc07', 'removeArticle':'uc09', 'editProfile':'uc08' }, inplace = True)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\")\n",
        "      print(tabulate(tbl_1x, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\")\n",
        "      tbl_5 = MyucdReq.thresholdvalue(0.6, tbl_1x)\n",
        "      print(tabulate(tbl_5, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      list_usecase = [num for num in tbl_5.columns]\n",
        "      tbl_6 = tbl_4.merge(tbl_5, how= 'inner', left_index= True, right_index= True, on= list_usecase)\n",
        "      print(\"\\nData hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi)\")\n",
        "      print(tabulate(tbl_6, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      MyucdReq.__del__()\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructor called.\n",
            "\n",
            "Data filter pengukuran maksmimum antara functional terhadap ucd1 dan ucd2 (txt)\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| id   |     uc01 |     uc02 |     uc03 |     uc04 |     uc05 |     uc06 |     uc07 |     uc08 |     uc09 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 0.646617 | 0.379862 | 0.666667 | 0.413534 | 0.340986 | 0.447233 | 0.448377 | 0.413534 | 0.6058   |\n",
            "| F02  | 0.3      | 0.529412 | 0.446852 | 0.334118 | 0.423232 | 0.44898  | 0.5      | 0.288889 | 0.24097  |\n",
            "| F03  | 0.286111 | 0.419594 | 0.295028 | 0.312465 | 0.401246 | 0.573865 | 0.432234 | 0.297152 | 0.28736  |\n",
            "| F04  | 0.316871 | 0.349811 | 0.4663   | 0.283709 | 0.44314  | 0.414408 | 0.470761 | 0.28643  | 0.488095 |\n",
            "| F05  | 0.383111 | 0.638312 | 0.281867 | 0.447619 | 0.638552 | 0.624292 | 0.75     | 0.504762 | 0.447744 |\n",
            "| F06  | 0.353968 | 0.335056 | 0.500418 | 0.2516   | 0.362415 | 0.471852 | 0.644558 | 0.559649 | 0.706767 |\n",
            "| F07  | 0.206667 | 0.188596 | 0.144134 | 0.133333 | 0.237243 | 0.237302 | 0.327059 | 0.365385 | 0.301587 |\n",
            "| F08  | 0.514286 | 0.6      | 0.514286 | 0.315294 | 0.276045 | 0.279349 | 0.288054 | 0.6      | 0.291939 |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (txt)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      1 |      0 |      1 |      1 |      0 |      1 |      1 |      1 |      1 |\n",
            "| F02  |      0 |      1 |      1 |      0 |      1 |      1 |      1 |      0 |      0 |\n",
            "| F03  |      0 |      1 |      0 |      0 |      1 |      1 |      1 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      1 |      0 |      1 |      1 |      1 |      0 |      1 |\n",
            "| F05  |      0 |      1 |      0 |      1 |      1 |      1 |      1 |      1 |      1 |\n",
            "| F06  |      0 |      0 |      1 |      0 |      0 |      1 |      1 |      1 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F08  |      1 |      1 |      1 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\n",
            "+------+-----------+-----------+-----------+----------+----------+----------+----------+-----------+-----------+\n",
            "| id   |      uc01 |      uc02 |      uc03 |     uc04 |     uc05 |     uc06 |     uc07 |      uc08 |      uc09 |\n",
            "|------+-----------+-----------+-----------+----------+----------+----------+----------+-----------+-----------|\n",
            "| F01  | 0.6       | 0.263333  | 0.453333  | 0.186667 | 0.244156 | 0.186667 | 0.320635 | 0.122222  | 0.516667  |\n",
            "| F02  | 0.266667  | 0.5       | 0.5       | 0.125    | 0.133333 | 0.238095 | 0.45     | 0.0701754 | 0.122807  |\n",
            "| F03  | 0.276679  | 0.46398   | 0.201786  | 0.13185  | 0.141414 | 0.386752 | 0.4041   | 0.0916667 | 0.163095  |\n",
            "| F04  | 0.220113  | 0.194628  | 0.425595  | 0.191298 | 0.43254  | 0.191298 | 0.232625 | 0.126587  | 0.477381  |\n",
            "| F05  | 0.304762  | 0.412037  | 0.165142  | 0.447619 | 0.312393 | 0.30873  | 0.714286 | 0.327059  | 0.189281  |\n",
            "| F06  | 0.173333  | 0.136842  | 0.494444  | 0.25     | 0.325714 | 0.293333 | 0.23817  | 0.305556  | 0.6       |\n",
            "| F07  | 0.0888889 | 0.0701754 | 0.0740741 | 0.133333 | 0.177778 | 0.177778 | 0.158824 | 0         | 0.0740741 |\n",
            "| F08  | 0.333333  | 0.222222  | 0.222222  | 0.142857 | 0.153846 | 0.210526 | 0.234921 | 0.0784314 | 0.137255  |\n",
            "+------+-----------+-----------+-----------+----------+----------+----------+----------+-----------+-----------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F08  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "Data hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      1 |      0 |      1 |      1 |      0 |      1 |      1 |      1 |      1 |\n",
            "| F02  |      0 |      1 |      1 |      0 |      1 |      1 |      1 |      0 |      0 |\n",
            "| F03  |      0 |      1 |      0 |      0 |      1 |      1 |      1 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      1 |      0 |      1 |      1 |      1 |      0 |      1 |\n",
            "| F05  |      0 |      1 |      0 |      1 |      1 |      1 |      1 |      1 |      1 |\n",
            "| F06  |      0 |      0 |      1 |      0 |      0 |      1 |      1 |      1 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F08  |      1 |      1 |      1 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "Destructor called.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seDGxhJIla3Q"
      },
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "dfs  = {\n",
        "          'tabel_freqs_ucd1'    : tbl_1, \n",
        "          'tabel_freqs_ucd2'    : tbl_2,\n",
        "          'tabel_filter'        : tbl_3filter,\n",
        "          'tabel_relasi_txt'    : tbl_4,\n",
        "          'tabel_relasi_xmi'    : tbl_5,\n",
        "          'tabel_join_txt_xmi'  : tbl_6,\n",
        "        } \n",
        "\n",
        "writer = pd.ExcelWriter('/content/mydrive/MyDrive/dataset/data_relasi.xlsx')\n",
        "\n",
        "for name,dataframe in dfs.items():\n",
        "    dataframe.to_excel(writer,name,index=False)\n",
        "\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCr2_xlVhi2R"
      },
      "source": [
        "# Alternatif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkW-XQc8nYfJ"
      },
      "source": [
        "### Alternatif Method: wsd + cosine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETzqGKt_nXVU",
        "outputId": "a24ab55b-a592-4333-d532-dff1fe209a56"
      },
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pywsd import disambiguate\n",
        "from pywsd.similarity import max_similarity as maxsim\n",
        "from pywsd.cosine import cosine_similarity\n",
        "\n",
        "# template class ucdReq\n",
        "class ucdReq:\n",
        "\n",
        "  #inicsialisasi\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  #PengukuranUCD\n",
        "  def useCaseMeasurement(self, keyword1, keyword2, id1, id2):\n",
        "    hasil_wsd = [[cosine_similarity(num, angka) for angka in keyword2] for num in keyword1]\n",
        "    df = pd.DataFrame(hasil_wsd, index= id1, columns= id2)\n",
        "    return df\n",
        "\n",
        "  def change_case(self, word):\n",
        "      return ''.join([' '+i.lower() if i.isupper() else i for i in word]).lstrip(' ')\n",
        "\n",
        "  def thresholdvalue(self, threshold, data):\n",
        "      d = data.values >= threshold\n",
        "      d1 = pd.DataFrame(d, index= data.index, columns= data.columns)\n",
        "      mask = d1.isin([True])\n",
        "      d2 = d1.where(mask, other= 0)\n",
        "      mask2 = d1.isin([False])\n",
        "      return d2.where(mask2, other= 1)\n",
        "\n",
        "  def __del__(self):\n",
        "    print ('Destructor called.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      MyucdReq = ucdReq()\n",
        "      tbl_1 = MyucdReq.useCaseMeasurement(keyword1= freqs.aksi, keyword2=ucd1.dropna().aksi , id1= freqs.id, id2= ucd1.dropna().usecase)\n",
        "      tbl_1.rename(columns = {'insertMetadata':'UC01', 'searchArticle':'UC03', 'viewNextResult':'UC04'}, inplace = True)\n",
        "      print(\"\\nData Pengukuran antara functional dan ucd1 (txt)\")\n",
        "      print(tabulate(tbl_1, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_2 = MyucdReq.useCaseMeasurement(keyword1= freqs.aksi, keyword2=ucd2.dropna().aksi , id1= freqs.id, id2= ucd2.dropna().usecase)\n",
        "      tbl_2.rename(columns = {'searchResearcher':'UC02', 'orderByRelevancy':'UC05', 'orderByScore':'UC06', \n",
        "                              'viewDetailResearcher':'UC07', 'removeArticle':'UC09', 'editProfile':'UC08' }, inplace = True)\n",
        "      print(\"\\nData Pengukuran antara functional dan ucd2 (txt)\")\n",
        "      print(tabulate(tbl_2, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_3 = pd.concat([tbl_1, tbl_2], axis= 1)\n",
        "      tbl_3['uc01'] = tbl_3.UC01.values.max(1)\n",
        "      tbl_3['uc02'] = tbl_3.UC02.values.max(1)\n",
        "      tbl_3['uc03'] = tbl_3.UC03.values.max(1)\n",
        "      tbl_3['uc04'] = tbl_3.UC04.values.max(1)\n",
        "      tbl_3['uc05'] = tbl_3.UC05.values.max(1)\n",
        "      tbl_3['uc06'] = tbl_3.UC06.values.max(1)\n",
        "      tbl_3['uc07'] = tbl_3.UC07.values.max(1)\n",
        "      tbl_3['uc08'] = tbl_3.UC08.values.max(1)\n",
        "      tbl_3['uc09'] = tbl_3.UC09.values.max(1)\n",
        "      tbl_3filter = tbl_3.drop(['UC01','UC02', 'UC03', 'UC04', 'UC05', 'UC06', 'UC07', 'UC08', 'UC09'], axis= 1)\n",
        "      print(\"\\nData filter pengukuran maksmimum (txt)\")\n",
        "      print(tabulate(tbl_3filter, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      tbl_4 = MyucdReq.thresholdvalue(0.6, tbl_3filter)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (txt)\")\n",
        "      print(tabulate(tbl_4, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      # Driver code\n",
        "      data_ucd = [MyucdReq.change_case(num) for num in useCaseTable.name]\n",
        "      tbl_1x = MyucdReq.useCaseMeasurement(keyword1= freqs.aksi, keyword2=data_ucd , id1= freqs.id, id2= useCaseTable.name)\n",
        "      tbl_1x.rename(columns = {'insertMetadata':'uc01', 'searchArticle':'uc03', 'viewNextResult':'uc04', \n",
        "                               'searchResearcher':'uc02', 'orderByRelevancy':'uc05', 'orderByScore':'uc06', \n",
        "                              'viewDetailOfResearcher':'uc07', 'removeArticle':'uc09', 'editProfile':'uc08' }, inplace = True)\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\")\n",
        "      print(tabulate(tbl_1x, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      print(\"\\nData hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\")\n",
        "      tbl_5 = MyucdReq.thresholdvalue(0.6, tbl_1x)\n",
        "      print(tabulate(tbl_5, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      list_usecase = [num for num in tbl_5.columns]\n",
        "      tbl_6 = tbl_4.merge(tbl_5, how= 'inner', left_index= True, right_index= True, on= list_usecase)\n",
        "      print(\"\\nData hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi)\")\n",
        "      print(tabulate(tbl_6, headers = 'keys', tablefmt = 'psql'))\n",
        "\n",
        "      MyucdReq.__del__()\n",
        "\n",
        "  except OSError as err:\n",
        "      print(\"OS error: {0}\".format(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destructor called.\n",
            "\n",
            "Data Pengukuran antara functional dan ucd1 (txt)\n",
            "+------+---------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+--------+--------+\n",
            "| id   |    UC01 |     UC01 |     UC01 |     UC01 |   UC01 |     UC03 |     UC03 |     UC03 |     UC03 |     UC03 |     UC04 |   UC04 |   UC04 |\n",
            "|------+---------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+--------+--------|\n",
            "| F01  | 0.57735 | 0.288675 | 0.471405 | 0        |      0 | 0.436436 | 0.235702 | 0        | 0.258199 | 0.57735  | 0        |      0 |      0 |\n",
            "| F02  | 0       | 0        | 0        | 0        |      0 | 0.377964 | 0.408248 | 0        | 0.223607 | 0.25     | 0        |      0 |      0 |\n",
            "| F03  | 0       | 0        | 0        | 0        |      0 | 0        | 0        | 0        | 0        | 0        | 0        |      0 |      0 |\n",
            "| F04  | 0       | 0        | 0        | 0        |      0 | 0.188982 | 0.204124 | 0        | 0.223607 | 0.25     | 0        |      0 |      0 |\n",
            "| F05  | 0       | 0        | 0        | 0        |      0 | 0        | 0        | 0        | 0        | 0        | 0.288675 |      0 |      0 |\n",
            "| F06  | 0       | 0        | 0        | 0        |      0 | 0.218218 | 0.235702 | 0        | 0.258199 | 0.288675 | 0        |      0 |      0 |\n",
            "| F07  | 0       | 0        | 0        | 0        |      0 | 0        | 0        | 0        | 0        | 0        | 0        |      0 |      0 |\n",
            "| F08  | 0       | 0        | 0        | 0.408248 |      0 | 0        | 0        | 0.408248 | 0        | 0        | 0        |      0 |      0 |\n",
            "+------+---------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+--------+--------+\n",
            "\n",
            "Data Pengukuran antara functional dan ucd2 (txt)\n",
            "+------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------+\n",
            "| id   |   UC02 |     UC02 |     UC02 |     UC05 |     UC05 |     UC05 |     UC06 |     UC06 |     UC06 |     UC07 |     UC07 |     UC07 |     UC09 |     UC09 |     UC09 |     UC08 |     UC08 |     UC08 |   UC08 |     UC08 |\n",
            "|------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------|\n",
            "| F01  |    0   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.408248 | 0.288675 | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F02  |    0.5 | 0        | 0.612372 | 0.474342 | 0.288675 | 0.204124 | 0.188982 | 0        | 0.204124 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F03  |    0   | 0        | 0.182574 | 0.141421 | 0        | 0.182574 | 0.338062 | 0.258199 | 0.182574 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F04  |    0   | 0        | 0        | 0.158114 | 0        | 0.204124 | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 0.25     | 0        | 0        | 0        | 0        |    0   | 0        |\n",
            "| F05  |    0   | 0        | 0        | 0.158114 | 0        | 0.204124 | 0.188982 | 0        | 0.204124 | 1        | 0.353553 | 0.223607 | 0        | 0        | 0.223607 | 0.353553 | 0.25     | 0.288675 |    0   | 0        |\n",
            "| F06  |    0   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.288675 | 0        | 0.258199 | 0.816497 | 0.57735  | 0.258199 | 0.408248 | 0.288675 | 0.333333 |    0   | 0        |\n",
            "| F07  |    0   | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 0        | 0.316228 | 0        | 0        | 0.316228 | 1        | 0.707107 | 0.408248 |    0.5 | 0        |\n",
            "| F08  |    0   | 0.816497 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        |    0   | 0.816497 |\n",
            "+------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------+\n",
            "\n",
            "Data filter pengukuran maksmimum (txt)\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| id   |     uc01 |     uc02 |     uc03 |     uc04 |     uc05 |     uc06 |     uc07 |     uc08 |     uc09 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 0.57735  | 0        | 0.57735  | 0        | 0        | 0        | 0        | 0        | 0.408248 |\n",
            "| F02  | 0        | 0.612372 | 0.408248 | 0        | 0.474342 | 0.204124 | 0        | 0        | 0        |\n",
            "| F03  | 0        | 0.182574 | 0        | 0        | 0.182574 | 0.338062 | 0        | 0        | 0        |\n",
            "| F04  | 0        | 0        | 0.25     | 0        | 0.204124 | 0        | 0        | 0        | 0.353553 |\n",
            "| F05  | 0        | 0        | 0        | 0.288675 | 0.204124 | 0.204124 | 1        | 0.353553 | 0.223607 |\n",
            "| F06  | 0        | 0        | 0.288675 | 0        | 0        | 0        | 0.288675 | 0.408248 | 0.816497 |\n",
            "| F07  | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 1        | 0.316228 |\n",
            "| F08  | 0.408248 | 0.816497 | 0.408248 | 0        | 0        | 0        | 0        | 0.816497 | 0        |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (txt)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "| F08  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\n",
            "+------+----------+----------+----------+----------+----------+----------+--------+----------+----------+\n",
            "| id   |     uc01 |     uc02 |     uc03 |     uc04 |     uc05 |     uc06 |   uc07 |     uc08 |     uc09 |\n",
            "|------+----------+----------+----------+----------+----------+----------+--------+----------+----------|\n",
            "| F01  | 0.816497 | 0        | 0.408248 | 0        | 0        | 0        |   0    | 0        | 0.408248 |\n",
            "| F02  | 0        | 0.353553 | 0.353553 | 0        | 0        | 0        |   0    | 0        | 0        |\n",
            "| F03  | 0        | 0        | 0        | 0        | 0        | 0.258199 |   0    | 0        | 0        |\n",
            "| F04  | 0        | 0        | 0.353553 | 0        | 0.288675 | 0        |   0    | 0        | 0.353553 |\n",
            "| F05  | 0        | 0.353553 | 0        | 0.288675 | 0        | 0        |   0.75 | 0.353553 | 0        |\n",
            "| F06  | 0        | 0        | 0.408248 | 0        | 0        | 0        |   0    | 0.408248 | 0.816497 |\n",
            "| F07  | 0        | 0        | 0        | 0        | 0        | 0        |   0    | 1        | 0        |\n",
            "| F08  | 0        | 0        | 0        | 0        | 0        | 0        |   0    | 0        | 0        |\n",
            "+------+----------+----------+----------+----------+----------+----------+--------+----------+----------+\n",
            "\n",
            "Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "| F08  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "Data hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi)\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| id   |   uc01 |   uc02 |   uc03 |   uc04 |   uc05 |   uc06 |   uc07 |   uc08 |   uc09 |\n",
            "|------+--------+--------+--------+--------+--------+--------+--------+--------+--------|\n",
            "| F01  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F02  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F03  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F04  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |\n",
            "| F05  |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |      0 |\n",
            "| F06  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |\n",
            "| F07  |      0 |      0 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "| F08  |      0 |      1 |      0 |      0 |      0 |      0 |      0 |      1 |      0 |\n",
            "+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "Destructor called.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvOskvYXc9AL",
        "outputId": "74bb6eee-788f-444b-cc87-b35ff1ec1b7c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uc01', 'uc02', 'uc03', 'uc04', 'uc05', 'uc06', 'uc07', 'uc08', 'uc09']"
            ]
          },
          "metadata": {},
          "execution_count": 686
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLDgm2smg3SP"
      },
      "source": [
        "### Alternatif Method: wsd + path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C6jMEXP71dE",
        "outputId": "839ae652-ad92-40f8-9eb9-d2726cef3cf2"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "\n",
        "class wupalmer:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def convert_tag(self, tag):\n",
        "      \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
        "      \n",
        "      tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
        "      try:\n",
        "          return tag_dict[tag[0]]\n",
        "      except KeyError:\n",
        "          return None\n",
        "\n",
        "\n",
        "  def doc_to_synsets(self, doc):\n",
        "      \"\"\"\n",
        "      Returns a list of synsets in document.\n",
        "\n",
        "      Tokenizes and tags the words in the document doc.\n",
        "      Then finds the first synset for each word/tag combination.\n",
        "      If a synset is not found for that combination it is skipped.\n",
        "\n",
        "      Args:\n",
        "          doc: string to be converted\n",
        "\n",
        "      Returns:\n",
        "          list of synsets\n",
        "\n",
        "      Example:\n",
        "          doc_to_synsets('Fish are nvqjp friends.')\n",
        "          Out: [Synset('fish.n.01'), Synset('be.v.01'), Synset('friend.n.01')]\n",
        "      \"\"\"\n",
        "      tokens = nltk.word_tokenize(doc)\n",
        "      pos = nltk.pos_tag(tokens)\n",
        "      tags = [tag[1] for tag in pos]\n",
        "      wntag = [wupalmer.convert_tag(self, tag) for tag in tags]\n",
        "      ans = list(zip(tokens,wntag))\n",
        "      sets = [wn.synsets(x,y) for x,y in ans]\n",
        "      final = [val[0] for val in sets if len(val) > 0]\n",
        "      return final\n",
        "\n",
        "\n",
        "  def similarity_score(self, s1, s2):\n",
        "      \"\"\"\n",
        "      Calculate the normalized similarity score of s1 onto s2\n",
        "\n",
        "      For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
        "      Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
        "      number of largest similarity values found.\n",
        "\n",
        "      Args:\n",
        "          s1, s2: list of synsets from doc_to_synsets\n",
        "\n",
        "      Returns:\n",
        "          normalized similarity score of s1 onto s2\n",
        "\n",
        "      Example:\n",
        "          synsets1 = doc_to_synsets('I like cats')\n",
        "          synsets2 = doc_to_synsets('I like dogs')\n",
        "          similarity_score(synsets1, synsets2)\n",
        "          Out: 0.73333333333333339\n",
        "      \"\"\"\n",
        "      s =[]\n",
        "      for i1 in s1:\n",
        "          # r = []\n",
        "          scores = [x for x in [i1.path_similarity(i2) for i2 in s2] if x is not None]\n",
        "          if scores:\n",
        "              s.append(max(scores))\n",
        "      return sum(s)/len(s)\n",
        "\n",
        "\n",
        "  def document_path_similarity(self, doc1, doc2):\n",
        "      \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
        "\n",
        "      synsets1 = wupalmer.doc_to_synsets(self, doc1)\n",
        "      synsets2 = wupalmer.doc_to_synsets(self, doc2)\n",
        "\n",
        "      return (wupalmer.similarity_score(self, synsets1, synsets2) + wupalmer.similarity_score(self, synsets2, synsets1)) / 2\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  try:\n",
        "      myWsd = wupalmer()\n",
        "      data1 = freqs\n",
        "      data2 = ucd2\n",
        "      dt_a = [[myWsd.document_path_similarity(num, angka) for angka in data2.aksi] for num in data1.aksi]\n",
        "      df = pd.DataFrame(dt_a, index= data1.id, columns= data2.id)\n",
        "      print(tabulate(df, headers = 'keys', tablefmt = 'psql'))\n",
        "  except:\n",
        "    print(\"terjadi kesalahan\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+-----------+----------+----------+-----------+-----------+----------+-----------+-----------+----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
            "| id   |        1 |         2 |        3 |       3a |      3a.1 |      3a.2 |       3b |      3b.1 |      3b.2 |       3c |      3c.1 |     3c.2 |     3d.2a |   3d.2a.1 |   3d.2a.2 |     3d.2b |   3d.2b.1 |   3d.2b.2 |   3d.2b.2 |   3d.2b.2 |\n",
            "|------+----------+-----------+----------+----------+-----------+-----------+----------+-----------+-----------+----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------|\n",
            "| F01  | 0.103039 | 0.0941697 | 0.106366 | 0.110297 | 0.0934343 | 0.110109  | 0.316419 | 0.0960438 | 0.104554  | 0.110995 | 0.0957071 | 0.341498 | 0.479735  |  0.367014 | 0.365993  | 0.12037   | 0.113773  | 0.0914141 | 0.0853313 | 0.0941697 |\n",
            "| F02  | 0.415548 | 0.139731  | 0.441919 | 0.661265 | 0.111493  | 0.441667  | 0.415873 | 0.109844  | 0.440556  | 0.443056 | 0.127976  | 0.132424 | 0.0986111 |  0.115377 | 0.0945076 | 0.0977273 | 0.111715  | 0.133207  | 0.086039  | 0.139731  |\n",
            "| F03  | 0.14375  | 0.0803544 | 0.384028 | 0.319557 | 0.0866703 | 0.355833  | 0.500397 | 0.13217   | 0.354924  | 0.331155 | 0.0915404 | 0.129986 | 0.179762  |  0.151894 | 0.137294  | 0.0924752 | 0.128788  | 0.104281  | 0.0844836 | 0.0803544 |\n",
            "| F04  | 0.156566 | 0.096773  | 0.163889 | 0.322222 | 0.100063  | 0.3575    | 0.328522 | 0.142256  | 0.160625  | 0.114931 | 0.105114  | 0.341623 | 0.521429  |  0.386364 | 0.370726  | 0.118056  | 0.14678   | 0.125461  | 0.0875473 | 0.096773  |\n",
            "| F05  | 0.103788 | 0.0876311 | 0.564732 | 0.441044 | 0.103472  | 0.518482  | 0.464683 | 0.104398  | 0.515357  | 1        | 0.10625   | 0.103756 | 0.10625   |  0.107986 | 0.105619  | 0.661364  | 0.327336  | 0.0926136 | 0.0842907 | 0.0876311 |\n",
            "| F06  | 0.164276 | 0.0965007 | 0.175347 | 0.170483 | 0.0990741 | 0.16798   | 0.370189 | 0.147222  | 0.164276  | 0.368519 | 0.0990741 | 0.391667 | 0.851852  |  0.628241 | 0.421875  | 0.706019  | 0.409491  | 0.116667  | 0.0900932 | 0.0965007 |\n",
            "| F07  | 0.096859 | 0.0793651 | 0.117614 | 0.116274 | 0.1       | 0.115682  | 0.105982 | 0.101852  | 0.107348  | 0.661364 | 0.1       | 0.114646 | 0.114583  |  0.105903 | 0.117266  | 1         | 0.659722  | 0.0957071 | 0.0769231 | 0.0793651 |\n",
            "| F08  | 0.100404 | 0.848485  | 0.082634 | 0.096538 | 0.0768271 | 0.0912937 | 0.10241  | 0.0782383 | 0.0907443 | 0.081481 | 0.0848485 | 0.097711 | 0.100852  |  0.10483  | 0.0912202 | 0.0714286 | 0.0909361 | 0.0775613 | 0.0763889 | 0.848485  |\n",
            "+------+----------+-----------+----------+----------+-----------+-----------+----------+-----------+-----------+----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcODID7GGsG5"
      },
      "source": [
        "# Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ajZYTdSpVnV",
        "outputId": "3d93bf7c-0f21-4bec-a4a8-ebba14e54bc0"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class eval:\n",
        "  def __init__(self, dataFile= r'/content/mydrive/MyDrive/dataset/data_relasi.xlsx'):\n",
        "      self.data = dataFile\n",
        "\n",
        "  def fulldataset(self, dataGT = 'groundtruth'):\n",
        "      xl = pd.ExcelFile(self.data)\n",
        "      dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}\n",
        "      kalimat = dfs[dataGT]\n",
        "      kalimat_semua = kalimat.head(len(kalimat))\n",
        "      return kalimat_semua\n",
        "\n",
        "  def preprocessing(self):\n",
        "      xl = pd.ExcelFile(self.data)\n",
        "      for sh in xl.sheet_names:\n",
        "        df = xl.parse(sh)\n",
        "        print('Processing: [{}] ...'.format(sh))\n",
        "        print(df.head())\n",
        "\n",
        "  def evaluasi(self, y_actual, y_predicted):\n",
        "      a = metrics.accuracy_score(y_true= y_actual, y_pred= y_predicted)\n",
        "      b = metrics.precision_score(y_true= y_actual, y_pred= y_predicted, average= 'macro')\n",
        "      c = metrics.recall_score(y_true= y_actual, y_pred= y_predicted, average= 'macro')\n",
        "      d = metrics.classification_report(y_true= y_actual, y_pred= y_predicted)\n",
        "      print(\"akurasi:{}\\npresisi:{}\\nrecall:{}\\nreport:{}\".format(a, b, c, d))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myEval = eval()\n",
        "    tbl_grd = myEval.fulldataset()\n",
        "    tbl_grd = tbl_grd.drop(['index'], axis= 1)\n",
        "    tbl_grd.index= tbl_6.index\n",
        "    nl_eval = myEval.evaluasi(tbl_6.values.astype(int), tbl_grd.astype(int))\n",
        "    print(nl_eval)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "akurasi:0.25\n",
            "presisi:0.6666666666666666\n",
            "recall:0.462962962962963\n",
            "report:              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       1.00      0.33      0.50         3\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       1.00      1.00      1.00         1\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       1.00      0.50      0.67         2\n",
            "\n",
            "   micro avg       0.80      0.57      0.67        14\n",
            "   macro avg       0.67      0.46      0.52        14\n",
            "weighted avg       0.93      0.57      0.67        14\n",
            " samples avg       0.75      0.50      0.58        14\n",
            "\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v21nTXqOs1oU"
      },
      "source": [
        "### Manual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8IdWfjfGKsZ",
        "outputId": "9fff6faf-04e6-4a60-945c-88c207212956"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "#define array of actual values\n",
        "y_actual = [\n",
        "1,0,1,0,0,0,0,0,1,\n",
        "0,1,1,0,1,0,0,0,0,\n",
        "0,0,0,0,0,1,0,0,0,\n",
        "0,0,0,0,0,0,0,0,1,\n",
        "0,1,0,0,1,1,1,1,0,\n",
        "0,0,0,0,0,0,1,1,1,\n",
        "0,0,0,0,0,0,1,1,1,\n",
        "1,1,1,1,0,0,0,1,0,\n",
        "]\n",
        "\n",
        "#define array of predicted values\n",
        "y_predicted = [\n",
        "1,0,0,0,0,0,0,0,0,\n",
        "0,1,1,0,0,0,0,0,0,\n",
        "0,0,0,0,0,1,0,0,0,\n",
        "0,0,0,0,1,0,0,0,0,\n",
        "0,0,0,0,0,0,1,0,0,\n",
        "0,0,0,0,0,0,0,0,1,\n",
        "0,0,0,0,0,0,0,1,0,\n",
        "1,1,0,0,0,0,0,0,0,\n",
        "]\n",
        "\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true= y_actual, y_pred= y_predicted).ravel()\n",
        "\n",
        "print(\"false positif : \",fp)\n",
        "print(\"false negative : \",fn)\n",
        "print(\"true positive : \", tp) \n",
        "print(\"true negative : \", tn)\n",
        "\n",
        "print(\"akurasi\", metrics.accuracy_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"recall\", metrics.recall_score(y_true= y_actual, y_pred= y_predicted))\n",
        "print(\"presion\", metrics.precision_score(y_true= y_actual, y_pred= y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positif :  1\n",
            "false negative :  15\n",
            "true positive :  9\n",
            "true negative :  47\n",
            "akurasi 0.7777777777777778\n",
            "recall 0.375\n",
            "presion 0.9\n"
          ]
        }
      ]
    }
  ]
}