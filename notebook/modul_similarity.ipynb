{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modul_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMb+p6AO2RxnXhKR1LEzYDY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyrofist/Extraction-Requirement/blob/main/modul_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDQVliyJpsJa",
        "outputId": "fc1df7cb-e1e9-4aa3-9ae0-bb5f27d811ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAxJyUI4cVJE"
      },
      "source": [
        "!pip install py-automl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dOtdbBNUjLFR",
        "outputId": "169d66c9-a66e-4602-e845-a71476693185"
      },
      "source": [
        "#@title Modul Preprocessing { vertical-output: true }\n",
        "dataset1 = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "dataset2 = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"}\n",
        "indeks1 = \"2005 - Grid 3D\" #@param {type:\"string\"}\n",
        "indeks2 = \"Requirement Statement\" #@param {type:\"string\"}\n",
        "indeks3 = \"ID\" #@param {type:\"string\"}\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tabulate import tabulate\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "\n",
        "# download nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemming = PorterStemmer()\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "class cleaningProcess:\n",
        "  \n",
        "  def __init__(self, data):\n",
        "      self.index0 = data\n",
        "\n",
        "  # fungsi ini untuk mengecek data preprocessing\n",
        "  def preprocessing(self):\n",
        "      x1 = pd.ExcelFile(self.index0)\n",
        "      dfs = {sh:x1.parse(sh) for sh in x1.sheet_names}\n",
        "      return dfs\n",
        "\n",
        "  # fungsi ini digunakan untuk mengecek data secara keseluruhan dataset tertentu\n",
        "  def fulldataset(self, index1):\n",
        "      x1 = pd.ExcelFile(self.index0)\n",
        "      dfs = {sh:x1.parse(sh) for sh in x1.sheet_names}[index1]\n",
        "      return dfs\n",
        "\n",
        "  def clean_text(self, raw_text):\n",
        "      text = raw_text.lower()\n",
        "      tokens = nltk.word_tokenize(text)\n",
        "      token_words = [w for w in tokens if w.isalpha()]\n",
        "      lemma_words = [lem.lemmatize(w) for w in token_words]\n",
        "      meaningful_words = [w for w in lemma_words if not w in stops]\n",
        "      joined_words = ( \" \".join(meaningful_words))\n",
        "      return joined_words\n",
        "\n",
        "  # cleaning text\n",
        "  def apply_cleaning_function_to_list(self, X):\n",
        "      cleaned_X = [cleaningProcess.clean_text(self, element) for element in X]\n",
        "      return cleaned_X\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    myCleaning1 = cleaningProcess(dataset1)\n",
        "    list_req1 = myCleaning1.fulldataset(indeks1)[indeks2]\n",
        "    id_req1 = myCleaning1.fulldataset(indeks1)[indeks3]\n",
        "    cleaned_text1 = myCleaning1.apply_cleaning_function_to_list(list_req1)\n",
        "    data_raw1 = pd.DataFrame(cleaned_text1, index= id_req1, columns= ['cleaned'])\n",
        "    print(\"cleaning data {}\".format(dataset1))\n",
        "    print(tabulate(data_raw1, headers = 'keys', tablefmt = 'psql'))   \n",
        "    print(\"\\n\")\n",
        "    myCleaning2 = cleaningProcess(dataset2)\n",
        "    list_req2 = myCleaning2.fulldataset(indeks1)[indeks2]\n",
        "    id_req2 = myCleaning2.fulldataset(indeks1)[indeks3]\n",
        "    cleaned_text2 = myCleaning2.apply_cleaning_function_to_list(list_req2)\n",
        "    data_raw2 = pd.DataFrame(cleaned_text2, index= id_req2, columns= ['cleaned'])\n",
        "    print(\"cleaning data {}\".format(dataset2))\n",
        "    print(tabulate(data_raw2, headers = 'keys', tablefmt = 'psql'))   \n",
        "    \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "cleaning data /content/drive/MyDrive/dataset/dataset_2.xlsx\n",
            "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| ID   | cleaned                                                                                                                                                 |\n",
            "|------+---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| F01  | product shall plot data point scientifically correct manner                                                                                             |\n",
            "| F02  | grid axis labelled correctly according input data file                                                                                                  |\n",
            "| F03  | data point coloured accordance cluster number contained data file                                                                                       |\n",
            "| F04  | product able handle data point                                                                                                                          |\n",
            "| F05  | single click mouse data point bring name data point                                                                                                     |\n",
            "| F06  | mouse data point cause application display data point detail                                                                                            |\n",
            "| F07  | product allow multiple point clicked multiple name displayed                                                                                            |\n",
            "| F08  | product allow grid oriented user rotation zoom move function employed                                                                                   |\n",
            "| F09  | data file contain name data point parameter data point plotted single parameter designate colour point attribute used comparison description data point |\n",
            "| NF01 | point large enough see select                                                                                                                           |\n",
            "| NF02 | point big distort overall pattern point spread                                                                                                          |\n",
            "| NF03 | axis clearly labelled easily recognised grid ha oriented different position                                                                             |\n",
            "| NF04 | application coloured screen shot printed clearly black white background                                                                                 |\n",
            "| NF05 | application intuitive require specialist training                                                                                                       |\n",
            "| NF06 | program start within second depends number data point plotted                                                                                           |\n",
            "| NF07 | interaction data point delay longer second                                                                                                              |\n",
            "| NF08 | response change orientation fast enough avoid interrupting user flow thought                                                                            |\n",
            "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\n",
            "cleaning data /content/drive/MyDrive/dataset/dataset_2_split.xlsx\n",
            "+-------+------------------------------------------------------------------------------+\n",
            "| ID    | cleaned                                                                      |\n",
            "|-------+------------------------------------------------------------------------------|\n",
            "| F01   | product shall plot data point scientifically correct manner                  |\n",
            "| F02   | grid axis labelled correctly according input data file                       |\n",
            "| F03   | data point coloured accordance cluster number contained data file            |\n",
            "| F04   | product able handle data point                                               |\n",
            "| F05   | single click mouse data point bring name data point                          |\n",
            "| F06   | mouse data point cause application display data point detail                 |\n",
            "| F07   | product allow multiple point clicked multiple name displayed                 |\n",
            "| F08a  | product allow grid oriented user                                             |\n",
            "| F08b  | rotation function employed                                                   |\n",
            "| F08c  | zoom function employed                                                       |\n",
            "| F08d  | move function employed                                                       |\n",
            "| F09a  | data file contain name data point                                            |\n",
            "| F09b  | data file contain parameter data point plotted                               |\n",
            "| F09c  | data file contain single parameter designate colour point                    |\n",
            "| F09d  | attribute                                                                    |\n",
            "| F09e  | used comparison description data point                                       |\n",
            "| NF01a | point large enough see                                                       |\n",
            "| NF01b | point large enough select                                                    |\n",
            "| NF02  | point big distort overall pattern point spread                               |\n",
            "| NF03a | axis clearly labelled                                                        |\n",
            "| NF03b | axis easily recognised                                                       |\n",
            "| NF03c | grid ha oriented different position                                          |\n",
            "| NF04a | application coloured                                                         |\n",
            "| NF04b | screen shot printed clearly black white background                           |\n",
            "| NF05a | application intuitive                                                        |\n",
            "| NF05b | application require specialist training                                      |\n",
            "| NF06a | program start within second                                                  |\n",
            "| NF06b | depends number data point                                                    |\n",
            "| NF06c | plotted                                                                      |\n",
            "| NF07  | interaction data point delay longer second                                   |\n",
            "| NF08  | response change orientation fast enough avoid interrupting user flow thought |\n",
            "+-------+------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "piDq_Uwwn4xh",
        "outputId": "e18f82b1-b67b-4919-82f2-3a94392ce108"
      },
      "source": [
        "#@title Modul Similarity\n",
        "#@markdown Berikut ini adalah modul similarity yang digunakan meliputi\n",
        "#@markdown 1. Cosine similarity\n",
        "#@markdown 2. Jaccard similarity\n",
        "#@markdown 3. Levenshtein similarity\n",
        "#@markdown 4. TFIDF\n",
        "#@markdown 5. VSM\n",
        "#@markdown 6. PMI\n",
        "parameter = \"cosine\" #@param [\"cosine\", \"jaccard\", \"levenshtein\", \"tfidf\", \"vsm\", \"pmi\"]\n",
        "\n",
        "import nltk, math, statistics\n",
        "import re, collections\n",
        "import numpy as np\n",
        "import string #allows for format()\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class similarityMeasurement:\n",
        "  \n",
        "  def __init__(self):\n",
        "      pass\n",
        "      \n",
        "  # lavenstein\n",
        "  def similarity_levenshtein(self, s1, s2):\n",
        "      if len(s1) < len(s2):\n",
        "          return similarityMeasurement.similarity_levenshtein(self, s2, s1)\n",
        "      if len(s2) == 0:\n",
        "          return len(s1)\n",
        "\n",
        "      previous_row = range(len(s2) + 1)\n",
        "      for i, c1 in enumerate(s1):\n",
        "          current_row = [i + 1]\n",
        "          for j, c2 in enumerate(s2):\n",
        "              insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
        "              deletions = current_row[j] + 1       # than s2\n",
        "              substitutions = previous_row[j] + (c1 != c2)\n",
        "              current_row.append(min(insertions, deletions, substitutions))\n",
        "          previous_row = current_row    \n",
        "      return previous_row[-1]\n",
        "\n",
        "  def measurement_levenshtein(self, data1, data2):\n",
        "      data = [[similarityMeasurement.similarity_levenshtein(self, num, angka) for angka in data2] for num in data1]\n",
        "      return data\n",
        "\n",
        "  # Cosine Similarity \n",
        "  def similarity_cosine(self, data1, data2):\n",
        "      X_list = word_tokenize(data1) \n",
        "      Y_list = word_tokenize(data2) \n",
        "      sw = stopwords.words('english') \n",
        "      l1 =[];l2 =[] \n",
        "      X_set = {w for w in X_list if not w in sw} \n",
        "      Y_set = {w for w in Y_list if not w in sw} \n",
        "      rvector = X_set.union(Y_set) \n",
        "      for w in rvector: \n",
        "        if w in X_set: l1.append(1) # create a vector \n",
        "        else: l1.append(0) \n",
        "        if w in Y_set: l2.append(1) \n",
        "        else: l2.append(0) \n",
        "      c = 0\n",
        "\n",
        "      # cosine formula \n",
        "      for i in range(len(rvector)): \n",
        "          c+= l1[i]*l2[i] \n",
        "      cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
        "      return cosine \n",
        "\n",
        "  def measurement_cosine(self, data1, data2):\n",
        "      data = [[similarityMeasurement.similarity_cosine(self, num, angka) for angka in data2] for num in data1]\n",
        "      return data\n",
        "\n",
        "  # jaccard similarity\n",
        "  def similarity_jaccard(self, list1, list2):\n",
        "      intersection = len(list(set(list1).intersection(list2)))\n",
        "      union = (len(list1) + len(list2)) - intersection\n",
        "      return float(intersection) / union\n",
        "\n",
        "  def measurement_jaccard(self, data1, data2):\n",
        "      data = [[similarityMeasurement.similarity_jaccard(self, num, angka) for angka in data2] for num in data1]\n",
        "      return data\n",
        "\n",
        "  # term frequency - inverse document frequency\n",
        "  def tfidf(self, data):\n",
        "      vect = TfidfVectorizer(stop_words='english')\n",
        "      tfidf_matrix = vect.fit_transform(data)\n",
        "      return tfidf_matrix.toarray()\n",
        "\n",
        "  # vsm similarity\n",
        "  def vsm_similarity(self, data1, data2):\n",
        "      hasil1 = similarityMeasurement.tfidf(self, data1)\n",
        "      hasil2 = similarityMeasurement.tfidf(self, data2)\n",
        "      vsm = cosine_similarity(hasil1, hasil2)\n",
        "      return vsm\n",
        "\n",
        "  def pmi_measurement(self, text1, text2):\n",
        "      stopwords_ = set(stopwords.words('english'))\n",
        "      words1 = [word.lower() for word in text1.split() if len(word) > 2 and word not in stopwords_]\n",
        "      words2 = [word.lower() for word in text2.split() if len(word) > 2 and word not in stopwords_]\n",
        "\n",
        "      finder = BigramCollocationFinder.from_words(words1+words2)\n",
        "      bgm = BigramAssocMeasures()\n",
        "      score = bgm.mi_like\n",
        "      collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\n",
        "      return collocations\n",
        "\n",
        "  def pmi_jumlah(self, text1, text2):\n",
        "      stopwords_ = set(stopwords.words('english'))\n",
        "      words1 = [word.lower() for word in text1.split() if len(word) > 2 and word not in stopwords_]\n",
        "      words2 = [word.lower() for word in text2.split() if len(word) > 2 and word not in stopwords_]\n",
        "      finder = BigramCollocationFinder.from_words(words1+words2)\n",
        "      bgm = BigramAssocMeasures()\n",
        "      score = bgm.mi_like\n",
        "      total_pmi = sum([math.log(pmi) for bigram, pmi in finder.score_ngrams(score)])\n",
        "      return total_pmi\n",
        "\n",
        "  def measurement_pmi_jumlah(self, data1, data2):\n",
        "      data = [[similarityMeasurement.pmi_jumlah(self, num, angka) for angka in data2] for num in data1]\n",
        "      return data\n",
        "\n",
        "  def co_occurrence(self, sentences, window_size= 2):\n",
        "        d = collections.defaultdict(int)\n",
        "        vocab = set()\n",
        "        for text in sentences:\n",
        "            # preprocessing (use tokenizer instead)\n",
        "            text = text.lower().split()\n",
        "            # iterate over sentences\n",
        "            for i in range(len(text)):\n",
        "                token = text[i]\n",
        "                vocab.add(token)  # add to vocab\n",
        "                next_token = text[i+1 : i+1+window_size]\n",
        "                for t in next_token:\n",
        "                    key = tuple( sorted([t, token]) )\n",
        "                    d[key] += 1\n",
        "\n",
        "        # formulate the dictionary into dataframe\n",
        "        vocab = sorted(vocab) # sort vocab\n",
        "        df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16), index=vocab, columns=vocab)\n",
        "        for key, value in d.items():\n",
        "            df.at[key[0], key[1]] = value\n",
        "            df.at[key[1], key[0]] = value\n",
        "        return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  if parameter == \"cosine\":\n",
        "      print(\"Data Parameter {}\".format(parameter))\n",
        "      mySim = similarityMeasurement()\n",
        "      data_cosine = mySim.measurement_cosine(cleaned_text1, cleaned_text2)\n",
        "      df_cosine = pd.DataFrame(data_cosine, index= id_req1, columns= id_req2)\n",
        "      print(tabulate(df_cosine, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "  elif parameter == \"jaccard\":\n",
        "      print(\"Data Parameter {}\".format(parameter))\n",
        "      data_jaccard = mySim.measurement_jaccard(cleaned_text1, cleaned_text2)\n",
        "      df_jaccard = pd.DataFrame(data_jaccard, index= id_req1, columns= id_req2)\n",
        "      print(tabulate(df_jaccard, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "  elif parameter == \"levenshtein\":\n",
        "      print(\"Data Parameter {}\".format(parameter))\n",
        "      data_levenshtein = mySim.measurement_levenshtein(cleaned_text1, cleaned_text2)\n",
        "      df_levensthein = pd.DataFrame(data_levenshtein, index= id_req1, columns= id_req2)\n",
        "      print(tabulate(df_levensthein, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "  elif parameter == \"tfidf\":\n",
        "      print(\"Data Parameter {}\".format(parameter))\n",
        "      data_tfidf1 = mySim.tfidf(cleaned_text1)\n",
        "      df_tfidf1 = pd.DataFrame(data_tfidf1, index= id_req1)\n",
        "      print(tabulate(df_tfidf1, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "      data_tfidf2 = mySim.tfidf(cleaned_text2)\n",
        "      df_tfidf2 = pd.DataFrame(data_tfidf2, index= id_req2)\n",
        "      print(tabulate(df_tfidf2, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "  elif parameter == \"vsm\":\n",
        "      print(\"Data Parameter {}\".format(parameter))\n",
        "      data_vsm = mySim.vsm_similarity(cleaned_text1, cleaned_text2)\n",
        "      df_vsm = pd.DataFrame(data_vsm, index= id_req1, columns= id_req2)\n",
        "      print(tabulate(df_vsm, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "  if parameter == \"pmi\":\n",
        "      print(\"Data Parameter {}\".format(parameter))\n",
        "      data_pmi_jumlah = mySim.measurement_pmi_jumlah(cleaned_text1, cleaned_text2)\n",
        "      df_pmi_jumlah = pd.DataFrame(data_pmi_jumlah, index= id_req1, columns= id_req2)\n",
        "      print(tabulate(df_pmi_jumlah, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Parameter cosine\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "| ID   |      F01 |      F02 |      F03 |      F04 |      F05 |      F06 |      F07 |     F08a |     F08b |     F08c |     F08d |     F09a |     F09b |     F09c |     F09d |     F09e |    NF01a |    NF01b |     NF02 |    NF03a |    NF03b |    NF03c |    NF04a |    NF04b |    NF05a |    NF05b |    NF06a |    NF06b |    NF06c |     NF07 |     NF08 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 1        | 0.125    | 0.25     | 0.474342 | 0.267261 | 0.267261 | 0.267261 | 0.158114 | 0        | 0        | 0        | 0.316228 | 0.288675 | 0.25     | 0        | 0.316228 | 0.176777 | 0.176777 | 0.144338 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.353553 | 0        | 0.288675 | 0        |\n",
            "| F02  | 0.125    | 1        | 0.25     | 0.158114 | 0.133631 | 0.133631 | 0        | 0.158114 | 0        | 0        | 0        | 0.316228 | 0.288675 | 0.25     | 0        | 0.158114 | 0        | 0        | 0        | 0.408248 | 0.204124 | 0.158114 | 0        | 0        | 0        | 0        | 0        | 0.176777 | 0        | 0.144338 | 0        |\n",
            "| F03  | 0.25     | 0.25     | 1        | 0.316228 | 0.267261 | 0.267261 | 0.133631 | 0        | 0        | 0        | 0        | 0.474342 | 0.433013 | 0.375    | 0        | 0.316228 | 0.176777 | 0.176777 | 0.144338 | 0        | 0        | 0        | 0.25     | 0        | 0        | 0        | 0        | 0.53033  | 0        | 0.288675 | 0        |\n",
            "| F04  | 0.474342 | 0.158114 | 0.316228 | 1        | 0.338062 | 0.338062 | 0.338062 | 0.2      | 0        | 0        | 0        | 0.4      | 0.365148 | 0.316228 | 0        | 0.4      | 0.223607 | 0.223607 | 0.182574 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.447214 | 0        | 0.365148 | 0        |\n",
            "| F05  | 0.267261 | 0.133631 | 0.267261 | 0.338062 | 1        | 0.428571 | 0.285714 | 0        | 0        | 0        | 0        | 0.507093 | 0.308607 | 0.400892 | 0        | 0.338062 | 0.188982 | 0.188982 | 0.154303 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.377964 | 0        | 0.308607 | 0        |\n",
            "| F06  | 0.267261 | 0.133631 | 0.267261 | 0.338062 | 0.428571 | 1        | 0.142857 | 0        | 0        | 0        | 0        | 0.338062 | 0.308607 | 0.267261 | 0        | 0.338062 | 0.188982 | 0.188982 | 0.154303 | 0        | 0        | 0        | 0.267261 | 0        | 0.267261 | 0.188982 | 0        | 0.377964 | 0        | 0.308607 | 0        |\n",
            "| F07  | 0.267261 | 0        | 0.133631 | 0.338062 | 0.285714 | 0.142857 | 1        | 0.338062 | 0        | 0        | 0        | 0.338062 | 0.154303 | 0.133631 | 0        | 0.169031 | 0.188982 | 0.188982 | 0.154303 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.188982 | 0        | 0.154303 | 0        |\n",
            "| F08  | 0.111803 | 0.111803 | 0        | 0.141421 | 0        | 0        | 0.239046 | 0.707107 | 0.547723 | 0.547723 | 0.547723 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.282843 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.1      |\n",
            "| F09  | 0.188982 | 0.188982 | 0.283473 | 0.239046 | 0.404061 | 0.202031 | 0.202031 | 0        | 0        | 0        | 0        | 0.597614 | 0.654654 | 0.755929 | 0.267261 | 0.597614 | 0.133631 | 0.133631 | 0.109109 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.267261 | 0.267261 | 0.218218 | 0        |\n",
            "| NF01 | 0.158114 | 0        | 0.158114 | 0.2      | 0.169031 | 0.169031 | 0.169031 | 0        | 0        | 0        | 0        | 0.2      | 0.182574 | 0.158114 | 0        | 0.2      | 0.894427 | 0.894427 | 0.182574 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.223607 | 0        | 0.182574 | 0.141421 |\n",
            "| NF02 | 0.144338 | 0        | 0.144338 | 0.182574 | 0.154303 | 0.154303 | 0.154303 | 0        | 0        | 0        | 0        | 0.182574 | 0.166667 | 0.144338 | 0        | 0.182574 | 0.204124 | 0.204124 | 1        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.204124 | 0        | 0.166667 | 0        |\n",
            "| NF03 | 0        | 0.33541  | 0        | 0        | 0        | 0        | 0        | 0.282843 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.547723 | 0.547723 | 0.707107 | 0        | 0.119523 | 0        | 0        | 0        | 0        | 0        | 0        | 0        |\n",
            "| NF04 | 0        | 0        | 0.117851 | 0        | 0        | 0.125988 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.19245  | 0        | 0        | 0.471405 | 0.881917 | 0.235702 | 0.166667 | 0        | 0        | 0        | 0        | 0        |\n",
            "| NF05 | 0        | 0        | 0        | 0        | 0        | 0.169031 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.316228 | 0        | 0.632456 | 0.894427 | 0        | 0        | 0        | 0        | 0        |\n",
            "| NF06 | 0.235702 | 0.117851 | 0.353553 | 0.298142 | 0.251976 | 0.251976 | 0.125988 | 0        | 0        | 0        | 0        | 0.298142 | 0.408248 | 0.235702 | 0        | 0.298142 | 0.166667 | 0.166667 | 0.136083 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.666667 | 0.666667 | 0.333333 | 0.408248 | 0        |\n",
            "| NF07 | 0.288675 | 0.144338 | 0.288675 | 0.365148 | 0.308607 | 0.308607 | 0.154303 | 0        | 0        | 0        | 0        | 0.365148 | 0.333333 | 0.288675 | 0        | 0.365148 | 0.204124 | 0.204124 | 0.166667 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.204124 | 0.408248 | 0        | 1        | 0        |\n",
            "| NF08 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.141421 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0.158114 | 0.158114 | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 0        | 1        |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "il58myMulHvq",
        "outputId": "0f6e3b73-0a32-425d-ea44-70ce3a70743b"
      },
      "source": [
        "#@title Modul Visualisasi dan Threshold\n",
        "parameter = \"vsm\" #@param [\"cosine\", \"jaccard\", \"levenshtein\", \"tfidf\", \"vsm\", \"pmi\"]\n",
        "#@markdown Berikut ini adalah deskripsi singkat, bagaimana threshold mempengaruhi nilai statistik dari setiap data yang akan dilihat\n",
        "%matplotlib notebook\n",
        "from sklearn.decomposition import PCA\n",
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "class visualThres:\n",
        "  \n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  # visualisasi dan threshold\n",
        "  def threshold_value(self, threshold, data):\n",
        "      dt = data.values >= threshold\n",
        "      dt1 = pd.DataFrame(dt, index= data.index, columns= data.columns)\n",
        "      mask = dt1.isin([True])\n",
        "      dt3 = dt1.where(mask, other= 0)\n",
        "      mask2 = dt3.isin([False])\n",
        "      th_cosine1 = dt3.where(mask2, other= 1)\n",
        "      return th_cosine1\n",
        "\n",
        "  def sent_PCA(self, sentence, n = 2):\n",
        "      pca = PCA(n_components = n)\n",
        "      pca.fit(np.array(sentence).transpose())\n",
        "      variance = np.array(pca.explained_variance_ratio_)\n",
        "      words = []\n",
        "      for _ in range(n):\n",
        "          idx = np.argmax(variance)\n",
        "          words.append(np.amax(variance) * sentence[idx])\n",
        "          variance[idx] = 0\n",
        "      return np.sum(words, axis = 0)\n",
        "\n",
        "  def visualisasi(self, data):\n",
        "      import matplotlib.pyplot as plt\n",
        "      from sklearn.decomposition import PCA\n",
        "      pca = PCA(n_components=len(data))\n",
        "      my_pca = pca.fit_transform(data)\n",
        "      plt.scatter(my_pca[:,0], my_pca[:,1])\n",
        "      for i, word in enumerate(data.index):\n",
        "          plt.annotate(word, xy=(my_pca[i,0], my_pca[i,1]))\n",
        "          plt.xlabel('widht')\n",
        "          plt.ylabel('height')\n",
        "      plt.title('Visualisasi')\n",
        "      plt.show()\n",
        "\n",
        "  def npstat(self, data):\n",
        "      maximum = np.max(data)\n",
        "      minimum = np.min(data)\n",
        "      mean = np.mean(data)\n",
        "      variance = np.var(data)\n",
        "      median = np.median(data)\n",
        "      standar_deviasi = np.std(data)\n",
        "      dataBaru = [maximum, minimum, mean, variance, median, standar_deviasi]\n",
        "      return dataBaru\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     id_default = ['maks', 'min', 'mean', 'var', 'med', 'std_dev']\n",
        "     id_kolom = ['stat_value']\n",
        "     if parameter == \"cosine\":\n",
        "        print(\"\\nstatistic {}\".format(parameter))\n",
        "        myVisual = visualThres()\n",
        "        stat_cosine = myVisual.npstat(data_cosine)\n",
        "        th_cosine = myVisual.threshold_value(stat_cosine[2], df_cosine)\n",
        "        stat_th_cosine = myVisual.npstat(th_cosine.values)\n",
        "        stat_cosine_df = pd.DataFrame(stat_th_cosine, index= id_default, columns= id_kolom)\n",
        "        print(tabulate(stat_cosine_df, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "     elif parameter == \"jaccard\":\n",
        "        print(\"\\nstatistic {}\".format(parameter))\n",
        "        stat_jaccard = myVisual.npstat(data_jaccard)\n",
        "        th_jaccard = myVisual.threshold_value(stat_jaccard[2], df_jaccard)\n",
        "        stat_th_jaccard = myVisual.npstat(th_jaccard.values)\n",
        "        stat_jaccard_df = pd.DataFrame(stat_th_jaccard, index= id_default, columns= id_kolom)\n",
        "        print(tabulate(stat_jaccard_df, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "     elif parameter == \"levenshtein\":\n",
        "        print(\"\\nstatistic {}\".format(parameter))\n",
        "        stat_levenshtein = myVisual.npstat(data_levenshtein)\n",
        "        th_levenshtein = myVisual.threshold_value(stat_levenshtein[2], df_levensthein)\n",
        "        stat_th_levenshtein = myVisual.npstat(th_levenshtein.values)\n",
        "        stat_levenshtein_df = pd.DataFrame(stat_th_levenshtein, index= id_default, columns= id_kolom)\n",
        "        print(tabulate(stat_levenshtein_df, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "     elif parameter == \"tfidf\":\n",
        "        print(\"\\nstatistic {}\".format(parameter))\n",
        "        stat_tfidf1 = myVisual.npstat(data_tfidf1)\n",
        "        th_tfidf1 = myVisual.threshold_value(stat_tfidf1[2], df_tfidf1)\n",
        "        stat_th_tfidf1 = myVisual.npstat(th_tfidf1.values)\n",
        "        stat_tfidf1_df = pd.DataFrame(stat_th_tfidf1, index= id_default, columns= id_kolom)\n",
        "        print(tabulate(stat_tfidf1_df, headers = 'keys', tablefmt = 'psql'))  \n",
        "\n",
        "     elif parameter == \"vsm\":\n",
        "        print(\"\\nstatistic {}\".format(parameter))\n",
        "        stat_vsm = myVisual.npstat(data_vsm)\n",
        "        th_vsm = myVisual.threshold_value(stat_vsm[2], df_vsm)\n",
        "        stat_th_vsm = myVisual.npstat(th_vsm.values)\n",
        "        stat_vsm_df = pd.DataFrame(stat_vsm, index= id_default, columns= id_kolom)\n",
        "        print(tabulate(stat_vsm_df, headers = 'keys', tablefmt = 'psql'))  \n",
        "\n",
        "    #  elif parameter == \"pmi\": #masih error\n",
        "    #     # df_pmi_jumlah = pd.DataFrame(data_pmi_jumlah)\n",
        "    #     th_pmi_jumlah = myVisual.threshold_value(stat_pmi_jumlah[0], df_pmi_jumlah)\n",
        "    #     stat_th_pmi_jumlah = npstat(th_pmi_jumlah.values)\n",
        "    #     print(\"\\nstatistic pmi\")\n",
        "    #     stat_pmi_df = pd.DataFrame(stat_th_pmi_jumlah, index= id_default, columns= id_kolom)\n",
        "    #     print(tabulate(stat_pmi_df, headers = 'keys', tablefmt = 'psql'))  "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "statistic vsm\n",
            "+---------+--------------+\n",
            "|         |   stat_value |\n",
            "|---------+--------------|\n",
            "| maks    |    0.999981  |\n",
            "| min     |    0         |\n",
            "| mean    |    0.100698  |\n",
            "| var     |    0.0378679 |\n",
            "| med     |    0         |\n",
            "| std_dev |    0.194597  |\n",
            "+---------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dHVJtM6TcqcO",
        "outputId": "046f3d59-264a-4649-ccb1-f862140ae908"
      },
      "source": [
        "#@title Modul Evaluasi\n",
        "#@markdown Berikut ini adalah modul evaluasi, menunjukkan data evaluasi antara groundtruth dan data testing\n",
        "parameter = \"cosine\" #@param [\"cosine\", \"jaccard\", \"levenshtein\", \"tfidf\", \"vsm\", \"pmi\"]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyAutoML.ml import ML,ml, EncodeCategorical\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class pengukuranEvaluasi:\n",
        "  def __init__(self, dataPertama, dataKedua):\n",
        "      self.data1 = dataPertama\n",
        "      self.data2 = dataKedua\n",
        "\n",
        "  def ukur_evaluasi(self):\n",
        "      X_train, X_test, y_train, y_test = train_test_split(self.data1, self.data2, test_size=0.3,random_state=109) # 70% training and 30% test\n",
        "      y_train = y_train.argmax(axis= 1)\n",
        "      X = X_train\n",
        "      Y = y_train\n",
        "      Y = EncodeCategorical(Y)\n",
        "      size = 0.4\n",
        "      return ML(X, Y, size, SVC(), RandomForestClassifier(), DecisionTreeClassifier(), KNeighborsClassifier(), LogisticRegression(max_iter = 7000))      \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  if parameter == 'levenshtein':\n",
        "    myEvaluasi = pengukuranEvaluasi(data_levenshtein, th_levenshtein.values) # normal\n",
        "    myEvaluasi.ukur_evaluasi()\n",
        "  elif parameter == 'cosine':\n",
        "    myEvaluasi = pengukuranEvaluasi(data_cosine, th_cosine.values) #evaluasi\n",
        "    myEvaluasi.ukur_evaluasi()    \n",
        "  elif parameter == 'jaccard':\n",
        "    myEvaluasi = pengukuranEvaluasi(data_jaccard, th_jaccard.values) #evaluasi\n",
        "    myEvaluasi.ukur_evaluasi()\n",
        "  elif parameter == 'tfidf':\n",
        "    myEvaluasi = pengukuranEvaluasi(data_tfidf1, th_tfidf1.values) #evaluasi\n",
        "    myEvaluasi.ukur_evaluasi()\n",
        "  elif parameter == 'vsm':\n",
        "    myEvaluasi = pengukuranEvaluasi(data_vsm, th_vsm.values) # evaluasi \n",
        "    myEvaluasi.ukur_evaluasi()\n",
        "  # elif parameter == 'pmi':\n",
        "  #   myEvaluasi = pengukuranEvaluasi(data_pmi_jumlah, th_pmi_jumlah.values) # evaluasi\n",
        "  #   myEvaluasi.ukur_evaluasi()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________________________________________\n",
            ".....................Py-AutoML......................\n",
            "____________________________________________________\n",
            "SVC ______________________________ \n",
            "\n",
            "Accuracy Score for SVC is \n",
            "0.8\n",
            "\n",
            "\n",
            "Confusion Matrix for SVC is \n",
            "[[4 0]\n",
            " [1 0]]\n",
            "\n",
            "\n",
            "Classification Report for SVC is \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80         5\n",
            "   macro avg       0.40      0.50      0.44         5\n",
            "weighted avg       0.64      0.80      0.71         5\n",
            "\n",
            "\n",
            "\n",
            "____________________________________________________\n",
            "RandomForestClassifier ______________________________ \n",
            "\n",
            "Accuracy Score for RandomForestClassifier is \n",
            "0.8\n",
            "\n",
            "\n",
            "Confusion Matrix for RandomForestClassifier is \n",
            "[[4 0]\n",
            " [1 0]]\n",
            "\n",
            "\n",
            "Classification Report for RandomForestClassifier is \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80         5\n",
            "   macro avg       0.40      0.50      0.44         5\n",
            "weighted avg       0.64      0.80      0.71         5\n",
            "\n",
            "\n",
            "\n",
            "____________________________________________________\n",
            "DecisionTreeClassifier ______________________________ \n",
            "\n",
            "Accuracy Score for DecisionTreeClassifier is \n",
            "0.8\n",
            "\n",
            "\n",
            "Confusion Matrix for DecisionTreeClassifier is \n",
            "[[4 0]\n",
            " [1 0]]\n",
            "\n",
            "\n",
            "Classification Report for DecisionTreeClassifier is \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80         5\n",
            "   macro avg       0.40      0.50      0.44         5\n",
            "weighted avg       0.64      0.80      0.71         5\n",
            "\n",
            "\n",
            "\n",
            "____________________________________________________\n",
            "KNeighborsClassifier ______________________________ \n",
            "\n",
            "Accuracy Score for KNeighborsClassifier is \n",
            "0.8\n",
            "\n",
            "\n",
            "Confusion Matrix for KNeighborsClassifier is \n",
            "[[4 0]\n",
            " [1 0]]\n",
            "\n",
            "\n",
            "Classification Report for KNeighborsClassifier is \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80         5\n",
            "   macro avg       0.40      0.50      0.44         5\n",
            "weighted avg       0.64      0.80      0.71         5\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________________________________________\n",
            "LogisticRegression ______________________________ \n",
            "\n",
            "Accuracy Score for LogisticRegression is \n",
            "0.8\n",
            "\n",
            "\n",
            "Confusion Matrix for LogisticRegression is \n",
            "[[4 0]\n",
            " [1 0]]\n",
            "\n",
            "\n",
            "Classification Report for LogisticRegression is \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80         5\n",
            "   macro avg       0.40      0.50      0.44         5\n",
            "weighted avg       0.64      0.80      0.71         5\n",
            "\n",
            "\n",
            "\n",
            "                    Model Accuracy\n",
            "0                     SVC      0.8\n",
            "1  RandomForestClassifier      0.8\n",
            "2  DecisionTreeClassifier      0.8\n",
            "3    KNeighborsClassifier      0.8\n",
            "4      LogisticRegression      0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}