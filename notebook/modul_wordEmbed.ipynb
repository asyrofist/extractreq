{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "modul_wordEmbed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyrofist/Extraction-Requirement/blob/main/modul_wordEmbed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdXypMBsU8yI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqU-GAnb9ojR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jC2mZLnCJ16"
      },
      "source": [
        "!pip install tracereq==0.0.2\n",
        "!pip install py-automl\n",
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbpx8StOKxqO"
      },
      "source": [
        "# Sent Embed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYGcY53N4pNk",
        "cellView": "form"
      },
      "source": [
        "#@title Modul Sentence Modeling { vertical-output: true }\n",
        "#@markdown - One of the methods to represent sentences as vectors (Mu et al 2017)\n",
        "#@markdown - Computing vector representations of each embedded word, and weight average them using PCA\n",
        "#@markdown     - If there are **n** words in a sentence, select **N** words with high explained variance (n>N)\n",
        "#@markdown     - Most of \"energy\" (around 80%) can be containted using only 4 words (N=4) in the original paper (Mu et al 2017)\n",
        "\n",
        "#@markdown ## Parameter\n",
        "dataset_param1 = '/content/drive/MyDrive/dataset/dataset_2.xlsx' #@param {type:\"string\"}\n",
        "dataset_param2 = '/content/drive/MyDrive/dataset/dataset_2_split.xlsx' #@param {type:\"string\"}\n",
        "dataset_index = '2005 - Grid 3D' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Load Model\n",
        "th_param =  3#@param {type:\"number\"}\n",
        "sg_param =  1#@param {type:\"number\"}\n",
        "window_param =  2#@param {type:\"number\"}\n",
        "min_count_param = 1 #@param {type:\"number\"}\n",
        "iter_param = 100 #@param {type:\"number\"}\n",
        "n_param =  3#@param {type:\"number\"}\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from multiprocessing import Pool\n",
        "from scipy import spatial\n",
        "from sklearn.decomposition import PCA\n",
        "from traceability import prosesData\n",
        "from tabulate import tabulate\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class sentenceMod:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def sent_model(self, data, th_param):\n",
        "      # sentences = data # import the corpus and convert into a list\n",
        "      sentences = [num.split() for num in data] # import the corpus and convert into a list\n",
        "      threshold = th_param # set threshold to consider only sentences longer than certain integer\n",
        "      for i in range(len(sentences)):\n",
        "          if len(sentences[i]) < threshold:\n",
        "              sentences[i] = None\n",
        "\n",
        "      sentences = [sentence for sentence in sentences if sentence is not None] \n",
        "      model = Word2Vec(sentences = sentences, size = len(sentences), sg = sg_param, window = window_param, min_count = min_count_param, iter = iter_param, workers = Pool()._processes)\n",
        "      model.init_sims(replace = True)\n",
        "      for i in range(len(sentences)): # converting each word into its vector representation\n",
        "          sentences[i] = [model[word] for word in sentences[i]]\n",
        "      return sentences\n",
        "\n",
        "  def sent_PCA(self, sentence, n = n_param): # define function to compute weighted vector representation of sentence\n",
        "      pca = PCA(n_components = n) # dataset_param 'n' means number of words to be accounted when computing weighted average\n",
        "      pca.fit(np.array(sentence).transpose())\n",
        "      variance = np.array(pca.explained_variance_ratio_)\n",
        "      words = []\n",
        "      for _ in range(n):\n",
        "          idx = np.argmax(variance)\n",
        "          words.append(np.amax(variance) * sentence[idx])\n",
        "          variance[idx] = 0\n",
        "      return np.sum(words, axis = 0)    \n",
        "\n",
        "  # define a function that computes cosine similarity between two words\n",
        "  def cosine_similarity(self, v1, v2):\n",
        "      return 1 - spatial.distance.cosine(v1, v2)    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myProses1 = prosesData(dataset_param1)\n",
        "    list_data1 = list(myProses1.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data1 = list(myProses1.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text1 = myProses1.apply_cleaning_function_to_list(list_data1)\n",
        "\n",
        "    myDoc = sentenceMod()\n",
        "    data_sent1 = myDoc.sent_model(cleaned_text1, th_param)\n",
        "    sent_v1 = [myDoc.sent_PCA(num) for num in data_sent1] # computing vector representation of each sentence\n",
        "\n",
        "    a = [[myDoc.cosine_similarity(num, angka) for angka in sent_v1] for num in sent_v1]\n",
        "    df = pd.DataFrame(a, index= id_data1, columns= id_data1)\n",
        "    print(tabulate(df, headers = 'keys', tablefmt = 'psql'))   \n",
        "\n",
        "    # myUkur= pengukuranEvaluasi(df.values, df.values)\n",
        "    # myUkur.ukur_evaluasi()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXlE3bo8CfZ4",
        "cellView": "form"
      },
      "source": [
        "#@title Modul doc2vec\n",
        "#@markdown Berikut ini penjelasan singkat bagaimana modul ini telah dibuat.\n",
        "\n",
        "#@markdown ### Preprocess data\n",
        "#@markdown - Use re module to preprocess data\n",
        "#@markdown - Convert all letters into lowercase\n",
        "#@markdown - Remove punctuations, numbers, etc.\n",
        "#@markdown - For the doc2vec model, input data should be in format of **iterable TaggedDocuments\"**\n",
        "#@markdown - Each TaggedDocument instance comprises **words** and **tags**\n",
        "#@markdown - Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable\n",
        "dataset_var = '/content/drive/MyDrive/dataset/dataset_2.xlsx' #@param {type:\"string\"}\n",
        "dataset_index = '2005 - Grid 3D' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown  ### Create and train model\n",
        "#@markdown - Create a doc2vec model and train it with Hamlet corpus\n",
        "#@markdown - Key parameter description (https://radimrehurek.com/gensim/models/doc2vec.html)\n",
        "#@markdown - **documents**: training data (has to be iterable TaggedDocument instances)\n",
        "#@markdown - **size**: dimension of embedding space\n",
        "#@markdown - **dm**: DBOW if 0, distributed-memory if 1\n",
        "#@markdown - **window**: number of words accounted for each context (if the window size is 3, 3 word in the left neighorhood and 3 word in the right neighborhood are considered)\n",
        "#@markdown - **min_count**: minimum count of words to be included in the vocabulary\n",
        "#@markdown - **iter**: number of training iterations\n",
        "#@markdown - **workers**: number of worker threads to train\n",
        "dm_param = 1 #@param {type:\"number\"}\n",
        "size_param = 100 #@param {type:\"number\"}\n",
        "window_param = 3 #@param {type:\"number\"}\n",
        "min_count_param = 1 #@param {type:\"number\"}\n",
        "iter_param = 10 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ### Save and load model\n",
        "#@markdown - doc2vec model can be saved and loaded locally\n",
        "#@markdown - Doing so can reduce time to train model again  \n",
        "model_data = '/content/sample_data/doc2vec_model' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Similarity calculation\n",
        "#@markdown - Similarity between embedded words (i.e., vectors) can be computed using metrics such as cosine similarity\n",
        "#@markdown - For other metrics and comparisons between them, refer to: https://github.com/taki0112/Vector_Similarity\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from traceability import prosesData\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.corpus import gutenberg\n",
        "from multiprocessing import Pool\n",
        "from scipy import spatial\n",
        "from tabulate import tabulate\n",
        "\n",
        "class docMod:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def doc2vec_func(self, data):\n",
        "      sentences = list(data)   # import the corpus and convert into a list\n",
        "      for i in range(len(sentences)):\n",
        "          sentences[i] = TaggedDocument(words = sentences[i], tags = ['sent{}'.format(i)])    # converting each sentence into a TaggedDocument    \n",
        "      model = Doc2Vec(documents = sentences, dm = dm_param, size = size_param, window = window_param, min_count = min_count_param, iter = iter_param, workers = Pool()._processes)\n",
        "      model.init_sims(replace = True)\n",
        "      model.save(model_data)\n",
        "      model = Doc2Vec.load(model_data)\n",
        "      v1 = [[model.infer_vector(angka) for angka in num] for num in sentences]\n",
        "      v1_df = pd.DataFrame(v1) # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
        "      a = [[docMod.cosine_similarity(self, v1[num][0], v1[angka][0]) for angka in v1_df.index] for num in v1_df.index]\n",
        "      return a\n",
        "\n",
        "  def cosine_similarity(self, v1, v2): # define a function that computes cosine similarity between two words\n",
        "      return 1 - spatial.distance.cosine(v1, v2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myProses = prosesData(dataset_var)\n",
        "    list_data = list(myProses.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data = list(myProses.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text = myProses.apply_cleaning_function_to_list(list_data)\n",
        "    myDoc = docMod()\n",
        "    data_doc = myDoc.doc2vec_func(cleaned_text)\n",
        "    data_df = pd.DataFrame(data_doc, index= id_data, columns= id_data)\n",
        "    print(tabulate(data_df, headers = 'keys', tablefmt = 'psql'))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-EqK-V0LFjb"
      },
      "source": [
        "# Word Embed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZcDKhIxCxFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e93827bd-84f6-4b1e-de79-5c923071284d"
      },
      "source": [
        "#@title Modul Word2vec\n",
        "#@markdown Berikut ini penjelasan lengkap modul word2vec\n",
        "\n",
        "#@markdown ### Preprocess data\n",
        "#@markdown - Use re module to preprocess data\n",
        "#@markdown - Convert all letters into lowercase\n",
        "#@markdown - Remove punctuations, numbers, etc.\n",
        "dataset_var1 = '/content/drive/MyDrive/dataset/dataset_2.xlsx'#@param {type:\"string\"}\n",
        "dataset_var2 = '/content/drive/MyDrive/dataset/dataset_2_split.xlsx'#@param {type:\"string\"}\n",
        "dataset_index = '2005 - Grid 3D'#@param {type:\"string\"}\n",
        "\n",
        "sg_param = 1 #@param {type:\"number\"}\n",
        "window_param = 3 #@param {type:\"number\"}\n",
        "min_count_param = 1 #@param {type:\"number\"}\n",
        "iter_param = 10 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "#@markdown ### Create and train model\n",
        "#@markdown - Create a word2vec model and train it with Hamlet corpus\n",
        "#@markdown - Key parameter description (https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "#@markdown     - **sentences**: training data (has to be a list with tokenized sentences)\n",
        "#@markdown     - **size**: dimension of embedding space\n",
        "#@markdown     - **sg**: CBOW if 0, skip-gram if 1\n",
        "#@markdown     - **window**: number of words accounted for each context (if the window size is 3, 3 word in the left neighorhood and 3 word in the right neighborhood are considered)\n",
        "#@markdown     - **min_count**: minimum count of words to be included in the vocabulary\n",
        "#@markdown     - **iter**: number of training iterations\n",
        "#@markdown     - **workers**: number of worker threads to train\n",
        "\n",
        "#@markdown ### Save and load model\n",
        "#@markdown - word2vec model can be saved and loaded locally\n",
        "#@markdown - Doing so can reduce time to train model again\n",
        "model_data = '/content/sample_data/word2vec_model'#@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Similarity calculation\n",
        "#@markdown - Similarity between embedded words (i.e., vectors) can be computed using metrics such as cosine similarity\n",
        "#@markdown - For other metrics and comparisons between them, refer to: https://github.com/taki0112/Vector_Similarity\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from multiprocessing import Pool\n",
        "from traceability import prosesData\n",
        "from gensim.utils import simple_preprocess  \n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "class wordMod:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def model_word(self, data):\n",
        "      sentences = [num.split() for num in data]\n",
        "      model = Word2Vec(sentences = sentences, \n",
        "                        size = len(sentences), \n",
        "                        sg = sg_param, window = window_param, \n",
        "                        min_count = min_count_param, iter = iter_param, \n",
        "                        workers = Pool()._processes)\n",
        "      model.init_sims(replace = True)    \n",
        "      model.save(model_data)\n",
        "      model = Word2Vec.load(model_data)\n",
        "      return model\n",
        "\n",
        "  def tidy_sentence(self, sentence, vocabulary):\n",
        "      return [word for word in simple_preprocess(sentence) if word in vocabulary]    \n",
        "\n",
        "  def compute_sentence_similarity(self, sentence_1, sentence_2, model_wv):\n",
        "      vocabulary = set(model_wv.wv.index2word)\n",
        "      tokens_1 = wordMod.tidy_sentence(self, sentence_1, vocabulary)    \n",
        "      tokens_2 = wordMod.tidy_sentence(self, sentence_2, vocabulary)    \n",
        "      return model_wv.wv.n_similarity(tokens_1, tokens_2)\n",
        "\n",
        "  def wordMeasure(self, data1, data2):\n",
        "      sim = [[wordMod.compute_sentence_similarity(self, num, angka, wordMod.model_word(self, data)) for angka in data2] for num in data1]\n",
        "      return sim\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myProses1 = prosesData(dataset_var1)\n",
        "    list_data1 = list(myProses1.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data1 = list(myProses1.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text1 = myProses1.apply_cleaning_function_to_list(list_data1)\n",
        "\n",
        "    myProses2 = prosesData(dataset_var1)\n",
        "    list_data2 = list(myProses2.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data2 = list(myProses2.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text2 = myProses2.apply_cleaning_function_to_list(list_data2)\n",
        "\n",
        "    dt_word = wordMod().wordMeasure(cleaned_text1, cleaned_text2)\n",
        "    df_word = pd.DataFrame(dt_word, index= id_data1, columns= id_data2)\n",
        "    print(tabulate(df_word, headers = 'keys', tablefmt = 'psql'))   "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
            "|      |        F01 |        F02 |        F03 |        F04 |        F05 |        F06 |        F07 |        F08 |        F09 |       NF01 |       NF02 |       NF03 |       NF04 |       NF05 |       NF06 |       NF07 |       NF08 |\n",
            "|------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------|\n",
            "| F01  |  1         |  0.422852  |  0.441888  |  0.521807  |  0.470901  |  0.620763  |  0.0441903 | -0.0243936 |  0.460406  | -0.134974  |  0.0604617 |  0.40881   |  0.125351  | -0.171688  |  0.0765274 |  0.169999  |  0.128459  |\n",
            "| F02  |  0.422852  |  1         |  0.367773  |  0.244047  |  0.246719  |  0.0647158 | -0.426563  |  0.18707   | -0.0226238 | -0.0378367 | -0.61146   |  0.528274  | -0.204775  | -0.0431131 | -0.0326819 |  0.0227044 | -0.0318129 |\n",
            "| F03  |  0.441888  |  0.367773  |  1         |  0.391119  |  0.27387   |  0.540147  | -0.186841  | -0.145602  |  0.429801  |  0.249387  |  0.057307  |  0.235205  |  0.028476  | -0.216014  |  0.550183  |  0.449876  |  0.0285634 |\n",
            "| F04  |  0.521807  |  0.244047  |  0.391119  |  1         |  0.361386  |  0.228341  |  0.0287052 |  0.157945  |  0.288681  |  0.239172  | -0.134119  |  0.180569  |  0.224752  | -0.129384  |  0.515583  |  0.382673  |  0.0979428 |\n",
            "| F05  |  0.470901  |  0.246719  |  0.27387   |  0.361386  |  1         |  0.661577  | -0.278934  |  0.179443  |  0.70539   | -0.494147  | -0.281172  |  0.224833  | -0.285271  | -0.252285  |  0.229987  |  0.176983  |  0.0508295 |\n",
            "| F06  |  0.620763  |  0.0647158 |  0.540147  |  0.228341  |  0.661577  |  1         | -0.118786  | -0.190255  |  0.642698  | -0.320278  |  0.0508102 |  0.0211191 |  0.0195896 | -0.182416  |  0.253357  |  0.3055    |  0.0326132 |\n",
            "| F07  |  0.0441903 | -0.426563  | -0.186841  |  0.0287052 | -0.278934  | -0.118786  |  1         | -0.139607  | -0.0615147 |  0.35763   |  0.472337  |  0.169275  |  0.405925  | -0.051533  |  0.0246607 |  0.160391  |  0.089758  |\n",
            "| F08  | -0.0243936 |  0.18707   | -0.145602  |  0.157945  |  0.179443  | -0.190255  | -0.139607  |  1         |  0.2979    |  0.140594  | -0.0963723 |  0.0847411 | -0.167349  |  0.137035  | -0.0423451 |  0.258552  |  0.15627   |\n",
            "| F09  |  0.460406  | -0.0226238 |  0.429801  |  0.288681  |  0.70539   |  0.642698  | -0.0615147 |  0.2979    |  1         | -0.130144  |  0.221124  |  0.057892  | -0.362966  | -0.186344  |  0.438626  |  0.548526  |  0.371845  |\n",
            "| NF01 | -0.134974  | -0.0378367 |  0.249387  |  0.239172  | -0.494147  | -0.320278  |  0.35763   |  0.140594  | -0.130144  |  1         |  0.279673  |  0.0341136 |  0.381022  |  0.387181  |  0.343544  |  0.221006  |  0.208544  |\n",
            "| NF02 |  0.0604617 | -0.61146   |  0.057307  | -0.134119  | -0.281172  |  0.0508102 |  0.472337  | -0.0963723 |  0.221124  |  0.279673  |  1         | -0.131549  |  0.197661  | -0.0443249 | -0.0899812 |  0.0469748 | -0.0435278 |\n",
            "| NF03 |  0.40881   |  0.528274  |  0.235205  |  0.180569  |  0.224833  |  0.0211191 |  0.169275  |  0.0847411 |  0.057892  |  0.0341136 | -0.131549  |  1         |  0.281948  | -0.398594  | -0.204109  |  0.0648702 |  0.142711  |\n",
            "| NF04 |  0.125351  | -0.204775  |  0.028476  |  0.224752  | -0.285271  |  0.0195896 |  0.405925  | -0.167349  | -0.362966  |  0.381022  |  0.197661  |  0.281948  |  1         | -0.01782   | -0.126345  | -0.136287  | -0.0209425 |\n",
            "| NF05 | -0.171688  | -0.0431131 | -0.216014  | -0.129384  | -0.252285  | -0.182416  | -0.051533  |  0.137035  | -0.186344  |  0.387181  | -0.0443249 | -0.398594  | -0.01782   |  1         | -0.013908  | -0.130561  |  0.321893  |\n",
            "| NF06 |  0.0765274 | -0.0326819 |  0.550183  |  0.515583  |  0.229987  |  0.253357  |  0.0246607 | -0.0423451 |  0.438626  |  0.343544  | -0.0899812 | -0.204109  | -0.126345  | -0.013908  |  1         |  0.59325   |  0.338444  |\n",
            "| NF07 |  0.169999  |  0.0227044 |  0.449876  |  0.382673  |  0.176983  |  0.3055    |  0.160391  |  0.258552  |  0.548526  |  0.221006  |  0.0469748 |  0.0648702 | -0.136287  | -0.130561  |  0.59325   |  1         |  0.472235  |\n",
            "| NF08 |  0.128459  | -0.0318129 |  0.0285634 |  0.0979428 |  0.0508295 |  0.0326132 |  0.089758  |  0.15627   |  0.371845  |  0.208544  | -0.0435278 |  0.142711  | -0.0209425 |  0.321893  |  0.338444  |  0.472235  |  1         |\n",
            "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEp4BbMO2a6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "89093e36-ecf1-4a8c-9471-281d5c0f1b70"
      },
      "source": [
        "#@title Modul Word2vec Pretrained { vertical-output: true }\n",
        "dataset_var1 = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "dataset_var2 = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"}\n",
        "dataset_index = \"2005 - Grid 3D\" #@param {type:\"string\"}\n",
        "load_bin = \"/content/drive/MyDrive/dataset/GoogleNews-vectors-negative300.bin\" #@param {type:\"string\"}\n",
        "#@markdown Word2Vec is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, strong and powerful would be close together and strong and Paris would be relatively far.\n",
        "#@markdown The are two versions of this model and Word2Vec class implements them both:\n",
        "#@markdown - Skip-grams (SG)\n",
        "#@markdown - Continuous-bag-of-words (CBOW)\n",
        "\n",
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "from traceability import prosesData\n",
        "from gensim.utils import simple_preprocess  \n",
        "from tabulate import tabulate\n",
        "from gensim.models import KeyedVectors # Load Google Pre trained word2vec model https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
        "  \n",
        "\n",
        "class pretrainedMeasure:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def tidy_sentence(self, sentence, vocabulary):\n",
        "      return [word for word in simple_preprocess(sentence) if word in vocabulary]    \n",
        "\n",
        "  def compute_sentence_similarity(self, sentence_1, sentence_2, model_wv):\n",
        "      vocabulary = set(model_wv.index2word)    \n",
        "      tokens_1 = pretrainedMeasure.tidy_sentence(self, sentence_1, vocabulary)    \n",
        "      tokens_2 = pretrainedMeasure.tidy_sentence(self, sentence_2, vocabulary)    \n",
        "      return model_wv.n_similarity(tokens_1, tokens_2)\n",
        "\n",
        "  def wordMeasure(self, data1, data2):\n",
        "      # model_word2vec = api.load('word2vec-google-news-300') # didownload langsung dari source\n",
        "      model_word2vec = KeyedVectors.load_word2vec_format(load_bin, binary=True)\n",
        "      sim = [[pretrainedMeasure.compute_sentence_similarity(self, num, angka, model_word2vec) for angka in data2] for num in data1]\n",
        "      return sim\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myProses1 = prosesData(dataset_var1)\n",
        "    list_data1 = list(myProses1.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data1 = list(myProses1.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text1 = myProses1.apply_cleaning_function_to_list(list_data1)  \n",
        "\n",
        "    myProses2 = prosesData(dataset_var2)\n",
        "    list_data2 = list(myProses2.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data2 = list(myProses2.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text2 = myProses2.apply_cleaning_function_to_list(list_data2)  \n",
        "\n",
        "    dt_word = pretrainedMeasure().wordMeasure(cleaned_text1, cleaned_text2)\n",
        "    df_word = pd.DataFrame(dt_word, index= id_data1, columns= id_data2)\n",
        "    print(tabulate(df_word, headers = 'keys', tablefmt = 'psql'))   "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "|      |      F01 |      F02 |      F03 |      F04 |      F05 |      F06 |      F07 |      F08 |      F09 |     NF01 |     NF02 |     NF03 |     NF04 |     NF05 |     NF06 |     NF07 |     NF08 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 1        | 0.535725 | 0.577641 | 0.590245 | 0.529952 | 0.554168 | 0.428587 | 0.446485 | 0.639938 | 0.314172 | 0.427552 | 0.446093 | 0.345466 | 0.330312 | 0.540891 | 0.478917 | 0.412512 |\n",
            "| F02  | 0.535725 | 1        | 0.686973 | 0.491833 | 0.586066 | 0.592669 | 0.41267  | 0.579903 | 0.678056 | 0.304399 | 0.329199 | 0.62628  | 0.361038 | 0.360136 | 0.513972 | 0.44782  | 0.454106 |\n",
            "| F03  | 0.577641 | 0.686973 | 1        | 0.629755 | 0.640965 | 0.654708 | 0.498676 | 0.425881 | 0.760632 | 0.404497 | 0.38636  | 0.351717 | 0.373694 | 0.337035 | 0.58419  | 0.537924 | 0.356714 |\n",
            "| F04  | 0.590245 | 0.491833 | 0.629755 | 1        | 0.637029 | 0.63268  | 0.576392 | 0.537995 | 0.683039 | 0.503537 | 0.489895 | 0.377954 | 0.319927 | 0.402883 | 0.655189 | 0.60887  | 0.44605  |\n",
            "| F05  | 0.529952 | 0.586066 | 0.640965 | 0.637029 | 1        | 0.904343 | 0.539508 | 0.532044 | 0.80086  | 0.41046  | 0.471684 | 0.403493 | 0.395527 | 0.355415 | 0.575808 | 0.583996 | 0.450267 |\n",
            "| F06  | 0.554168 | 0.592669 | 0.654708 | 0.63268  | 0.904343 | 1        | 0.571789 | 0.537552 | 0.796815 | 0.400015 | 0.459932 | 0.368112 | 0.472879 | 0.436079 | 0.589591 | 0.588118 | 0.454454 |\n",
            "| F07  | 0.428587 | 0.41267  | 0.498676 | 0.576392 | 0.539508 | 0.571789 | 1        | 0.552938 | 0.521839 | 0.556698 | 0.466636 | 0.452875 | 0.47357  | 0.340487 | 0.500508 | 0.421855 | 0.441078 |\n",
            "| F08  | 0.446485 | 0.579903 | 0.425881 | 0.537995 | 0.532044 | 0.537552 | 0.552938 | 1        | 0.500883 | 0.382348 | 0.377362 | 0.641026 | 0.389879 | 0.520193 | 0.429165 | 0.377823 | 0.539192 |\n",
            "| F09  | 0.639938 | 0.678056 | 0.760632 | 0.683039 | 0.80086  | 0.796815 | 0.521839 | 0.500883 | 1        | 0.446664 | 0.519466 | 0.453433 | 0.402732 | 0.347611 | 0.653886 | 0.583015 | 0.469406 |\n",
            "| NF01 | 0.314172 | 0.304399 | 0.404497 | 0.503537 | 0.41046  | 0.400015 | 0.556698 | 0.382348 | 0.446664 | 1        | 0.601789 | 0.386246 | 0.372397 | 0.216674 | 0.52529  | 0.467927 | 0.255186 |\n",
            "| NF02 | 0.427552 | 0.329199 | 0.38636  | 0.489895 | 0.471684 | 0.459932 | 0.466636 | 0.377362 | 0.519466 | 0.601789 | 1        | 0.495967 | 0.379267 | 0.136315 | 0.54611  | 0.491843 | 0.45262  |\n",
            "| NF03 | 0.446093 | 0.62628  | 0.351717 | 0.377954 | 0.403493 | 0.368112 | 0.452875 | 0.641026 | 0.453433 | 0.386246 | 0.495967 | 1        | 0.365119 | 0.304103 | 0.409298 | 0.312155 | 0.463944 |\n",
            "| NF04 | 0.345466 | 0.361038 | 0.373694 | 0.319927 | 0.395527 | 0.472879 | 0.47357  | 0.389879 | 0.402732 | 0.372397 | 0.379267 | 0.365119 | 1        | 0.30796  | 0.376158 | 0.364498 | 0.362252 |\n",
            "| NF05 | 0.330312 | 0.360136 | 0.337035 | 0.402883 | 0.355415 | 0.436079 | 0.340487 | 0.520193 | 0.347611 | 0.216674 | 0.136315 | 0.304103 | 0.30796  | 1        | 0.35938  | 0.324615 | 0.409026 |\n",
            "| NF06 | 0.540891 | 0.513972 | 0.58419  | 0.655189 | 0.575808 | 0.589591 | 0.500508 | 0.429165 | 0.653886 | 0.52529  | 0.54611  | 0.409298 | 0.376158 | 0.35938  | 1        | 0.695927 | 0.446638 |\n",
            "| NF07 | 0.478917 | 0.44782  | 0.537924 | 0.60887  | 0.583996 | 0.588118 | 0.421855 | 0.377823 | 0.583015 | 0.467927 | 0.491843 | 0.312155 | 0.364498 | 0.324615 | 0.695927 | 1        | 0.477849 |\n",
            "| NF08 | 0.412512 | 0.454106 | 0.356714 | 0.44605  | 0.450267 | 0.454454 | 0.441078 | 0.539192 | 0.469406 | 0.255186 | 0.45262  | 0.463944 | 0.362252 | 0.409026 | 0.446638 | 0.477849 | 1        |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK5f_TNL2QrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "5dca39b5-2a62-4998-9303-23e812ef4cdc"
      },
      "source": [
        "#@title Modul Glove Pretrained { vertical-output: true }\n",
        "dataset_var1 = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "dataset_var2 = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"}\n",
        "dataset_index = \"2005 - Grid 3D\" #@param {type:\"string\"}\n",
        "data_input = \"/content/drive/MyDrive/dataset/glove.6B.300d.txt\" #@param {type:\"string\"}\n",
        "data_output = \"/content/drive/MyDrive/dataset/glove_vectors.txt\" #@param {type:\"string\"}\n",
        "#@markdown This script allows to convert GloVe vectors into the word2vec. Both files are presented in text format and almost identical except that word2vec includes number of vectors and its dimension which is only difference regard to GloVe.\n",
        "\n",
        "import pandas as pd\n",
        "from traceability import prosesData\n",
        "from gensim.utils import simple_preprocess  \n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from tabulate import tabulate\n",
        "\n",
        "class pretrainedMeasure:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def tidy_sentence(self, sentence, vocabulary):\n",
        "      return [word for word in simple_preprocess(sentence) if word in vocabulary]    \n",
        "\n",
        "  def compute_sentence_similarity(self, sentence_1, sentence_2, model_wv):\n",
        "      vocabulary = set(model_wv.index2word)    \n",
        "      tokens_1 = pretrainedMeasure.tidy_sentence(self, sentence_1, vocabulary)    \n",
        "      tokens_2 = pretrainedMeasure.tidy_sentence(self, sentence_2, vocabulary)    \n",
        "      return model_wv.n_similarity(tokens_1, tokens_2)\n",
        "\n",
        "  def wordMeasure(self, data1, data2):\n",
        "      # Convert and save glove word embedding to gensim format # https://nlp.stanford.edu/projects/glove/\n",
        "      glove2word2vec(glove_input_file= data_input, word2vec_output_file= data_output) \n",
        "      glove_model = KeyedVectors.load_word2vec_format(data_output, binary=False) # Read saved gensim glove word embedding        \n",
        "      sim = [[pretrainedMeasure.compute_sentence_similarity(self, num, angka, glove_model) for angka in data2] for num in data1]\n",
        "      return sim\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myProses1 = prosesData(dataset_var1)\n",
        "    list_data1 = list(myProses1.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data1 = list(myProses1.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text1 = myProses1.apply_cleaning_function_to_list(list_data1)    \n",
        "\n",
        "    myProses2 = prosesData(dataset_var2)\n",
        "    list_data2 = list(myProses2.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data2 = list(myProses2.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text2 = myProses2.apply_cleaning_function_to_list(list_data2)    \n",
        "\n",
        "    dt_word = pretrainedMeasure().wordMeasure(cleaned_text1, cleaned_text2)\n",
        "    df_word = pd.DataFrame(dt_word, index= id_data1, columns= id_data2)\n",
        "    print(tabulate(df_word, headers = 'keys', tablefmt = 'psql'))   "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "|      |      F01 |      F02 |      F03 |      F04 |      F05 |      F06 |      F07 |      F08 |      F09 |     NF01 |     NF02 |     NF03 |     NF04 |     NF05 |     NF06 |     NF07 |     NF08 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 1        | 0.658532 | 0.724895 | 0.772626 | 0.711642 | 0.736143 | 0.656843 | 0.613114 | 0.768676 | 0.551979 | 0.658281 | 0.623811 | 0.502694 | 0.493835 | 0.712047 | 0.675505 | 0.630335 |\n",
            "| F02  | 0.658532 | 1        | 0.74617  | 0.658928 | 0.702454 | 0.719665 | 0.607963 | 0.690776 | 0.791737 | 0.423885 | 0.429631 | 0.720458 | 0.412376 | 0.414175 | 0.60749  | 0.561243 | 0.592377 |\n",
            "| F03  | 0.724895 | 0.74617  | 1        | 0.77768  | 0.756943 | 0.789839 | 0.712317 | 0.575335 | 0.838868 | 0.638652 | 0.587856 | 0.555235 | 0.588244 | 0.43395  | 0.735216 | 0.680018 | 0.533006 |\n",
            "| F04  | 0.772626 | 0.658928 | 0.77768  | 1        | 0.802533 | 0.800596 | 0.756576 | 0.682438 | 0.787088 | 0.715285 | 0.706895 | 0.564822 | 0.488371 | 0.516283 | 0.818542 | 0.804243 | 0.653338 |\n",
            "| F05  | 0.711642 | 0.702454 | 0.756943 | 0.802533 | 1        | 0.940632 | 0.691108 | 0.653434 | 0.867167 | 0.585215 | 0.654265 | 0.56812  | 0.540311 | 0.459597 | 0.761927 | 0.734742 | 0.662487 |\n",
            "| F06  | 0.736143 | 0.719665 | 0.789839 | 0.800596 | 0.940632 | 1        | 0.753782 | 0.674096 | 0.867013 | 0.579168 | 0.639928 | 0.569021 | 0.620937 | 0.549338 | 0.752096 | 0.730384 | 0.68729  |\n",
            "| F07  | 0.656843 | 0.607963 | 0.712317 | 0.756576 | 0.691108 | 0.753782 | 1        | 0.673626 | 0.664117 | 0.646056 | 0.588978 | 0.610577 | 0.605341 | 0.467699 | 0.682885 | 0.648892 | 0.60862  |\n",
            "| F08  | 0.613114 | 0.690776 | 0.575335 | 0.682438 | 0.653434 | 0.674096 | 0.673626 | 1        | 0.6439   | 0.487514 | 0.482667 | 0.686312 | 0.475754 | 0.588703 | 0.577984 | 0.557697 | 0.677043 |\n",
            "| F09  | 0.768676 | 0.791737 | 0.838868 | 0.787088 | 0.867167 | 0.867013 | 0.664117 | 0.6439   | 1        | 0.556526 | 0.644006 | 0.616845 | 0.522096 | 0.457856 | 0.748854 | 0.723191 | 0.645279 |\n",
            "| NF01 | 0.551979 | 0.423885 | 0.638652 | 0.715285 | 0.585215 | 0.579168 | 0.646056 | 0.487514 | 0.556526 | 1        | 0.708715 | 0.457701 | 0.484357 | 0.333666 | 0.716901 | 0.64781  | 0.449104 |\n",
            "| NF02 | 0.658281 | 0.429631 | 0.587856 | 0.706895 | 0.654265 | 0.639928 | 0.588978 | 0.482667 | 0.644006 | 0.708715 | 1        | 0.545192 | 0.503292 | 0.290972 | 0.734165 | 0.698915 | 0.609767 |\n",
            "| NF03 | 0.623811 | 0.720458 | 0.555235 | 0.564822 | 0.56812  | 0.569021 | 0.610577 | 0.686312 | 0.616845 | 0.457701 | 0.545192 | 1        | 0.494914 | 0.400171 | 0.552972 | 0.481578 | 0.612279 |\n",
            "| NF04 | 0.502694 | 0.412376 | 0.588244 | 0.488371 | 0.540311 | 0.620937 | 0.605341 | 0.475754 | 0.522096 | 0.484357 | 0.503292 | 0.494914 | 1        | 0.397142 | 0.491778 | 0.469057 | 0.513159 |\n",
            "| NF05 | 0.493835 | 0.414175 | 0.43395  | 0.516283 | 0.459597 | 0.549338 | 0.467699 | 0.588703 | 0.457856 | 0.333666 | 0.290972 | 0.400171 | 0.397142 | 1        | 0.472934 | 0.452729 | 0.562829 |\n",
            "| NF06 | 0.712047 | 0.60749  | 0.735216 | 0.818542 | 0.761927 | 0.752096 | 0.682885 | 0.577984 | 0.748854 | 0.716901 | 0.734165 | 0.552972 | 0.491778 | 0.472934 | 1        | 0.861736 | 0.654225 |\n",
            "| NF07 | 0.675505 | 0.561243 | 0.680018 | 0.804243 | 0.734742 | 0.730384 | 0.648892 | 0.557697 | 0.723191 | 0.64781  | 0.698915 | 0.481578 | 0.469057 | 0.452729 | 0.861736 | 1        | 0.672048 |\n",
            "| NF08 | 0.630335 | 0.592377 | 0.533006 | 0.653338 | 0.662487 | 0.68729  | 0.60862  | 0.677043 | 0.645279 | 0.449104 | 0.609767 | 0.612279 | 0.513159 | 0.562829 | 0.654225 | 0.672048 | 1        |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igfx80eD-0Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "01baba96-f4b1-43e9-e4f9-4c318ac9d8f9"
      },
      "source": [
        "#@title Modul Fasttext Pretrained { vertical-output: true }\n",
        "#@markdown We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.\n",
        "\n",
        "dataset_var1 = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"}\n",
        "dataset_var2 = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"}\n",
        "dataset_index = \"2005 - Grid 3D\" #@param {type:\"string\"}\n",
        "pretrained_data = \"/content/drive/MyDrive/dataset/cc.en.300.bin\" #@param {type:\"string\"}\n",
        "\n",
        "import pandas as pd\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from traceability import prosesData\n",
        "from tabulate import tabulate\n",
        "from scipy.spatial import distance\n",
        "\n",
        "class pretrainedMeasure:\n",
        "  def __init__(self, dataWord1, dataWord2):\n",
        "      self.data1 = dataWord1\n",
        "      self.data2 = dataWord2\n",
        "\n",
        "  # define a function that computes cosine similarity between two words\n",
        "  def cosine_similarity(self, v1, v2):\n",
        "      return 1 - distance.cosine(v1, v2)\n",
        "\n",
        "  def sim_calculation(self): # pencarian kesmaaan dokumen\n",
        "      fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "      ft = fasttext.load_model(pretrained_data)\n",
        "      raw_text1 = [ft.get_word_vector(num) for num in self.data1]\n",
        "      raw_text2 = [ft.get_word_vector(num) for num in self.data2]\n",
        "      b = [[pretrainedMeasure.cosine_similarity(self, num, angka) for angka in raw_text2] for num in raw_text1]\n",
        "      return b  #kesamaan berdasarkan kata pertama\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    myProses1 = prosesData(dataset_var1)\n",
        "    list_data1 = list(myProses1.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data1 = list(myProses1.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text1 = myProses1.apply_cleaning_function_to_list(list_data1)    \n",
        "\n",
        "    myProses2 = prosesData(dataset_var2)\n",
        "    list_data2 = list(myProses2.fulldataset(dataset_index)['Requirement Statement'])\n",
        "    id_data2 = list(myProses2.fulldataset(dataset_index)['ID'])\n",
        "    cleaned_text2 = myProses2.apply_cleaning_function_to_list(list_data2)    \n",
        "\n",
        "    data_word = pretrainedMeasure(cleaned_text1, cleaned_text2).sim_calculation()\n",
        "    word_df = pd.DataFrame(data_word, index= id_data1, columns= id_data2)\n",
        "    print(tabulate(word_df, headers = 'keys', tablefmt = 'psql'))       "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
            "|      |      F01 |      F02 |      F03 |      F04 |      F05 |      F06 |      F07 |      F08 |      F09 |     NF01 |     NF02 |     NF03 |     NF04 |     NF05 |     NF06 |     NF07 |     NF08 |\n",
            "|------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
            "| F01  | 1        | 0.620941 | 0.663894 | 0.663342 | 0.701809 | 0.734858 | 0.558847 | 0.563464 | 0.764665 | 0.50245  | 0.671332 | 0.545056 | 0.637175 | 0.365165 | 0.67028  | 0.628573 | 0.551983 |\n",
            "| F02  | 0.620941 | 1        | 0.694993 | 0.547442 | 0.617525 | 0.608526 | 0.619202 | 0.596892 | 0.744773 | 0.459781 | 0.62146  | 0.669359 | 0.737103 | 0.344321 | 0.608425 | 0.496231 | 0.492421 |\n",
            "| F03  | 0.663894 | 0.694993 | 1        | 0.620779 | 0.664818 | 0.672234 | 0.655827 | 0.626264 | 0.792757 | 0.519012 | 0.645347 | 0.601835 | 0.698166 | 0.324512 | 0.648832 | 0.642742 | 0.558591 |\n",
            "| F04  | 0.663342 | 0.547442 | 0.620779 | 1        | 0.700519 | 0.685775 | 0.577657 | 0.581331 | 0.725608 | 0.468959 | 0.561898 | 0.453589 | 0.562426 | 0.315647 | 0.606304 | 0.61262  | 0.54094  |\n",
            "| F05  | 0.701809 | 0.617525 | 0.664818 | 0.700519 | 1        | 0.866    | 0.550209 | 0.589057 | 0.819233 | 0.490507 | 0.654408 | 0.560416 | 0.619468 | 0.359854 | 0.706899 | 0.670223 | 0.618119 |\n",
            "| F06  | 0.734858 | 0.608526 | 0.672234 | 0.685775 | 0.866    | 1        | 0.583306 | 0.568325 | 0.821753 | 0.505769 | 0.635483 | 0.517701 | 0.626014 | 0.398817 | 0.714243 | 0.702191 | 0.612788 |\n",
            "| F07  | 0.558847 | 0.619202 | 0.655827 | 0.577657 | 0.550209 | 0.583306 | 1        | 0.653583 | 0.668031 | 0.508418 | 0.615359 | 0.533199 | 0.624661 | 0.396258 | 0.643115 | 0.501521 | 0.549361 |\n",
            "| F08  | 0.563464 | 0.596892 | 0.626264 | 0.581331 | 0.589057 | 0.568325 | 0.653583 | 1        | 0.686553 | 0.534145 | 0.601377 | 0.641841 | 0.668569 | 0.509136 | 0.561799 | 0.580524 | 0.651769 |\n",
            "| F09  | 0.764665 | 0.744773 | 0.792757 | 0.725608 | 0.819233 | 0.821753 | 0.668031 | 0.686553 | 1        | 0.541006 | 0.73274  | 0.674355 | 0.748505 | 0.449888 | 0.724477 | 0.717835 | 0.686697 |\n",
            "| NF01 | 0.50245  | 0.459781 | 0.519012 | 0.468959 | 0.490507 | 0.505769 | 0.508418 | 0.534145 | 0.541006 | 1        | 0.583276 | 0.49185  | 0.519348 | 0.387861 | 0.557832 | 0.574935 | 0.50751  |\n",
            "| NF02 | 0.671332 | 0.62146  | 0.645347 | 0.561898 | 0.654408 | 0.635483 | 0.615359 | 0.601377 | 0.73274  | 0.583276 | 1        | 0.631488 | 0.696964 | 0.426452 | 0.658959 | 0.614924 | 0.629564 |\n",
            "| NF03 | 0.545056 | 0.669359 | 0.601835 | 0.453589 | 0.560416 | 0.517701 | 0.533199 | 0.641841 | 0.674355 | 0.49185  | 0.631488 | 1        | 0.671279 | 0.342662 | 0.542266 | 0.500624 | 0.563264 |\n",
            "| NF04 | 0.637175 | 0.737103 | 0.698166 | 0.562426 | 0.619468 | 0.626014 | 0.624661 | 0.668569 | 0.748505 | 0.519348 | 0.696964 | 0.671279 | 1        | 0.402445 | 0.605542 | 0.621334 | 0.642515 |\n",
            "| NF05 | 0.365165 | 0.344321 | 0.324512 | 0.315647 | 0.359854 | 0.398817 | 0.396258 | 0.509136 | 0.449888 | 0.387861 | 0.426452 | 0.342662 | 0.402445 | 1        | 0.295496 | 0.466845 | 0.468795 |\n",
            "| NF06 | 0.67028  | 0.608425 | 0.648832 | 0.606304 | 0.706899 | 0.714243 | 0.643115 | 0.561799 | 0.724477 | 0.557832 | 0.658959 | 0.542266 | 0.605542 | 0.295496 | 1        | 0.603498 | 0.537769 |\n",
            "| NF07 | 0.628573 | 0.496231 | 0.642742 | 0.61262  | 0.670223 | 0.702191 | 0.501521 | 0.580524 | 0.717835 | 0.574935 | 0.614924 | 0.500624 | 0.621334 | 0.466845 | 0.603498 | 1        | 0.604307 |\n",
            "| NF08 | 0.551983 | 0.492421 | 0.558591 | 0.54094  | 0.618119 | 0.612788 | 0.549361 | 0.651769 | 0.686697 | 0.50751  | 0.629564 | 0.563264 | 0.642515 | 0.468795 | 0.537769 | 0.604307 | 1        |\n",
            "+------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n"
          ]
        }
      ]
    }
  ]
}