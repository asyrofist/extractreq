# -*- coding: utf-8 -*-
"""modul_partof_(ekspart).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cRCUN0tsJGjcOOjI8igSOCKK9s8_h6eK
"""

"""# modul2: pencarian relasi partOf"""

import pandas as pd
import numpy as np
from tabulate import tabulate
from pywsd.cosine import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn import metrics
from sklearn.metrics import confusion_matrix

# nltk.download('averaged_perceptron_tagger')
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

stops = set(stopwords.words("english"))
lem = WordNetLemmatizer()
class wsd_partof:
  def __init__(self):
      pass

  def fulldataset(self, dataFile, inputSRS):
      xl = pd.ExcelFile(dataFile)
      dfs = {sh:xl.parse(sh) for sh in xl.sheet_names}
      kalimat = dfs[inputSRS]
      kalimat_semua = kalimat.head(len(kalimat))
      return kalimat_semua

  def preprocessing(self, dataFile):
    xl = pd.ExcelFile(dataFile)
    for sh in xl.sheet_names:
      df = xl.parse(sh)
      print('Processing: [{}] ...'.format(sh))
      print(df.head())

  # cleaning text
  def apply_cleaning_function_to_list(self, X):
      cleaned_X = []
      for element in X:
          cleaned_X.append(wsd_partof.clean_text(self, raw_text= element))
      return cleaned_X

  def clean_text(self, raw_text):
      text = raw_text.lower()
      tokens = word_tokenize(text)
      token_words = [w for w in tokens if w.isalpha()]
      lemma_words = [lem.lemmatize(w) for w in token_words]
      meaningful_words = [w for w in lemma_words if not w in stops]
      joined_words = ( " ".join(meaningful_words))
      return joined_words    
      

if __name__ == "__main__":
  try:
      myWsd_partof = wsd_partof()
      # myWsd_partof.preprocessing(dataFile= r'../dataset_2_split.xlsx')

      file1 = r'../dataset_2.xlsx'
      dataSRS =  '2005 - Grid 3D'
      a = myWsd_partof.fulldataset(dataFile= file1, inputSRS= dataSRS)
      list_req1 = list(a['Requirement Statement'])
      id_req1 = list(a['ID'])
      cleaned1 = myWsd_partof.apply_cleaning_function_to_list(X= list_req1)

      file2 = r'../dataset_2_split.xlsx'
      b = myWsd_partof.fulldataset(dataFile= file2, inputSRS= dataSRS)
      list_req2 = list(b['Requirement Statement'])
      id_req2 = list(b['ID'])
      cleaned2 = myWsd_partof.apply_cleaning_function_to_list(X= list_req2)

      hasil_wsd = []
      for num in cleaned1:
        text = [cosine_similarity(num, angka) for angka in cleaned2]
        hasil_wsd.append(text)

      data_raw = pd.DataFrame(hasil_wsd, index= id_req1, columns= id_req2)
      print("Hasil pengukuran semantik antar kebutuhan atomik dan non atomik {}".format(dataSRS))
      print(tabulate(data_raw, headers = 'keys', tablefmt = 'psql'))   

      # thresholding
      threshold = 0.2
      d = data_raw.values >= threshold
      d1 = pd.DataFrame(d, index= id_req1, columns= id_req2)
      mask = d1.isin([True])
      d2 = d1.where(mask, other= 0)
      mask2 = d1.isin([False])
      d3 = d2.where(mask2, other= 1)
      print("\nHasil ukur semantik diatas threshold {}".format(threshold))
      print(tabulate(d3, headers = 'keys', tablefmt = 'psql'))   

      file3 = r'../wsd_groundtruth.xlsx'
      dataGT = 'grid3d_eval'
      b3 = myWsd_partof.fulldataset(dataFile= file3, inputSRS= dataGT)
      b3 = b3.drop(['Index'], axis= 1)
      b3.index= d3.index
      print("\nData Hasil Ground Truth {}".format(dataGT))
      print(tabulate(b3, headers = 'keys', tablefmt = 'psql'))  

      y_actual = d3.values.astype(int)
      y_predicted = b3.values.astype(int)
      print("akurasi", metrics.accuracy_score(y_true= y_actual, y_pred= y_predicted))
      print("presion", metrics.precision_score(y_true= y_actual, y_pred= y_predicted, average= 'macro'))
      print("recall", metrics.recall_score(y_true= y_actual, y_pred= y_predicted, average= 'macro'))
      print("metrics {}".format(metrics.classification_report(y_true= y_actual, y_pred= y_predicted)))       

  except OSError as err:
    print("OS error: {0}".format(err))