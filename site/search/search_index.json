{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing Author Rakha Asyrofi Version 0.0.3 Updated 2021-10-09 overview live_demo How to make Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing described in our Proceeding Conference at ISRITI2020 . Please kindly cite the following paper when you use this tool. It would also be appreciated if you send me a courtesy website and google scholar , so I could survey what kind of tasks the tool is used for. Developed by Asyrofi (c) 2021 How to install installation using pypi: pip install extractreq How easy to use ekspart module: from extractreq.modul_ekspart import partOf grd_param = \"grd.xlsx\" file_param = \"test.xlsx\" srs_param = \"indeks_parameter\" # output_mode = ['pertama', 'kedua', 'ketiga', 'stat'] partOf () . extractPart ( grd_param , file_param , srs_param , 'pertama' ) Check out: https://youtu.be/-d96h9mhh9s Stanford modul: from extractreq.modul_stanfordSent import stanford_clause sent = \"I have friends, but nobody cares me\" stanford_clause () . get_clause_list ( sent ) # stanford_clause(file_param).main(srs_param) Spacy modul: from extractreq.modul_spacySent import spacyClause , spacy nlp = spacy . load ( 'en_core_web_sm' ) doc = nlp ( \"I love you 300, but I don't like you\" ) spacyClause () . extractData ( doc ) # spacyClause(file_param).main(srs_param) Triplet modul: from extractreq.modul_triplet import extractNlp sent = \"I have friends, but nobody cares me\" # output_mode = ['parse_tree', 'spo', 'result'] extractNlp () . triplet_extraction ( sent , 'parse_tree' ) # extractNlp(file_param).main(srs_param, output) Features Dapat digunakan untuk mengekstraksi kebergantungan kebutuhan Dapat digunakan untuk mencari relasi kebergantungan kebutuhan Dapat digunakan memisahkan kalusa dari setiap kalimat menggunakan stanford, spacy Dapat memisahkan triplet dari sebua kalimat Contribute Issue Tracker: https://github.com/asyrofist/Extraction-Requirement/issues Source Code: https://github.com/asyrofist/Extraction-Requirement Support If you are having issues, please let us know. We have a mailing list located at: asyrofi.19051@mhs.its.ac.id Citation If you find this repository useful for your research, please use the following. @INPROCEEDINGS { 9315489 , author = { Asyrofi , Rakha and Siahaan , Daniel Oranova and Priyadi , Yudi } , booktitle = { 2020 3 rd International Seminar on Research of Information Technology and Intelligent Systems ( ISRITI ) } , title = { Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing } , year = { 2020 } , volume = {} , number = {} , pages = { 332 - 337 } , doi = { 10.1109 / ISRITI51436 .2020.9315489 }} License The project is licensed under the MIT License Reference [1] https://github.com/rahulkg31/sentence-to-clauses [2] https://subscription.packtpub.com/book/data/9781838987312/2/ch02lvl1sec13/ [3] https://github.com/kj-lai/SentenceTriplet [4] https://youtu.be/-d96h9mhh9s","title":"Home"},{"location":"#extraction-dependency-based-on-evolutionary-requirement-using-natural-language-processing","text":"Author Rakha Asyrofi Version 0.0.3 Updated 2021-10-09","title":"Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing"},{"location":"#overview","text":"live_demo How to make Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing described in our Proceeding Conference at ISRITI2020 . Please kindly cite the following paper when you use this tool. It would also be appreciated if you send me a courtesy website and google scholar , so I could survey what kind of tasks the tool is used for. Developed by Asyrofi (c) 2021","title":"overview"},{"location":"#how-to-install","text":"installation using pypi: pip install extractreq","title":"How to install"},{"location":"#how-easy-to-use","text":"","title":"How easy to use"},{"location":"#ekspart-module","text":"from extractreq.modul_ekspart import partOf grd_param = \"grd.xlsx\" file_param = \"test.xlsx\" srs_param = \"indeks_parameter\" # output_mode = ['pertama', 'kedua', 'ketiga', 'stat'] partOf () . extractPart ( grd_param , file_param , srs_param , 'pertama' ) Check out: https://youtu.be/-d96h9mhh9s","title":"ekspart module:"},{"location":"#stanford-modul","text":"from extractreq.modul_stanfordSent import stanford_clause sent = \"I have friends, but nobody cares me\" stanford_clause () . get_clause_list ( sent ) # stanford_clause(file_param).main(srs_param)","title":"Stanford modul:"},{"location":"#spacy-modul","text":"from extractreq.modul_spacySent import spacyClause , spacy nlp = spacy . load ( 'en_core_web_sm' ) doc = nlp ( \"I love you 300, but I don't like you\" ) spacyClause () . extractData ( doc ) # spacyClause(file_param).main(srs_param)","title":"Spacy modul:"},{"location":"#triplet-modul","text":"from extractreq.modul_triplet import extractNlp sent = \"I have friends, but nobody cares me\" # output_mode = ['parse_tree', 'spo', 'result'] extractNlp () . triplet_extraction ( sent , 'parse_tree' ) # extractNlp(file_param).main(srs_param, output)","title":"Triplet modul:"},{"location":"#features","text":"Dapat digunakan untuk mengekstraksi kebergantungan kebutuhan Dapat digunakan untuk mencari relasi kebergantungan kebutuhan Dapat digunakan memisahkan kalusa dari setiap kalimat menggunakan stanford, spacy Dapat memisahkan triplet dari sebua kalimat","title":"Features"},{"location":"#contribute","text":"Issue Tracker: https://github.com/asyrofist/Extraction-Requirement/issues Source Code: https://github.com/asyrofist/Extraction-Requirement","title":"Contribute"},{"location":"#support","text":"If you are having issues, please let us know. We have a mailing list located at: asyrofi.19051@mhs.its.ac.id","title":"Support"},{"location":"#citation","text":"If you find this repository useful for your research, please use the following. @INPROCEEDINGS { 9315489 , author = { Asyrofi , Rakha and Siahaan , Daniel Oranova and Priyadi , Yudi } , booktitle = { 2020 3 rd International Seminar on Research of Information Technology and Intelligent Systems ( ISRITI ) } , title = { Extraction Dependency Based on Evolutionary Requirement Using Natural Language Processing } , year = { 2020 } , volume = {} , number = {} , pages = { 332 - 337 } , doi = { 10.1109 / ISRITI51436 .2020.9315489 }}","title":"Citation"},{"location":"#license","text":"The project is licensed under the MIT License","title":"License"},{"location":"#reference","text":"[1] https://github.com/rahulkg31/sentence-to-clauses [2] https://subscription.packtpub.com/book/data/9781838987312/2/ch02lvl1sec13/ [3] https://github.com/kj-lai/SentenceTriplet [4] https://youtu.be/-d96h9mhh9s","title":"Reference"},{"location":"CHANGELOG/","text":"Install the latest To install the latest version simply run: pip3 install extractreq see the Installation QuickStart for more instructions. Changelog 0.0.9 - Oct 10 2021 mengubah modul ekspart menjadi satu modul, dan menambah modul kalimat menjadi klausa menggunakan spacy, stanford, dan triplet RDF 0.0.4 - Oct 10 2021 menambahkan modul ekspart yang terdiri atas modul1, modul2 0.0.2 - Oct 9 2021 menambahkan modul ekscase yang terdiri atas modul1, modul2, dan modul3 0.0.1 - Sept 12 2021 pembuatan pertama kali package extractreq","title":"Changelog"},{"location":"CHANGELOG/#install-the-latest","text":"To install the latest version simply run: pip3 install extractreq see the Installation QuickStart for more instructions.","title":"Install the latest"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#009-oct-10-2021","text":"mengubah modul ekspart menjadi satu modul, dan menambah modul kalimat menjadi klausa menggunakan spacy, stanford, dan triplet RDF","title":"0.0.9 - Oct 10 2021"},{"location":"CHANGELOG/#004-oct-10-2021","text":"menambahkan modul ekspart yang terdiri atas modul1, modul2","title":"0.0.4 - Oct 10 2021"},{"location":"CHANGELOG/#002-oct-9-2021","text":"menambahkan modul ekscase yang terdiri atas modul1, modul2, dan modul3","title":"0.0.2 - Oct 9 2021"},{"location":"CHANGELOG/#001-sept-12-2021","text":"pembuatan pertama kali package extractreq","title":"0.0.1 - Sept 12 2021"},{"location":"TROUBLESHOOTING/","text":"Troubleshooting / FAQ Guide Bagaimana jika menemukan masalah, seperti ini dikemudian hari, apabila saat menggunakan package ini. Maka muncul error seperti ini No module not found portray raises this exception when it cant find a project in the current directory. This means that there is no pyproject.toml or setup.py file in the directory you ran portray AND that you haven't specified modules to include on the command line. Solution 1: instalasi terlebih dahulu requirements.txt If you do have a requirements.txt sehingga cukup instalasi dengan cara menginstal semua modul yang dibutuhkan dalam package ini. Dengan cara pip install - r requirements . txt Solution 2: Hubungi pihak admin apabila dikemudian hari, anda menemukan masalah yang lebih spesifik. Alangkah lebih baiknya, anda cukup menghubungi asyrofi.19051@mhs.its.ac.id agar segera ditangani dan dikerjakan secara cepat. Solution 3: Gunakanlah demostrasi Apabila, anda hanya mencoba aplikasi yang digunakan. Ada baiknya, sekarang anda tinggal mengklik tombol live demo dari keterangan deskripsi README.md . Sehingga agar dapat mempermudah anda. Akhir Kata dari Penulis Baik itu saja, yang akan saya sampaikan. Semoga tidak ada kesalahan, atau error yang berarti dalam menggunakan package ini. Baik, saya ucapkan happy coding.. semoga bermanfaat","title":"Troubleshooting"},{"location":"TROUBLESHOOTING/#troubleshooting-faq-guide","text":"Bagaimana jika menemukan masalah, seperti ini dikemudian hari, apabila saat menggunakan package ini. Maka muncul error seperti ini","title":"Troubleshooting / FAQ Guide"},{"location":"TROUBLESHOOTING/#no-module-not-found","text":"portray raises this exception when it cant find a project in the current directory. This means that there is no pyproject.toml or setup.py file in the directory you ran portray AND that you haven't specified modules to include on the command line.","title":"No module not found"},{"location":"TROUBLESHOOTING/#solution-1-instalasi-terlebih-dahulu-requirementstxt","text":"If you do have a requirements.txt sehingga cukup instalasi dengan cara menginstal semua modul yang dibutuhkan dalam package ini. Dengan cara pip install - r requirements . txt","title":"Solution 1: instalasi terlebih dahulu requirements.txt"},{"location":"TROUBLESHOOTING/#solution-2-hubungi-pihak-admin","text":"apabila dikemudian hari, anda menemukan masalah yang lebih spesifik. Alangkah lebih baiknya, anda cukup menghubungi asyrofi.19051@mhs.its.ac.id agar segera ditangani dan dikerjakan secara cepat.","title":"Solution 2: Hubungi pihak admin"},{"location":"TROUBLESHOOTING/#solution-3-gunakanlah-demostrasi","text":"Apabila, anda hanya mencoba aplikasi yang digunakan. Ada baiknya, sekarang anda tinggal mengklik tombol live demo dari keterangan deskripsi README.md . Sehingga agar dapat mempermudah anda.","title":"Solution 3: Gunakanlah demostrasi"},{"location":"TROUBLESHOOTING/#akhir-kata-dari-penulis","text":"Baik itu saja, yang akan saya sampaikan. Semoga tidak ada kesalahan, atau error yang berarti dalam menggunakan package ini. Baik, saya ucapkan happy coding.. semoga bermanfaat","title":"Akhir Kata dari Penulis"},{"location":"docs/kontribusi/","text":"Contributing to extractreq Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place. Getting portray set up for local development The first step when contributing to any project is getting it set up on your local machine. portray aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ poetry bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/asyrofist/Extraction-Requirement.git cd Extraction-Requirement python setup.py Making a contribution Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main portray project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :). Thank you! I can not tell you how thankful I am for the hard work done by portray contributors like you . Thank you! ~Rakha Asyrofi","title":"1. Contributing Guide"},{"location":"docs/kontribusi/#contributing-to-extractreq","text":"Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place.","title":"Contributing to extractreq"},{"location":"docs/kontribusi/#getting-portray-set-up-for-local-development","text":"The first step when contributing to any project is getting it set up on your local machine. portray aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ poetry bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/asyrofist/Extraction-Requirement.git cd Extraction-Requirement python setup.py","title":"Getting portray set up for local development"},{"location":"docs/kontribusi/#making-a-contribution","text":"Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main portray project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :).","title":"Making a contribution"},{"location":"docs/kontribusi/#thank-you","text":"I can not tell you how thankful I am for the hard work done by portray contributors like you . Thank you! ~Rakha Asyrofi","title":"Thank you!"},{"location":"docs/standard/","text":"HOPE 8 -- Style Guide for Hug Code HOPE: 8 Title: Style Guide for Hug Code Author(s): Rakha Asyrofi asyrofi.19051@mhs.its.ac.id Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019 Introduction This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference. PEP 8 Foundation All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines. Line Length Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters. Descriptive Variable names Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand. Adding new modules New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible. Automated Code Cleaners All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place. Automated Code Linting All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"2. Coding Standard"},{"location":"docs/standard/#hope-8-style-guide-for-hug-code","text":"HOPE: 8 Title: Style Guide for Hug Code Author(s): Rakha Asyrofi asyrofi.19051@mhs.its.ac.id Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019","title":"HOPE 8 -- Style Guide for Hug Code"},{"location":"docs/standard/#introduction","text":"This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference.","title":"Introduction"},{"location":"docs/standard/#pep-8-foundation","text":"All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines.","title":"PEP 8 Foundation"},{"location":"docs/standard/#line-length","text":"Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters.","title":"Line Length"},{"location":"docs/standard/#descriptive-variable-names","text":"Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand.","title":"Descriptive Variable names"},{"location":"docs/standard/#adding-new-modules","text":"New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible.","title":"Adding new modules"},{"location":"docs/standard/#automated-code-cleaners","text":"All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place.","title":"Automated Code Cleaners"},{"location":"docs/standard/#automated-code-linting","text":"All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"Automated Code Linting"},{"location":"reference/extractreq/","text":"Module extractreq None None View Source from extractreq.usecase_modul1 import xmlParser from extractreq.usecase_modul2 import parsingRequirement from extractreq.usecase_modul3 import ucdReq from extractreq.modul_ekspart import partOf from extractreq.modul_spacySent import spacyClause from extractreq.modul_stanfordSent import stanford_clause from extractreq.modul_triplet import extractNlp Sub-modules extractreq.modul_ekspart extractreq.modul_spacySent extractreq.modul_stanfordSent extractreq.modul_triplet extractreq.usecase_modul1 extractreq.usecase_modul2 extractreq.usecase_modul3","title":"Index"},{"location":"reference/extractreq/#module-extractreq","text":"None None View Source from extractreq.usecase_modul1 import xmlParser from extractreq.usecase_modul2 import parsingRequirement from extractreq.usecase_modul3 import ucdReq from extractreq.modul_ekspart import partOf from extractreq.modul_spacySent import spacyClause from extractreq.modul_stanfordSent import stanford_clause from extractreq.modul_triplet import extractNlp","title":"Module extractreq"},{"location":"reference/extractreq/#sub-modules","text":"extractreq.modul_ekspart extractreq.modul_spacySent extractreq.modul_stanfordSent extractreq.modul_triplet extractreq.usecase_modul1 extractreq.usecase_modul2 extractreq.usecase_modul3","title":"Sub-modules"},{"location":"reference/extractreq/modul_ekspart/","text":"Module extractreq.modul_ekspart None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" #@title Modul1: Ekstraksi Kebutuhan partOf { vertical-output: true } url_param = \"http://corenlp.run\" #@param {type:\"string\"} model_param = \"/content/drive/MyDrive/stanford-corenlp-4.0.0\" #@param {type:\"string\"} spacy_param = \"en_core_web_sm\" #@param {type:\"string\"} file_param = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"} dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"} grd_param = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"} save_param = \"/content/drive/MyDrive/dataset/partOfAll/\" #@param {type:\"string\"} srs_param = \"2005 - Grid 3D\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"] data_simpan = save_param + \"partOf{}\" . format ( srs_param ) tab_param = \"pertama\" #@param ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat'] mode_data = \"manual\" #@param [\"manual\", \"stanford\", \"spacy\", 'clausy'] col_param = \"Requirement Statement\" # library yang digunakan import graphviz as gf , pandas as pd , xlsxwriter , re , spacy from tabulate import tabulate from sklearn . preprocessing import LabelEncoder from sklearn . metrics import accuracy_score , recall_score , precision_score , classification_report class partOf : #template class partOf def __init__ ( self , inputData = file_param ): \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = inputData # data inisiliasi file parameter def fulldataset ( self , inputSRS ): # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names }[ inputSRS ] return dfs def preprocessing ( self ): # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def visualisasiGraph ( self , source_data , part_data , srs_param ): \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node. part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan partOf().visualisasiGraph(source_data, part_data, srs_param) \"\"\" f = gf . Digraph ( 'finite_state_machine' , filename = 'partOf.gv' , engine = 'neato' ) f . attr ( rankdir = 'LR' , size = '8,5' ) f . attr ( 'node' , shape = 'doublecircle' ) # node for angka in source_data . ID : f . node ( angka ) f . attr ( kw = 'node' , shape = 'circle' ) # edge for idx , num in zip ( part_data . label , part_data . ID ): f . edge ( idx , num , label = 'partOf' ) f . attr ( overlap = 'false' ) f . attr ( label = r 'Visulasisasi relasi partOf {}\\n' . format ( srs_param )) f . attr ( fontsize = '20' ) f . view ( data_simpan ) print ( \"Gambar disimpan ke {}\" . format ( data_simpan )) return f def evaluasi_data ( self , data1 , data2 ): \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) \"\"\" y_actual = data1 . values . astype ( int ) #define array of actual values y_predicted = data2 . values . astype ( int ) #define array of predicted values nilai_akurasi = accuracy_score ( y_actual , y_predicted , normalize = True ) nilai_recall = recall_score ( y_actual , y_predicted , average = 'macro' ) nilai_presisi = precision_score ( y_actual , y_predicted , average = 'macro' ) print ( \"akurasi {} \\n recall {} \\n presisi {} \\n \" . format ( nilai_akurasi , nilai_recall , nilai_presisi )) print ( classification_report ( y_actual , y_predicted )) def simpan_excel ( self , data1 , data2 , data3 , data4 ): \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan, partOf, relasi, dan nilai data secara statistik. cara menggunakan syntax ini yaitu melalui partOf().simpan_excel(data1, data2, data3, data4) \"\"\" dfs = { # save file 'tabel_kebutuhan' : data1 , 'tabel_partOf' : data2 , 'tabel_relasi' : data3 , 'tabel_statistika' : data4 , } writer = pd . ExcelWriter ( data_simpan + '.xlsx' ) for name , dataframe in dfs . items (): dataframe . to_excel ( writer , name , index = False ) writer . save () print ( \"data excel disimpan di {}\" . format ( data_simpan + '.xlsx' )) # def tabulasi_filter(self, data, mode_data= ['manual', 'triplet']): # tabulasi_filter def tabulasi_filter ( self , data ): # tabulasi_filter \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford. sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya. cara menggunakan syntax ini yaitu melalui partOf().tabulasi_filter(hasil_req) \"\"\" hasil_srs = [] for idx , num in zip ( data [ 'ID' ], data [ 'Requirement Statement' ] . fillna ( \"empty\" )): data = [ x10 for x1 in num . split ( \".\" ) for x2 in x1 . split ( \" that \" ) for x3 in x2 . split ( \"/\" ) for x4 in x3 . split ( \" so \" ) for x5 in x4 . split ( \",\" ) for x6 in x5 . split ( \" and \" ) for x7 in x6 . split ( \" i.e.\" ) for x8 in x7 . split ( \" or \" ) for x9 in x8 . split ( \" if \" ) for x10 in x9 . split ( \" ; \" )] conv = lambda i : i or None res = [ conv ( i ) for i in data ] hasil_srs . append ([ idx , res ]) a_df = pd . DataFrame ( hasil_srs , columns = [ 'ID' , 'data' ]) return a_df def tabulasi_pertama ( self , data , dataReq ): # tabulasi_pertama \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik, dari banyak kalimat yang digunakan. jika terdiri atas satu kalimat maka disebut sebagai atomik. namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. partOf().tabulasi_pertama(hasil_filter, hasil_req) \"\"\" c_df = data . copy () data_df = pd . DataFrame ([ sh for sh in c_df . data ], index = dataReq . ID ) list_column = [ \"data{}\" . format ( num ) for num in range ( data_df . columns . stop )] data_df . columns = list_column b_df = [] b_df_jumlah = [] for num in c_df . data : # menentukan data atomik dan if len ( num ) > 1 : # non atomik berdasarkan jumlah b_df . append ( 'non_atomik' ) b_df_jumlah . append ( len ( num )) elif len ( num ) == 1 : b_df . append ( 'atomik' ) b_df_jumlah . append ( len ( num )) c_df [ 'label' ] = b_df c_df [ 'jumlah' ] = b_df_jumlah return c_df def tabulasi_kedua ( self , data ): # tabulasi kedua \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'req' , 'label' ]) a_data = c_df . loc [ c_df [ 'label' ] == 'atomik' ] data_a = [([ a_data . ID [ num ], index , 'atomik' ]) for num in a_data . index for idx , index in enumerate ( a_data . data [ num ]) if index is not None ] a_df = pd . DataFrame ( data_a , columns = [ 'ID' , 'req' , 'label' ]) part_df = pd . concat ([ a_df , na_df ], ignore_index = True ) part_srt = part_df . sort_values ( by = 'ID' , ignore_index = True ) . drop_duplicates () return part_srt def tabulasi_ketiga ( self , data , data_index ): # tabulasi ketiga \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain. sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan partOf().tabulasi_ketiga(hasil_kedua, hasil_req) \"\"\" part_srt = data . copy () list_data = [ part_srt . loc [ part_srt . ID == num ] . label for num in data_index . ID ] tb_part = pd . DataFrame ( list_data ) . fillna ( 0 ) tb_part . columns = part_srt . ID tb_part . index = data_index . ID return tb_part . reset_index () def tabulasi_alternatifernatif ( self , data ): # Alternatif \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) \"\"\" d_df = data . copy () na_data = d_df . loc [ d_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'data' , 'label' ]) a_data = d_df . loc [ d_df [ 'label' ] == 'atomik' ] dt = pd . concat ([ a_data , na_data , na_df ], ignore_index = True ) part_br = dt . sort_values ( by = 'ID' , ignore_index = True ) list_data = [ part_br . loc [ part_br . ID == num ] . label for num in data . ID ] dt_part = pd . DataFrame ( list_data ) . fillna ( 0 ) # rename data data_part = [([ '{}_{}' . format ( na_data . ID [ num ], idy ), index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_na = pd . DataFrame ( data_part , columns = [ 'ID' , 'req' , 'label' ]) dt_rename = pd . concat ([ a_data , na_data , part_na ], ignore_index = True ) sort_rename = dt_rename . sort_values ( by = 'ID' ) dt_part . columns = sort_rename . ID dt_part . index = data . ID return dt_part . reset_index () def tabulasi_visual ( self , data ): # visualisasi \"\"\" fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] part_list = [([ na_data . ID [ num ], index , 'p{}_{}' . format ( idx , idy )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_visual = pd . DataFrame ( part_list , columns = [ 'ID' , 'req' , 'label' ]) return partOf . visualisasiGraph ( self , data , part_visual , srs_param ) def nilai_stat ( self , data1 , data2 ): # fungsi menentukan nilai statistik \"\"\" fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_stat(data1, data2) \"\"\" jml_kebutuhan = len ( data1 ) jml_minimum = data1 . jumlah . min () jml_maksimum = data1 . jumlah . max () jml_atomik = len ( data1 . loc [ data1 [ 'label' ] == 'atomik' ]) jml_nonatomik = len ( data1 . loc [ data1 [ 'label' ] == 'non_atomik' ]) jml_klausa = len ( data2 . loc [ data2 [ 'label' ] != 'atomik' ]) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonatomik , jml_minimum , jml_maksimum ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_test' ] return jml_df . reset_index () def stat_grountruth ( self , data ): \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) \"\"\" df_part = data . copy () nlp = spacy . load ( 'en_core_web_sm' ) jml_atomik = df_part . loc [ df_part [ 'Sentence' ] == 'a' ] . Sentence . count () jml_nonAtomik = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . drop_duplicates ( subset = 'Sentence' ) . Sentence . count () jml_klausa = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . Sentence . count () jml_kebutuhan = jml_atomik + jml_nonAtomik jml_data = [ len ([ idx for idx in nlp ( num ) . sents ]) for num in df_part [ 'Requirement Statement' ]] jml_a = [ num for num in df_part . Sentence . value_counts () . astype ( int )] jml_min = min ( jml_data ) try : jml_maks = max ( jml_a [ 1 :]) except : jml_maks = max ( jml_data ) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonAtomik , jml_min , jml_maks ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_groundtruth' ] return jml_df . reset_index () def __del__ ( self ): \"\"\" fungsi ini digunakan untuk mendestruksi, cara meggunakan panggil fungsi dengan syntax berikut ini: partOf().__del__() \"\"\" pass def extractPart ( self , grd_param , file_param , srs_param , output = tab_param ): \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']) \"\"\" part2 = partOf ( grd_param ) part_grd = part2 . fulldataset ( srs_param ) data_grountruth = part2 . stat_grountruth ( part_grd ) part2 . __del__ () part1 = partOf ( file_param ) dataReq = part1 . fulldataset ( srs_param ) data_filtrasi = part1 . tabulasi_filter ( dataReq ) data_pertama = part1 . tabulasi_pertama ( data_filtrasi , dataReq ) data_kedua = part1 . tabulasi_kedua ( data_pertama ) data_ketiga = part1 . tabulasi_ketiga ( data_kedua , data_pertama ) alternatif = part1 . tabulasi_alternatifernatif ( data_pertama ) data_visual = part1 . tabulasi_visual ( data_pertama ) data_stat = part1 . nilai_stat ( data_pertama , data_kedua ) part1 . simpan_excel ( data_pertama , data_kedua , data_ketiga , data_stat ) part1 . __del__ () if 'pertama' in output : print ( \" \\n Tabulasi Pertama {}\" . format ( srs_param )) print ( tabulate ( data_pertama , headers = 'keys' , tablefmt = 'psql' )) elif 'kedua' in output : print ( \" \\n Tabulasi Kedua {}\" . format ( srs_param )) print ( tabulate ( data_kedua , headers = 'keys' , tablefmt = 'psql' )) elif 'ketiga' in output : print ( \" \\n Tabulasi Ketiga {}\" . format ( srs_param )) print ( tabulate ( data_ketiga , headers = 'keys' , tablefmt = 'psql' )) elif 'alternatif' in output : print ( \" \\n Tabulasi Ketiga Alternatif {}\" . format ( srs_param )) print ( tabulate ( alternatif , headers = 'keys' , tablefmt = 'psql' )) elif 'stat' in output : print ( \" \\n Tabulasi Statistik {}\" . format ( srs_param )) print ( tabulate ( data_grountruth , headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( data_stat , headers = 'keys' , tablefmt = 'psql' )) part2 . evaluasi_data ( data_stat . drop ( 'index' , axis = 1 ), data_grountruth . drop ( 'index' , axis = 1 )) if __name__ == \"__main__\" : try : partOf () . extractPart ( tab_param ) except OSError as err : print ( \"OS error: {0}\" . format ( err )) Variables col_param dataFile data_simpan file_param grd_param mode_data model_param save_param spacy_param srs_param tab_param url_param Classes partOf class partOf ( inputData = '/content/drive/MyDrive/dataset/dataset_2.xlsx' ) View Source class partOf : #template class partOf def __init__ ( self , inputData = file_param ): \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = inputData # data inisiliasi file parameter def fulldataset ( self , inputSRS ): # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names }[ inputSRS ] return dfs def preprocessing ( self ): # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def visualisasiGraph ( self , source_data , part_data , srs_param ): \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node. part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan partOf().visualisasiGraph(source_data, part_data, srs_param) \"\"\" f = gf . Digraph ( 'finite_state_machine' , filename = 'partOf.gv' , engine = 'neato' ) f . attr ( rankdir = 'LR' , size = '8,5' ) f . attr ( 'node' , shape = 'doublecircle' ) # node for angka in source_data . ID : f . node ( angka ) f . attr ( kw = 'node' , shape = 'circle' ) # edge for idx , num in zip ( part_data . label , part_data . ID ): f . edge ( idx , num , label = 'partOf' ) f . attr ( overlap = 'false' ) f . attr ( label = r 'Visulasisasi relasi partOf {}\\n' . format ( srs_param )) f . attr ( fontsize = '20' ) f . view ( data_simpan ) print ( \"Gambar disimpan ke {}\" . format ( data_simpan )) return f def evaluasi_data ( self , data1 , data2 ): \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) \"\"\" y_actual = data1 . values . astype ( int ) #define array of actual values y_predicted = data2 . values . astype ( int ) #define array of predicted values nilai_akurasi = accuracy_score ( y_actual , y_predicted , normalize = True ) nilai_recall = recall_score ( y_actual , y_predicted , average = 'macro' ) nilai_presisi = precision_score ( y_actual , y_predicted , average = 'macro' ) print ( \"akurasi {} \\n recall {} \\n presisi {} \\n \" . format ( nilai_akurasi , nilai_recall , nilai_presisi )) print ( classification_report ( y_actual , y_predicted )) def simpan_excel ( self , data1 , data2 , data3 , data4 ): \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan, partOf, relasi, dan nilai data secara statistik. cara menggunakan syntax ini yaitu melalui partOf().simpan_excel(data1, data2, data3, data4) \"\"\" dfs = { # save file 'tabel_kebutuhan' : data1 , 'tabel_partOf' : data2 , 'tabel_relasi' : data3 , 'tabel_statistika' : data4 , } writer = pd . ExcelWriter ( data_simpan + '.xlsx' ) for name , dataframe in dfs . items (): dataframe . to_excel ( writer , name , index = False ) writer . save () print ( \"data excel disimpan di {}\" . format ( data_simpan + '.xlsx' )) # def tabulasi_filter(self, data, mode_data= ['manual', 'triplet']): # tabulasi_filter def tabulasi_filter ( self , data ): # tabulasi_filter \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford. sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya. cara menggunakan syntax ini yaitu melalui partOf().tabulasi_filter(hasil_req) \"\"\" hasil_srs = [] for idx , num in zip ( data [ 'ID' ], data [ 'Requirement Statement' ] . fillna ( \"empty\" )): data = [ x10 for x1 in num . split ( \".\" ) for x2 in x1 . split ( \" that \" ) for x3 in x2 . split ( \"/\" ) for x4 in x3 . split ( \" so \" ) for x5 in x4 . split ( \",\" ) for x6 in x5 . split ( \" and \" ) for x7 in x6 . split ( \" i.e.\" ) for x8 in x7 . split ( \" or \" ) for x9 in x8 . split ( \" if \" ) for x10 in x9 . split ( \" ; \" )] conv = lambda i : i or None res = [ conv ( i ) for i in data ] hasil_srs . append ([ idx , res ]) a_df = pd . DataFrame ( hasil_srs , columns = [ 'ID' , 'data' ]) return a_df def tabulasi_pertama ( self , data , dataReq ): # tabulasi_pertama \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik, dari banyak kalimat yang digunakan. jika terdiri atas satu kalimat maka disebut sebagai atomik. namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. partOf().tabulasi_pertama(hasil_filter, hasil_req) \"\"\" c_df = data . copy () data_df = pd . DataFrame ([ sh for sh in c_df . data ], index = dataReq . ID ) list_column = [ \"data{}\" . format ( num ) for num in range ( data_df . columns . stop )] data_df . columns = list_column b_df = [] b_df_jumlah = [] for num in c_df . data : # menentukan data atomik dan if len ( num ) > 1 : # non atomik berdasarkan jumlah b_df . append ( 'non_atomik' ) b_df_jumlah . append ( len ( num )) elif len ( num ) == 1 : b_df . append ( 'atomik' ) b_df_jumlah . append ( len ( num )) c_df [ 'label' ] = b_df c_df [ 'jumlah' ] = b_df_jumlah return c_df def tabulasi_kedua ( self , data ): # tabulasi kedua \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'req' , 'label' ]) a_data = c_df . loc [ c_df [ 'label' ] == 'atomik' ] data_a = [([ a_data . ID [ num ], index , 'atomik' ]) for num in a_data . index for idx , index in enumerate ( a_data . data [ num ]) if index is not None ] a_df = pd . DataFrame ( data_a , columns = [ 'ID' , 'req' , 'label' ]) part_df = pd . concat ([ a_df , na_df ], ignore_index = True ) part_srt = part_df . sort_values ( by = 'ID' , ignore_index = True ) . drop_duplicates () return part_srt def tabulasi_ketiga ( self , data , data_index ): # tabulasi ketiga \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain. sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan partOf().tabulasi_ketiga(hasil_kedua, hasil_req) \"\"\" part_srt = data . copy () list_data = [ part_srt . loc [ part_srt . ID == num ] . label for num in data_index . ID ] tb_part = pd . DataFrame ( list_data ) . fillna ( 0 ) tb_part . columns = part_srt . ID tb_part . index = data_index . ID return tb_part . reset_index () def tabulasi_alternatifernatif ( self , data ): # Alternatif \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) \"\"\" d_df = data . copy () na_data = d_df . loc [ d_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'data' , 'label' ]) a_data = d_df . loc [ d_df [ 'label' ] == 'atomik' ] dt = pd . concat ([ a_data , na_data , na_df ], ignore_index = True ) part_br = dt . sort_values ( by = 'ID' , ignore_index = True ) list_data = [ part_br . loc [ part_br . ID == num ] . label for num in data . ID ] dt_part = pd . DataFrame ( list_data ) . fillna ( 0 ) # rename data data_part = [([ '{}_{}' . format ( na_data . ID [ num ], idy ), index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_na = pd . DataFrame ( data_part , columns = [ 'ID' , 'req' , 'label' ]) dt_rename = pd . concat ([ a_data , na_data , part_na ], ignore_index = True ) sort_rename = dt_rename . sort_values ( by = 'ID' ) dt_part . columns = sort_rename . ID dt_part . index = data . ID return dt_part . reset_index () def tabulasi_visual ( self , data ): # visualisasi \"\"\" fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] part_list = [([ na_data . ID [ num ], index , 'p{}_{}' . format ( idx , idy )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_visual = pd . DataFrame ( part_list , columns = [ 'ID' , 'req' , 'label' ]) return partOf . visualisasiGraph ( self , data , part_visual , srs_param ) def nilai_stat ( self , data1 , data2 ): # fungsi menentukan nilai statistik \"\"\" fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_stat(data1, data2) \"\"\" jml_kebutuhan = len ( data1 ) jml_minimum = data1 . jumlah . min () jml_maksimum = data1 . jumlah . max () jml_atomik = len ( data1 . loc [ data1 [ 'label' ] == 'atomik' ]) jml_nonatomik = len ( data1 . loc [ data1 [ 'label' ] == 'non_atomik' ]) jml_klausa = len ( data2 . loc [ data2 [ 'label' ] != 'atomik' ]) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonatomik , jml_minimum , jml_maksimum ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_test' ] return jml_df . reset_index () def stat_grountruth ( self , data ): \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) \"\"\" df_part = data . copy () nlp = spacy . load ( 'en_core_web_sm' ) jml_atomik = df_part . loc [ df_part [ 'Sentence' ] == 'a' ] . Sentence . count () jml_nonAtomik = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . drop_duplicates ( subset = 'Sentence' ) . Sentence . count () jml_klausa = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . Sentence . count () jml_kebutuhan = jml_atomik + jml_nonAtomik jml_data = [ len ([ idx for idx in nlp ( num ) . sents ]) for num in df_part [ 'Requirement Statement' ]] jml_a = [ num for num in df_part . Sentence . value_counts () . astype ( int )] jml_min = min ( jml_data ) try : jml_maks = max ( jml_a [ 1 :]) except : jml_maks = max ( jml_data ) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonAtomik , jml_min , jml_maks ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_groundtruth' ] return jml_df . reset_index () def __del__ ( self ): \"\"\" fungsi ini digunakan untuk mendestruksi, cara meggunakan panggil fungsi dengan syntax berikut ini: partOf().__del__() \"\"\" pass def extractPart ( self , grd_param , file_param , srs_param , output = tab_param ): \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']) \"\"\" part2 = partOf ( grd_param ) part_grd = part2 . fulldataset ( srs_param ) data_grountruth = part2 . stat_grountruth ( part_grd ) part2 . __del__ () part1 = partOf ( file_param ) dataReq = part1 . fulldataset ( srs_param ) data_filtrasi = part1 . tabulasi_filter ( dataReq ) data_pertama = part1 . tabulasi_pertama ( data_filtrasi , dataReq ) data_kedua = part1 . tabulasi_kedua ( data_pertama ) data_ketiga = part1 . tabulasi_ketiga ( data_kedua , data_pertama ) alternatif = part1 . tabulasi_alternatifernatif ( data_pertama ) data_visual = part1 . tabulasi_visual ( data_pertama ) data_stat = part1 . nilai_stat ( data_pertama , data_kedua ) part1 . simpan_excel ( data_pertama , data_kedua , data_ketiga , data_stat ) part1 . __del__ () if 'pertama' in output : print ( \" \\n Tabulasi Pertama {}\" . format ( srs_param )) print ( tabulate ( data_pertama , headers = 'keys' , tablefmt = 'psql' )) elif 'kedua' in output : print ( \" \\n Tabulasi Kedua {}\" . format ( srs_param )) print ( tabulate ( data_kedua , headers = 'keys' , tablefmt = 'psql' )) elif 'ketiga' in output : print ( \" \\n Tabulasi Ketiga {}\" . format ( srs_param )) print ( tabulate ( data_ketiga , headers = 'keys' , tablefmt = 'psql' )) elif 'alternatif' in output : print ( \" \\n Tabulasi Ketiga Alternatif {}\" . format ( srs_param )) print ( tabulate ( alternatif , headers = 'keys' , tablefmt = 'psql' )) elif 'stat' in output : print ( \" \\n Tabulasi Statistik {}\" . format ( srs_param )) print ( tabulate ( data_grountruth , headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( data_stat , headers = 'keys' , tablefmt = 'psql' )) part2 . evaluasi_data ( data_stat . drop ( 'index' , axis = 1 ), data_grountruth . drop ( 'index' , axis = 1 )) Methods evaluasi_data def evaluasi_data ( self , data1 , data2 ) fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) View Source def evaluasi_data(self, data1, data2): \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) \"\"\" y_actual = data1.values.astype(int) #define array of actual values y_predicted = data2.values.astype(int) #define array of predicted values nilai_akurasi = accuracy_score(y_actual, y_predicted, normalize=True) nilai_recall = recall_score(y_actual, y_predicted, average= 'macro') nilai_presisi = precision_score(y_actual, y_predicted, average= 'macro') print(\"akurasi {}\\n recall {}\\n presisi {}\\n\".format(nilai_akurasi, nilai_recall, nilai_presisi)) print(classification_report(y_actual, y_predicted)) extractPart def extractPart ( self , grd_param , file_param , srs_param , output = 'pertama' ) fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']) View Source def extractPart ( self , grd_param , file_param , srs_param , output = tab_param ): \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= [ 'pertama' , 'kedua' , 'ketiga' , 'alternatif' , 'stat' ] ) \"\"\" part2 = partOf ( grd_param ) part_grd = part2 . fulldataset ( srs_param ) data_grountruth = part2 . stat_grountruth ( part_grd ) part2 . __del__ () part1 = partOf ( file_param ) dataReq = part1 . fulldataset ( srs_param ) data_filtrasi = part1 . tabulasi_filter ( dataReq ) data_pertama = part1 . tabulasi_pertama ( data_filtrasi , dataReq ) data_kedua = part1 . tabulasi_kedua ( data_pertama ) data_ketiga = part1 . tabulasi_ketiga ( data_kedua , data_pertama ) alternatif = part1 . tabulasi_alternatifernatif ( data_pertama ) data_visual = part1 . tabulasi_visual ( data_pertama ) data_stat = part1 . nilai_stat ( data_pertama , data_kedua ) part1 . simpan_excel ( data_pertama , data_kedua , data_ketiga , data_stat ) part1 . __del__ () if 'pertama' in output : print ( \"\\nTabulasi Pertama {}\" . format ( srs_param )) print ( tabulate ( data_pertama , headers = 'keys' , tablefmt = 'psql' )) elif 'kedua' in output : print ( \"\\nTabulasi Kedua {}\" . format ( srs_param )) print ( tabulate ( data_kedua , headers = 'keys' , tablefmt = 'psql' )) elif 'ketiga' in output : print ( \"\\nTabulasi Ketiga {}\" . format ( srs_param )) print ( tabulate ( data_ketiga , headers = 'keys' , tablefmt = 'psql' )) elif 'alternatif' in output : print ( \"\\nTabulasi Ketiga Alternatif {}\" . format ( srs_param )) print ( tabulate ( alternatif , headers = 'keys' , tablefmt = 'psql' )) elif 'stat' in output : print ( \"\\nTabulasi Statistik {}\" . format ( srs_param )) print ( tabulate ( data_grountruth , headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( data_stat , headers = 'keys' , tablefmt = 'psql' )) part2 . evaluasi_data ( data_stat . drop ( 'index' , axis = 1 ), data_grountruth . drop ( 'index' , axis = 1 )) fulldataset def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs nilai_stat def nilai_stat ( self , data1 , data2 ) fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_stat(data1, data2) View Source def nilai_stat ( self , data1 , data2 ) : # fungsi menentukan nilai statistik \"\"\" fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan , yang meliputi jumlah kebutuhan , atomik , nonatomik , klausa , maksimum kalimat , minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf () . stat_stat ( data1 , data2 ) \"\"\" jml_kebutuhan = len ( data1 ) jml_minimum = data1 . jumlah . min () jml_maksimum = data1 . jumlah . max () jml_atomik = len ( data1 . loc [ data1 [ ' label ' ] == ' atomik ' ] ) jml_nonatomik = len ( data1 . loc [ data1 [ ' label ' ] == ' non_atomik ' ] ) jml_klausa = len ( data2 . loc [ data2 [ ' label ' ] != ' atomik ' ] ) jml_df = pd . DataFrame ( [ jml_kebutuhan , jml_atomik , jml_nonatomik , jml_minimum , jml_maksimum ] ) jml_df . index = [ ' jumlah_kebtuhan ' , ' jumlah_atomik ' , ' jumlah_nonatomik ' , ' minimum_jumlah_kalimat ' , ' maksimum_jumlah_kalimat ' ] jml_df . columns = [ ' statistik_test ' ] return jml_df . reset_index () preprocessing def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ()) simpan_excel def simpan_excel ( self , data1 , data2 , data3 , data4 ) fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan, partOf, relasi, dan nilai data secara statistik. cara menggunakan syntax ini yaitu melalui partOf().simpan_excel(data1, data2, data3, data4) View Source def simpan_excel ( self , data1 , data2 , data3 , data4 ) : \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan , partOf , relasi , dan nilai data secara statistik . cara menggunakan syntax ini yaitu melalui partOf () . simpan_excel ( data1 , data2 , data3 , data4 ) \"\"\" dfs = { # save file ' tabel_kebutuhan ' : data1 , ' tabel_partOf ' : data2 , ' tabel_relasi ' : data3 , ' tabel_statistika ' : data4 , } writer = pd . ExcelWriter ( data_simpan + ' .xlsx ' ) for name , dataframe in dfs . items () : dataframe . to_excel ( writer , name , index = False ) writer . save () print ( \" data excel disimpan di {} \" . format ( data_simpan + ' .xlsx ' )) stat_grountruth def stat_grountruth ( self , data ) fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) View Source def stat_grountruth ( self , data ): \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) \"\"\" df_part = data . copy () nlp = spacy . load ( 'en_core_web_sm' ) jml_atomik = df_part . loc [ df_part [ 'Sentence' ] == 'a' ] . Sentence . count () jml_nonAtomik = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . drop_duplicates ( subset = 'Sentence' ) . Sentence . count () jml_klausa = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . Sentence . count () jml_kebutuhan = jml_atomik + jml_nonAtomik jml_data = [ len ([ idx for idx in nlp ( num ) . sents ]) for num in df_part [ 'Requirement Statement' ]] jml_a = [ num for num in df_part . Sentence . value_counts () . astype ( int )] jml_min = min ( jml_data ) try : jml_maks = max ( jml_a [ 1 :]) except : jml_maks = max ( jml_data ) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonAtomik , jml_min , jml_maks ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_groundtruth' ] return jml_df . reset_index () tabulasi_alternatifernatif def tabulasi_alternatifernatif ( self , data ) fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) View Source def tabulasi_alternatifernatif ( self , data ) : # Alternatif \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) \"\"\" d_df = data . copy () na_data = d_df . loc [ d_df['label' ] == 'non_atomik' ] data_na = [ ([na_data.ID[num ] , index , 'p{}' . format ( idx ) ] ) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID', 'data', 'label' ] ) a_data = d_df . loc [ d_df['label' ] == 'atomik' ] dt = pd . concat ( [ a_data, na_data, na_df ] , ignore_index = True ) part_br = dt . sort_values ( by = 'ID' , ignore_index = True ) list_data = [ part_br.loc[part_br.ID == num ] . label for num in data . ID ] dt_part = pd . DataFrame ( list_data ). fillna ( 0 ) # rename data data_part = [ (['{}_{}'.format(na_data.ID[num ] , idy ), index , 'p{}' . format ( idx ) ] ) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ] ) if index is not None ] part_na = pd . DataFrame ( data_part , columns = [ 'ID', 'req', 'label' ] ) dt_rename = pd . concat ( [ a_data, na_data, part_na ] , ignore_index = True ) sort_rename = dt_rename . sort_values ( by = 'ID' ) dt_part . columns = sort_rename . ID dt_part . index = data . ID return dt_part . reset_index () tabulasi_filter def tabulasi_filter ( self , data ) fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford. sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya. cara menggunakan syntax ini yaitu melalui partOf().tabulasi_filter(hasil_req) View Source def tabulasi_filter ( self , data ) : # tabulasi_filter \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual , triplet , spacy , dan stanford . sesuai dengan namanya . maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya . cara menggunakan syntax ini yaitu melalui partOf () . tabulasi_filter ( hasil_req ) \"\"\" hasil_srs = [] for idx , num in zip ( data [ ' ID ' ], data [ ' Requirement Statement ' ]. fillna ( \" empty \" )) : data = [ x10 for x1 in num . split ( \" . \" ) for x2 in x1 . split ( \" that \" ) for x3 in x2 . split ( \" / \" ) for x4 in x3 . split ( \" so \" ) for x5 in x4 . split ( \" , \" ) for x6 in x5 . split ( \" and \" ) for x7 in x6 . split ( \" i.e. \" ) for x8 in x7 . split ( \" or \" ) for x9 in x8 . split ( \" if \" ) for x10 in x9 . split ( \" ; \" ) ] conv = lambda i : i or None res = [ conv ( i ) for i in data ] hasil_srs . append ( [ idx , res ] ) a_df = pd . DataFrame ( hasil_srs , columns = [ ' ID ' , ' data ' ] ) return a_df tabulasi_kedua def tabulasi_kedua ( self , data ) fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) View Source def tabulasi_kedua ( self , data ) : # tabulasi kedua \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df['label' ] == 'non_atomik' ] data_na = [ ([na_data.ID[num ] , index , 'p{}' . format ( idx ) ] ) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID', 'req', 'label' ] ) a_data = c_df . loc [ c_df['label' ] == 'atomik' ] data_a = [ ([a_data.ID[num ] , index , 'atomik' ] ) for num in a_data . index for idx , index in enumerate ( a_data . data [ num ] ) if index is not None ] a_df = pd . DataFrame ( data_a , columns = [ 'ID', 'req', 'label' ] ) part_df = pd . concat ( [ a_df, na_df ] , ignore_index = True ) part_srt = part_df . sort_values ( by = 'ID' , ignore_index = True ). drop_duplicates () return part_srt tabulasi_ketiga def tabulasi_ketiga ( self , data , data_index ) fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain. sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan partOf().tabulasi_ketiga(hasil_kedua, hasil_req) View Source def tabulasi_ketiga ( self , data , data_index ) : # tabulasi ketiga \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain . sehingga dengan cara ini , dapat terlihat relasi atomik , p # dalam sebuah kebutuhan partOf () . tabulasi_ketiga ( hasil_kedua , hasil_req ) \"\"\" part_srt = data . copy () list_data = [ part_srt . loc [ part_srt . ID == num ]. label for num in data_index . ID ] tb_part = pd . DataFrame ( list_data ) . fillna ( 0 ) tb_part . columns = part_srt . ID tb_part . index = data_index . ID return tb_part . reset_index () tabulasi_pertama def tabulasi_pertama ( self , data , dataReq ) fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik, dari banyak kalimat yang digunakan. jika terdiri atas satu kalimat maka disebut sebagai atomik. namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. partOf().tabulasi_pertama(hasil_filter, hasil_req) View Source def tabulasi_pertama ( self , data , dataReq ) : # tabulasi_pertama \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik , dari banyak kalimat yang digunakan . jika terdiri atas satu kalimat maka disebut sebagai atomik . namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik . partOf () . tabulasi_pertama ( hasil_filter , hasil_req ) \"\"\" c_df = data . copy () data_df = pd . DataFrame ( [ sh for sh in c_df . data ], index = dataReq . ID ) list_column = [ \" data{} \" . format ( num ) for num in range ( data_df . columns . stop ) ] data_df . columns = list_column b_df = [] b_df_jumlah = [] for num in c_df . data : # menentukan data atomik dan if len ( num ) > 1 : # non atomik berdasarkan jumlah b_df . append ( ' non_atomik ' ) b_df_jumlah . append ( len ( num )) elif len ( num ) == 1 : b_df . append ( ' atomik ' ) b_df_jumlah . append ( len ( num )) c_df [ ' label ' ] = b_df c_df [ ' jumlah ' ] = b_df_jumlah return c_df tabulasi_visual def tabulasi_visual ( self , data ) fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) View Source def tabulasi_visual ( self , data ) : # visualisasi \"\"\" fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df['label' ] == 'non_atomik' ] part_list = [ ([na_data.ID[num ] , index , 'p{}_{}' . format ( idx , idy ) ] ) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ] ) if index is not None ] part_visual = pd . DataFrame ( part_list , columns = [ 'ID', 'req', 'label' ] ) return partOf . visualisasiGraph ( self , data , part_visual , srs_param ) visualisasiGraph def visualisasiGraph ( self , source_data , part_data , srs_param ) fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node. part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan partOf().visualisasiGraph(source_data, part_data, srs_param) View Source def visualisasiGraph ( self , source_data , part_data , srs_param ) : \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node . part_data untuk node child , dan indeks yang digunakan sesuai data srs yang digunakan partOf () . visualisasiGraph ( source_data , part_data , srs_param ) \"\"\" f = gf . Digraph ( ' finite_state_machine ' , filename = ' partOf.gv ' , engine = ' neato ' ) f . attr ( rankdir = ' LR ' , size = ' 8,5 ' ) f . attr ( ' node ' , shape = ' doublecircle ' ) # node for angka in source_data . ID : f . node ( angka ) f . attr ( kw = ' node ' , shape = ' circle ' ) # edge for idx , num in zip ( part_data . label , part_data . ID ) : f . edge ( idx , num , label = ' partOf ' ) f . attr ( overlap = ' false ' ) f . attr ( label = r ' Visulasisasi relasi partOf {} \\n ' . format ( srs_param )) f . attr ( fontsize = ' 20 ' ) f . view ( data_simpan ) print ( \" Gambar disimpan ke {} \" . format ( data_simpan )) return f","title":"Modul Ekspart"},{"location":"reference/extractreq/modul_ekspart/#module-extractreqmodul_ekspart","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" #@title Modul1: Ekstraksi Kebutuhan partOf { vertical-output: true } url_param = \"http://corenlp.run\" #@param {type:\"string\"} model_param = \"/content/drive/MyDrive/stanford-corenlp-4.0.0\" #@param {type:\"string\"} spacy_param = \"en_core_web_sm\" #@param {type:\"string\"} file_param = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"} dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" #@param {type:\"string\"} grd_param = \"/content/drive/MyDrive/dataset/dataset_2_split.xlsx\" #@param {type:\"string\"} save_param = \"/content/drive/MyDrive/dataset/partOfAll/\" #@param {type:\"string\"} srs_param = \"2005 - Grid 3D\" #@param [\"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\"] data_simpan = save_param + \"partOf{}\" . format ( srs_param ) tab_param = \"pertama\" #@param ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat'] mode_data = \"manual\" #@param [\"manual\", \"stanford\", \"spacy\", 'clausy'] col_param = \"Requirement Statement\" # library yang digunakan import graphviz as gf , pandas as pd , xlsxwriter , re , spacy from tabulate import tabulate from sklearn . preprocessing import LabelEncoder from sklearn . metrics import accuracy_score , recall_score , precision_score , classification_report class partOf : #template class partOf def __init__ ( self , inputData = file_param ): \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = inputData # data inisiliasi file parameter def fulldataset ( self , inputSRS ): # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names }[ inputSRS ] return dfs def preprocessing ( self ): # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def visualisasiGraph ( self , source_data , part_data , srs_param ): \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node. part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan partOf().visualisasiGraph(source_data, part_data, srs_param) \"\"\" f = gf . Digraph ( 'finite_state_machine' , filename = 'partOf.gv' , engine = 'neato' ) f . attr ( rankdir = 'LR' , size = '8,5' ) f . attr ( 'node' , shape = 'doublecircle' ) # node for angka in source_data . ID : f . node ( angka ) f . attr ( kw = 'node' , shape = 'circle' ) # edge for idx , num in zip ( part_data . label , part_data . ID ): f . edge ( idx , num , label = 'partOf' ) f . attr ( overlap = 'false' ) f . attr ( label = r 'Visulasisasi relasi partOf {}\\n' . format ( srs_param )) f . attr ( fontsize = '20' ) f . view ( data_simpan ) print ( \"Gambar disimpan ke {}\" . format ( data_simpan )) return f def evaluasi_data ( self , data1 , data2 ): \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) \"\"\" y_actual = data1 . values . astype ( int ) #define array of actual values y_predicted = data2 . values . astype ( int ) #define array of predicted values nilai_akurasi = accuracy_score ( y_actual , y_predicted , normalize = True ) nilai_recall = recall_score ( y_actual , y_predicted , average = 'macro' ) nilai_presisi = precision_score ( y_actual , y_predicted , average = 'macro' ) print ( \"akurasi {} \\n recall {} \\n presisi {} \\n \" . format ( nilai_akurasi , nilai_recall , nilai_presisi )) print ( classification_report ( y_actual , y_predicted )) def simpan_excel ( self , data1 , data2 , data3 , data4 ): \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan, partOf, relasi, dan nilai data secara statistik. cara menggunakan syntax ini yaitu melalui partOf().simpan_excel(data1, data2, data3, data4) \"\"\" dfs = { # save file 'tabel_kebutuhan' : data1 , 'tabel_partOf' : data2 , 'tabel_relasi' : data3 , 'tabel_statistika' : data4 , } writer = pd . ExcelWriter ( data_simpan + '.xlsx' ) for name , dataframe in dfs . items (): dataframe . to_excel ( writer , name , index = False ) writer . save () print ( \"data excel disimpan di {}\" . format ( data_simpan + '.xlsx' )) # def tabulasi_filter(self, data, mode_data= ['manual', 'triplet']): # tabulasi_filter def tabulasi_filter ( self , data ): # tabulasi_filter \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford. sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya. cara menggunakan syntax ini yaitu melalui partOf().tabulasi_filter(hasil_req) \"\"\" hasil_srs = [] for idx , num in zip ( data [ 'ID' ], data [ 'Requirement Statement' ] . fillna ( \"empty\" )): data = [ x10 for x1 in num . split ( \".\" ) for x2 in x1 . split ( \" that \" ) for x3 in x2 . split ( \"/\" ) for x4 in x3 . split ( \" so \" ) for x5 in x4 . split ( \",\" ) for x6 in x5 . split ( \" and \" ) for x7 in x6 . split ( \" i.e.\" ) for x8 in x7 . split ( \" or \" ) for x9 in x8 . split ( \" if \" ) for x10 in x9 . split ( \" ; \" )] conv = lambda i : i or None res = [ conv ( i ) for i in data ] hasil_srs . append ([ idx , res ]) a_df = pd . DataFrame ( hasil_srs , columns = [ 'ID' , 'data' ]) return a_df def tabulasi_pertama ( self , data , dataReq ): # tabulasi_pertama \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik, dari banyak kalimat yang digunakan. jika terdiri atas satu kalimat maka disebut sebagai atomik. namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. partOf().tabulasi_pertama(hasil_filter, hasil_req) \"\"\" c_df = data . copy () data_df = pd . DataFrame ([ sh for sh in c_df . data ], index = dataReq . ID ) list_column = [ \"data{}\" . format ( num ) for num in range ( data_df . columns . stop )] data_df . columns = list_column b_df = [] b_df_jumlah = [] for num in c_df . data : # menentukan data atomik dan if len ( num ) > 1 : # non atomik berdasarkan jumlah b_df . append ( 'non_atomik' ) b_df_jumlah . append ( len ( num )) elif len ( num ) == 1 : b_df . append ( 'atomik' ) b_df_jumlah . append ( len ( num )) c_df [ 'label' ] = b_df c_df [ 'jumlah' ] = b_df_jumlah return c_df def tabulasi_kedua ( self , data ): # tabulasi kedua \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'req' , 'label' ]) a_data = c_df . loc [ c_df [ 'label' ] == 'atomik' ] data_a = [([ a_data . ID [ num ], index , 'atomik' ]) for num in a_data . index for idx , index in enumerate ( a_data . data [ num ]) if index is not None ] a_df = pd . DataFrame ( data_a , columns = [ 'ID' , 'req' , 'label' ]) part_df = pd . concat ([ a_df , na_df ], ignore_index = True ) part_srt = part_df . sort_values ( by = 'ID' , ignore_index = True ) . drop_duplicates () return part_srt def tabulasi_ketiga ( self , data , data_index ): # tabulasi ketiga \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain. sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan partOf().tabulasi_ketiga(hasil_kedua, hasil_req) \"\"\" part_srt = data . copy () list_data = [ part_srt . loc [ part_srt . ID == num ] . label for num in data_index . ID ] tb_part = pd . DataFrame ( list_data ) . fillna ( 0 ) tb_part . columns = part_srt . ID tb_part . index = data_index . ID return tb_part . reset_index () def tabulasi_alternatifernatif ( self , data ): # Alternatif \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) \"\"\" d_df = data . copy () na_data = d_df . loc [ d_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'data' , 'label' ]) a_data = d_df . loc [ d_df [ 'label' ] == 'atomik' ] dt = pd . concat ([ a_data , na_data , na_df ], ignore_index = True ) part_br = dt . sort_values ( by = 'ID' , ignore_index = True ) list_data = [ part_br . loc [ part_br . ID == num ] . label for num in data . ID ] dt_part = pd . DataFrame ( list_data ) . fillna ( 0 ) # rename data data_part = [([ '{}_{}' . format ( na_data . ID [ num ], idy ), index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_na = pd . DataFrame ( data_part , columns = [ 'ID' , 'req' , 'label' ]) dt_rename = pd . concat ([ a_data , na_data , part_na ], ignore_index = True ) sort_rename = dt_rename . sort_values ( by = 'ID' ) dt_part . columns = sort_rename . ID dt_part . index = data . ID return dt_part . reset_index () def tabulasi_visual ( self , data ): # visualisasi \"\"\" fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] part_list = [([ na_data . ID [ num ], index , 'p{}_{}' . format ( idx , idy )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_visual = pd . DataFrame ( part_list , columns = [ 'ID' , 'req' , 'label' ]) return partOf . visualisasiGraph ( self , data , part_visual , srs_param ) def nilai_stat ( self , data1 , data2 ): # fungsi menentukan nilai statistik \"\"\" fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_stat(data1, data2) \"\"\" jml_kebutuhan = len ( data1 ) jml_minimum = data1 . jumlah . min () jml_maksimum = data1 . jumlah . max () jml_atomik = len ( data1 . loc [ data1 [ 'label' ] == 'atomik' ]) jml_nonatomik = len ( data1 . loc [ data1 [ 'label' ] == 'non_atomik' ]) jml_klausa = len ( data2 . loc [ data2 [ 'label' ] != 'atomik' ]) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonatomik , jml_minimum , jml_maksimum ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_test' ] return jml_df . reset_index () def stat_grountruth ( self , data ): \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) \"\"\" df_part = data . copy () nlp = spacy . load ( 'en_core_web_sm' ) jml_atomik = df_part . loc [ df_part [ 'Sentence' ] == 'a' ] . Sentence . count () jml_nonAtomik = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . drop_duplicates ( subset = 'Sentence' ) . Sentence . count () jml_klausa = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . Sentence . count () jml_kebutuhan = jml_atomik + jml_nonAtomik jml_data = [ len ([ idx for idx in nlp ( num ) . sents ]) for num in df_part [ 'Requirement Statement' ]] jml_a = [ num for num in df_part . Sentence . value_counts () . astype ( int )] jml_min = min ( jml_data ) try : jml_maks = max ( jml_a [ 1 :]) except : jml_maks = max ( jml_data ) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonAtomik , jml_min , jml_maks ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_groundtruth' ] return jml_df . reset_index () def __del__ ( self ): \"\"\" fungsi ini digunakan untuk mendestruksi, cara meggunakan panggil fungsi dengan syntax berikut ini: partOf().__del__() \"\"\" pass def extractPart ( self , grd_param , file_param , srs_param , output = tab_param ): \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']) \"\"\" part2 = partOf ( grd_param ) part_grd = part2 . fulldataset ( srs_param ) data_grountruth = part2 . stat_grountruth ( part_grd ) part2 . __del__ () part1 = partOf ( file_param ) dataReq = part1 . fulldataset ( srs_param ) data_filtrasi = part1 . tabulasi_filter ( dataReq ) data_pertama = part1 . tabulasi_pertama ( data_filtrasi , dataReq ) data_kedua = part1 . tabulasi_kedua ( data_pertama ) data_ketiga = part1 . tabulasi_ketiga ( data_kedua , data_pertama ) alternatif = part1 . tabulasi_alternatifernatif ( data_pertama ) data_visual = part1 . tabulasi_visual ( data_pertama ) data_stat = part1 . nilai_stat ( data_pertama , data_kedua ) part1 . simpan_excel ( data_pertama , data_kedua , data_ketiga , data_stat ) part1 . __del__ () if 'pertama' in output : print ( \" \\n Tabulasi Pertama {}\" . format ( srs_param )) print ( tabulate ( data_pertama , headers = 'keys' , tablefmt = 'psql' )) elif 'kedua' in output : print ( \" \\n Tabulasi Kedua {}\" . format ( srs_param )) print ( tabulate ( data_kedua , headers = 'keys' , tablefmt = 'psql' )) elif 'ketiga' in output : print ( \" \\n Tabulasi Ketiga {}\" . format ( srs_param )) print ( tabulate ( data_ketiga , headers = 'keys' , tablefmt = 'psql' )) elif 'alternatif' in output : print ( \" \\n Tabulasi Ketiga Alternatif {}\" . format ( srs_param )) print ( tabulate ( alternatif , headers = 'keys' , tablefmt = 'psql' )) elif 'stat' in output : print ( \" \\n Tabulasi Statistik {}\" . format ( srs_param )) print ( tabulate ( data_grountruth , headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( data_stat , headers = 'keys' , tablefmt = 'psql' )) part2 . evaluasi_data ( data_stat . drop ( 'index' , axis = 1 ), data_grountruth . drop ( 'index' , axis = 1 )) if __name__ == \"__main__\" : try : partOf () . extractPart ( tab_param ) except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"Module extractreq.modul_ekspart"},{"location":"reference/extractreq/modul_ekspart/#variables","text":"col_param dataFile data_simpan file_param grd_param mode_data model_param save_param spacy_param srs_param tab_param url_param","title":"Variables"},{"location":"reference/extractreq/modul_ekspart/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/modul_ekspart/#partof","text":"class partOf ( inputData = '/content/drive/MyDrive/dataset/dataset_2.xlsx' ) View Source class partOf : #template class partOf def __init__ ( self , inputData = file_param ): \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = inputData # data inisiliasi file parameter def fulldataset ( self , inputSRS ): # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names }[ inputSRS ] return dfs def preprocessing ( self ): # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def visualisasiGraph ( self , source_data , part_data , srs_param ): \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node. part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan partOf().visualisasiGraph(source_data, part_data, srs_param) \"\"\" f = gf . Digraph ( 'finite_state_machine' , filename = 'partOf.gv' , engine = 'neato' ) f . attr ( rankdir = 'LR' , size = '8,5' ) f . attr ( 'node' , shape = 'doublecircle' ) # node for angka in source_data . ID : f . node ( angka ) f . attr ( kw = 'node' , shape = 'circle' ) # edge for idx , num in zip ( part_data . label , part_data . ID ): f . edge ( idx , num , label = 'partOf' ) f . attr ( overlap = 'false' ) f . attr ( label = r 'Visulasisasi relasi partOf {}\\n' . format ( srs_param )) f . attr ( fontsize = '20' ) f . view ( data_simpan ) print ( \"Gambar disimpan ke {}\" . format ( data_simpan )) return f def evaluasi_data ( self , data1 , data2 ): \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) \"\"\" y_actual = data1 . values . astype ( int ) #define array of actual values y_predicted = data2 . values . astype ( int ) #define array of predicted values nilai_akurasi = accuracy_score ( y_actual , y_predicted , normalize = True ) nilai_recall = recall_score ( y_actual , y_predicted , average = 'macro' ) nilai_presisi = precision_score ( y_actual , y_predicted , average = 'macro' ) print ( \"akurasi {} \\n recall {} \\n presisi {} \\n \" . format ( nilai_akurasi , nilai_recall , nilai_presisi )) print ( classification_report ( y_actual , y_predicted )) def simpan_excel ( self , data1 , data2 , data3 , data4 ): \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan, partOf, relasi, dan nilai data secara statistik. cara menggunakan syntax ini yaitu melalui partOf().simpan_excel(data1, data2, data3, data4) \"\"\" dfs = { # save file 'tabel_kebutuhan' : data1 , 'tabel_partOf' : data2 , 'tabel_relasi' : data3 , 'tabel_statistika' : data4 , } writer = pd . ExcelWriter ( data_simpan + '.xlsx' ) for name , dataframe in dfs . items (): dataframe . to_excel ( writer , name , index = False ) writer . save () print ( \"data excel disimpan di {}\" . format ( data_simpan + '.xlsx' )) # def tabulasi_filter(self, data, mode_data= ['manual', 'triplet']): # tabulasi_filter def tabulasi_filter ( self , data ): # tabulasi_filter \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford. sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya. cara menggunakan syntax ini yaitu melalui partOf().tabulasi_filter(hasil_req) \"\"\" hasil_srs = [] for idx , num in zip ( data [ 'ID' ], data [ 'Requirement Statement' ] . fillna ( \"empty\" )): data = [ x10 for x1 in num . split ( \".\" ) for x2 in x1 . split ( \" that \" ) for x3 in x2 . split ( \"/\" ) for x4 in x3 . split ( \" so \" ) for x5 in x4 . split ( \",\" ) for x6 in x5 . split ( \" and \" ) for x7 in x6 . split ( \" i.e.\" ) for x8 in x7 . split ( \" or \" ) for x9 in x8 . split ( \" if \" ) for x10 in x9 . split ( \" ; \" )] conv = lambda i : i or None res = [ conv ( i ) for i in data ] hasil_srs . append ([ idx , res ]) a_df = pd . DataFrame ( hasil_srs , columns = [ 'ID' , 'data' ]) return a_df def tabulasi_pertama ( self , data , dataReq ): # tabulasi_pertama \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik, dari banyak kalimat yang digunakan. jika terdiri atas satu kalimat maka disebut sebagai atomik. namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. partOf().tabulasi_pertama(hasil_filter, hasil_req) \"\"\" c_df = data . copy () data_df = pd . DataFrame ([ sh for sh in c_df . data ], index = dataReq . ID ) list_column = [ \"data{}\" . format ( num ) for num in range ( data_df . columns . stop )] data_df . columns = list_column b_df = [] b_df_jumlah = [] for num in c_df . data : # menentukan data atomik dan if len ( num ) > 1 : # non atomik berdasarkan jumlah b_df . append ( 'non_atomik' ) b_df_jumlah . append ( len ( num )) elif len ( num ) == 1 : b_df . append ( 'atomik' ) b_df_jumlah . append ( len ( num )) c_df [ 'label' ] = b_df c_df [ 'jumlah' ] = b_df_jumlah return c_df def tabulasi_kedua ( self , data ): # tabulasi kedua \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'req' , 'label' ]) a_data = c_df . loc [ c_df [ 'label' ] == 'atomik' ] data_a = [([ a_data . ID [ num ], index , 'atomik' ]) for num in a_data . index for idx , index in enumerate ( a_data . data [ num ]) if index is not None ] a_df = pd . DataFrame ( data_a , columns = [ 'ID' , 'req' , 'label' ]) part_df = pd . concat ([ a_df , na_df ], ignore_index = True ) part_srt = part_df . sort_values ( by = 'ID' , ignore_index = True ) . drop_duplicates () return part_srt def tabulasi_ketiga ( self , data , data_index ): # tabulasi ketiga \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain. sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan partOf().tabulasi_ketiga(hasil_kedua, hasil_req) \"\"\" part_srt = data . copy () list_data = [ part_srt . loc [ part_srt . ID == num ] . label for num in data_index . ID ] tb_part = pd . DataFrame ( list_data ) . fillna ( 0 ) tb_part . columns = part_srt . ID tb_part . index = data_index . ID return tb_part . reset_index () def tabulasi_alternatifernatif ( self , data ): # Alternatif \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) \"\"\" d_df = data . copy () na_data = d_df . loc [ d_df [ 'label' ] == 'non_atomik' ] data_na = [([ na_data . ID [ num ], index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID' , 'data' , 'label' ]) a_data = d_df . loc [ d_df [ 'label' ] == 'atomik' ] dt = pd . concat ([ a_data , na_data , na_df ], ignore_index = True ) part_br = dt . sort_values ( by = 'ID' , ignore_index = True ) list_data = [ part_br . loc [ part_br . ID == num ] . label for num in data . ID ] dt_part = pd . DataFrame ( list_data ) . fillna ( 0 ) # rename data data_part = [([ '{}_{}' . format ( na_data . ID [ num ], idy ), index , 'p{}' . format ( idx )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_na = pd . DataFrame ( data_part , columns = [ 'ID' , 'req' , 'label' ]) dt_rename = pd . concat ([ a_data , na_data , part_na ], ignore_index = True ) sort_rename = dt_rename . sort_values ( by = 'ID' ) dt_part . columns = sort_rename . ID dt_part . index = data . ID return dt_part . reset_index () def tabulasi_visual ( self , data ): # visualisasi \"\"\" fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df [ 'label' ] == 'non_atomik' ] part_list = [([ na_data . ID [ num ], index , 'p{}_{}' . format ( idx , idy )]) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ]) if index is not None ] part_visual = pd . DataFrame ( part_list , columns = [ 'ID' , 'req' , 'label' ]) return partOf . visualisasiGraph ( self , data , part_visual , srs_param ) def nilai_stat ( self , data1 , data2 ): # fungsi menentukan nilai statistik \"\"\" fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_stat(data1, data2) \"\"\" jml_kebutuhan = len ( data1 ) jml_minimum = data1 . jumlah . min () jml_maksimum = data1 . jumlah . max () jml_atomik = len ( data1 . loc [ data1 [ 'label' ] == 'atomik' ]) jml_nonatomik = len ( data1 . loc [ data1 [ 'label' ] == 'non_atomik' ]) jml_klausa = len ( data2 . loc [ data2 [ 'label' ] != 'atomik' ]) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonatomik , jml_minimum , jml_maksimum ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_test' ] return jml_df . reset_index () def stat_grountruth ( self , data ): \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) \"\"\" df_part = data . copy () nlp = spacy . load ( 'en_core_web_sm' ) jml_atomik = df_part . loc [ df_part [ 'Sentence' ] == 'a' ] . Sentence . count () jml_nonAtomik = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . drop_duplicates ( subset = 'Sentence' ) . Sentence . count () jml_klausa = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . Sentence . count () jml_kebutuhan = jml_atomik + jml_nonAtomik jml_data = [ len ([ idx for idx in nlp ( num ) . sents ]) for num in df_part [ 'Requirement Statement' ]] jml_a = [ num for num in df_part . Sentence . value_counts () . astype ( int )] jml_min = min ( jml_data ) try : jml_maks = max ( jml_a [ 1 :]) except : jml_maks = max ( jml_data ) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonAtomik , jml_min , jml_maks ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_groundtruth' ] return jml_df . reset_index () def __del__ ( self ): \"\"\" fungsi ini digunakan untuk mendestruksi, cara meggunakan panggil fungsi dengan syntax berikut ini: partOf().__del__() \"\"\" pass def extractPart ( self , grd_param , file_param , srs_param , output = tab_param ): \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']) \"\"\" part2 = partOf ( grd_param ) part_grd = part2 . fulldataset ( srs_param ) data_grountruth = part2 . stat_grountruth ( part_grd ) part2 . __del__ () part1 = partOf ( file_param ) dataReq = part1 . fulldataset ( srs_param ) data_filtrasi = part1 . tabulasi_filter ( dataReq ) data_pertama = part1 . tabulasi_pertama ( data_filtrasi , dataReq ) data_kedua = part1 . tabulasi_kedua ( data_pertama ) data_ketiga = part1 . tabulasi_ketiga ( data_kedua , data_pertama ) alternatif = part1 . tabulasi_alternatifernatif ( data_pertama ) data_visual = part1 . tabulasi_visual ( data_pertama ) data_stat = part1 . nilai_stat ( data_pertama , data_kedua ) part1 . simpan_excel ( data_pertama , data_kedua , data_ketiga , data_stat ) part1 . __del__ () if 'pertama' in output : print ( \" \\n Tabulasi Pertama {}\" . format ( srs_param )) print ( tabulate ( data_pertama , headers = 'keys' , tablefmt = 'psql' )) elif 'kedua' in output : print ( \" \\n Tabulasi Kedua {}\" . format ( srs_param )) print ( tabulate ( data_kedua , headers = 'keys' , tablefmt = 'psql' )) elif 'ketiga' in output : print ( \" \\n Tabulasi Ketiga {}\" . format ( srs_param )) print ( tabulate ( data_ketiga , headers = 'keys' , tablefmt = 'psql' )) elif 'alternatif' in output : print ( \" \\n Tabulasi Ketiga Alternatif {}\" . format ( srs_param )) print ( tabulate ( alternatif , headers = 'keys' , tablefmt = 'psql' )) elif 'stat' in output : print ( \" \\n Tabulasi Statistik {}\" . format ( srs_param )) print ( tabulate ( data_grountruth , headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( data_stat , headers = 'keys' , tablefmt = 'psql' )) part2 . evaluasi_data ( data_stat . drop ( 'index' , axis = 1 ), data_grountruth . drop ( 'index' , axis = 1 ))","title":"partOf"},{"location":"reference/extractreq/modul_ekspart/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/modul_ekspart/#evaluasi_data","text":"def evaluasi_data ( self , data1 , data2 ) fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) View Source def evaluasi_data(self, data1, data2): \"\"\" fungsi ini digunakan untuk mengevaluasi data. nilai evaluasi meliputi nilai akurasi, recall, presisi dengan mengubah datanya menjadi int terlebih dahulu. cara menggunakan syntax ini yaitu melalui partOf().evaluasi_data(data1, data2) \"\"\" y_actual = data1.values.astype(int) #define array of actual values y_predicted = data2.values.astype(int) #define array of predicted values nilai_akurasi = accuracy_score(y_actual, y_predicted, normalize=True) nilai_recall = recall_score(y_actual, y_predicted, average= 'macro') nilai_presisi = precision_score(y_actual, y_predicted, average= 'macro') print(\"akurasi {}\\n recall {}\\n presisi {}\\n\".format(nilai_akurasi, nilai_recall, nilai_presisi)) print(classification_report(y_actual, y_predicted))","title":"evaluasi_data"},{"location":"reference/extractreq/modul_ekspart/#extractpart","text":"def extractPart ( self , grd_param , file_param , srs_param , output = 'pertama' ) fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= ['pertama', 'kedua', 'ketiga', 'alternatif', 'stat']) View Source def extractPart ( self , grd_param , file_param , srs_param , output = tab_param ): \"\"\" fungsi ini digunakan untuk mengekstraksi secara lengkap data yang digunakan. fungsi ini menunjukkan data ekstraksi yang digunakan meliputi - part1: data filtrasi, data pertama, data kedua, data ketiga/alternatif, data visual, data statistik, dan simpan - par2; data groundtruth beserta nilai statistiknya partOf().extractPart(grd_param, file_param, srs_param, output= [ 'pertama' , 'kedua' , 'ketiga' , 'alternatif' , 'stat' ] ) \"\"\" part2 = partOf ( grd_param ) part_grd = part2 . fulldataset ( srs_param ) data_grountruth = part2 . stat_grountruth ( part_grd ) part2 . __del__ () part1 = partOf ( file_param ) dataReq = part1 . fulldataset ( srs_param ) data_filtrasi = part1 . tabulasi_filter ( dataReq ) data_pertama = part1 . tabulasi_pertama ( data_filtrasi , dataReq ) data_kedua = part1 . tabulasi_kedua ( data_pertama ) data_ketiga = part1 . tabulasi_ketiga ( data_kedua , data_pertama ) alternatif = part1 . tabulasi_alternatifernatif ( data_pertama ) data_visual = part1 . tabulasi_visual ( data_pertama ) data_stat = part1 . nilai_stat ( data_pertama , data_kedua ) part1 . simpan_excel ( data_pertama , data_kedua , data_ketiga , data_stat ) part1 . __del__ () if 'pertama' in output : print ( \"\\nTabulasi Pertama {}\" . format ( srs_param )) print ( tabulate ( data_pertama , headers = 'keys' , tablefmt = 'psql' )) elif 'kedua' in output : print ( \"\\nTabulasi Kedua {}\" . format ( srs_param )) print ( tabulate ( data_kedua , headers = 'keys' , tablefmt = 'psql' )) elif 'ketiga' in output : print ( \"\\nTabulasi Ketiga {}\" . format ( srs_param )) print ( tabulate ( data_ketiga , headers = 'keys' , tablefmt = 'psql' )) elif 'alternatif' in output : print ( \"\\nTabulasi Ketiga Alternatif {}\" . format ( srs_param )) print ( tabulate ( alternatif , headers = 'keys' , tablefmt = 'psql' )) elif 'stat' in output : print ( \"\\nTabulasi Statistik {}\" . format ( srs_param )) print ( tabulate ( data_grountruth , headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( data_stat , headers = 'keys' , tablefmt = 'psql' )) part2 . evaluasi_data ( data_stat . drop ( 'index' , axis = 1 ), data_grountruth . drop ( 'index' , axis = 1 ))","title":"extractPart"},{"location":"reference/extractreq/modul_ekspart/#fulldataset","text":"def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs","title":"fulldataset"},{"location":"reference/extractreq/modul_ekspart/#nilai_stat","text":"def nilai_stat ( self , data1 , data2 ) fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_stat(data1, data2) View Source def nilai_stat ( self , data1 , data2 ) : # fungsi menentukan nilai statistik \"\"\" fungsi ini digunakan untuk melihat data statistik test, fungsi efektif untuk melihat statistik secara keseluruhan , yang meliputi jumlah kebutuhan , atomik , nonatomik , klausa , maksimum kalimat , minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf () . stat_stat ( data1 , data2 ) \"\"\" jml_kebutuhan = len ( data1 ) jml_minimum = data1 . jumlah . min () jml_maksimum = data1 . jumlah . max () jml_atomik = len ( data1 . loc [ data1 [ ' label ' ] == ' atomik ' ] ) jml_nonatomik = len ( data1 . loc [ data1 [ ' label ' ] == ' non_atomik ' ] ) jml_klausa = len ( data2 . loc [ data2 [ ' label ' ] != ' atomik ' ] ) jml_df = pd . DataFrame ( [ jml_kebutuhan , jml_atomik , jml_nonatomik , jml_minimum , jml_maksimum ] ) jml_df . index = [ ' jumlah_kebtuhan ' , ' jumlah_atomik ' , ' jumlah_nonatomik ' , ' minimum_jumlah_kalimat ' , ' maksimum_jumlah_kalimat ' ] jml_df . columns = [ ' statistik_test ' ] return jml_df . reset_index ()","title":"nilai_stat"},{"location":"reference/extractreq/modul_ekspart/#preprocessing","text":"def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/extractreq/modul_ekspart/#simpan_excel","text":"def simpan_excel ( self , data1 , data2 , data3 , data4 ) fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan, partOf, relasi, dan nilai data secara statistik. cara menggunakan syntax ini yaitu melalui partOf().simpan_excel(data1, data2, data3, data4) View Source def simpan_excel ( self , data1 , data2 , data3 , data4 ) : \"\"\" fungsi ini digunakan untuk menyimpanda data. data yang digunakan meliputi tabel kebutuhan , partOf , relasi , dan nilai data secara statistik . cara menggunakan syntax ini yaitu melalui partOf () . simpan_excel ( data1 , data2 , data3 , data4 ) \"\"\" dfs = { # save file ' tabel_kebutuhan ' : data1 , ' tabel_partOf ' : data2 , ' tabel_relasi ' : data3 , ' tabel_statistika ' : data4 , } writer = pd . ExcelWriter ( data_simpan + ' .xlsx ' ) for name , dataframe in dfs . items () : dataframe . to_excel ( writer , name , index = False ) writer . save () print ( \" data excel disimpan di {} \" . format ( data_simpan + ' .xlsx ' ))","title":"simpan_excel"},{"location":"reference/extractreq/modul_ekspart/#stat_grountruth","text":"def stat_grountruth ( self , data ) fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) View Source def stat_grountruth ( self , data ): \"\"\" fungsi ini digunakan untuk melihat data statistik groundtruth, fungsi efektif untuk melihat statistik secara keseluruhan, yang meliputi jumlah kebutuhan, atomik, nonatomik, klausa, maksimum kalimat, minimum kalimat cara menggunakan syntax ini adalah dengan cara partOf().stat_grountruth(data) \"\"\" df_part = data . copy () nlp = spacy . load ( 'en_core_web_sm' ) jml_atomik = df_part . loc [ df_part [ 'Sentence' ] == 'a' ] . Sentence . count () jml_nonAtomik = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . drop_duplicates ( subset = 'Sentence' ) . Sentence . count () jml_klausa = df_part . loc [ df_part [ 'Sentence' ] != 'a' ] . Sentence . count () jml_kebutuhan = jml_atomik + jml_nonAtomik jml_data = [ len ([ idx for idx in nlp ( num ) . sents ]) for num in df_part [ 'Requirement Statement' ]] jml_a = [ num for num in df_part . Sentence . value_counts () . astype ( int )] jml_min = min ( jml_data ) try : jml_maks = max ( jml_a [ 1 :]) except : jml_maks = max ( jml_data ) jml_df = pd . DataFrame ([ jml_kebutuhan , jml_atomik , jml_nonAtomik , jml_min , jml_maks ]) jml_df . index = [ 'jumlah_kebtuhan' , 'jumlah_atomik' , 'jumlah_nonatomik' , 'minimum_jumlah_kalimat' , 'maksimum_jumlah_kalimat' ] jml_df . columns = [ 'statistik_groundtruth' ] return jml_df . reset_index ()","title":"stat_grountruth"},{"location":"reference/extractreq/modul_ekspart/#tabulasi_alternatifernatif","text":"def tabulasi_alternatifernatif ( self , data ) fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) View Source def tabulasi_alternatifernatif ( self , data ) : # Alternatif \"\"\" fungsi ini digunakan untuk tabulasi ketiga alternatif. untuk memodifikasi kolom yang semula hanya memiliki p# saja, namun dengan fungsi ini dapat melihat jenis non_atomik dalam sebuah kebutuhan. Berikut ini syntax yang digunakan. partOf().tabulasi_alternatifernatif(data) \"\"\" d_df = data . copy () na_data = d_df . loc [ d_df['label' ] == 'non_atomik' ] data_na = [ ([na_data.ID[num ] , index , 'p{}' . format ( idx ) ] ) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID', 'data', 'label' ] ) a_data = d_df . loc [ d_df['label' ] == 'atomik' ] dt = pd . concat ( [ a_data, na_data, na_df ] , ignore_index = True ) part_br = dt . sort_values ( by = 'ID' , ignore_index = True ) list_data = [ part_br.loc[part_br.ID == num ] . label for num in data . ID ] dt_part = pd . DataFrame ( list_data ). fillna ( 0 ) # rename data data_part = [ (['{}_{}'.format(na_data.ID[num ] , idy ), index , 'p{}' . format ( idx ) ] ) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ] ) if index is not None ] part_na = pd . DataFrame ( data_part , columns = [ 'ID', 'req', 'label' ] ) dt_rename = pd . concat ( [ a_data, na_data, part_na ] , ignore_index = True ) sort_rename = dt_rename . sort_values ( by = 'ID' ) dt_part . columns = sort_rename . ID dt_part . index = data . ID return dt_part . reset_index ()","title":"tabulasi_alternatifernatif"},{"location":"reference/extractreq/modul_ekspart/#tabulasi_filter","text":"def tabulasi_filter ( self , data ) fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual, triplet, spacy, dan stanford. sesuai dengan namanya. maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya. cara menggunakan syntax ini yaitu melalui partOf().tabulasi_filter(hasil_req) View Source def tabulasi_filter ( self , data ) : # tabulasi_filter \"\"\" fungsi ini digunakan untuk memfilter data berdasarkan mode yang digunakan. mode ini terdiri atas 4 macam mode yaitu manual , triplet , spacy , dan stanford . sesuai dengan namanya . maka fungsi ini menunjukkan hasil berbeda sesuai dengan fungsinya . cara menggunakan syntax ini yaitu melalui partOf () . tabulasi_filter ( hasil_req ) \"\"\" hasil_srs = [] for idx , num in zip ( data [ ' ID ' ], data [ ' Requirement Statement ' ]. fillna ( \" empty \" )) : data = [ x10 for x1 in num . split ( \" . \" ) for x2 in x1 . split ( \" that \" ) for x3 in x2 . split ( \" / \" ) for x4 in x3 . split ( \" so \" ) for x5 in x4 . split ( \" , \" ) for x6 in x5 . split ( \" and \" ) for x7 in x6 . split ( \" i.e. \" ) for x8 in x7 . split ( \" or \" ) for x9 in x8 . split ( \" if \" ) for x10 in x9 . split ( \" ; \" ) ] conv = lambda i : i or None res = [ conv ( i ) for i in data ] hasil_srs . append ( [ idx , res ] ) a_df = pd . DataFrame ( hasil_srs , columns = [ ' ID ' , ' data ' ] ) return a_df","title":"tabulasi_filter"},{"location":"reference/extractreq/modul_ekspart/#tabulasi_kedua","text":"def tabulasi_kedua ( self , data ) fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) View Source def tabulasi_kedua ( self , data ) : # tabulasi kedua \"\"\" fungsi ini digunakan untuk mengubah data tabulasi pertama menjadi dari non atomik menjadi p#, sehingga hasilnya cukup detail menunjukkan setiap non atomik memiliki kebergantungan partOf didalamnya. partOf().tabulasi_kedua(hasil_pertama) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df['label' ] == 'non_atomik' ] data_na = [ ([na_data.ID[num ] , index , 'p{}' . format ( idx ) ] ) for idx , num in enumerate ( na_data . index ) for index in na_data . data [ num ] if index is not None ] na_df = pd . DataFrame ( data_na , columns = [ 'ID', 'req', 'label' ] ) a_data = c_df . loc [ c_df['label' ] == 'atomik' ] data_a = [ ([a_data.ID[num ] , index , 'atomik' ] ) for num in a_data . index for idx , index in enumerate ( a_data . data [ num ] ) if index is not None ] a_df = pd . DataFrame ( data_a , columns = [ 'ID', 'req', 'label' ] ) part_df = pd . concat ( [ a_df, na_df ] , ignore_index = True ) part_srt = part_df . sort_values ( by = 'ID' , ignore_index = True ). drop_duplicates () return part_srt","title":"tabulasi_kedua"},{"location":"reference/extractreq/modul_ekspart/#tabulasi_ketiga","text":"def tabulasi_ketiga ( self , data , data_index ) fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain. sehingga dengan cara ini, dapat terlihat relasi atomik, p# dalam sebuah kebutuhan partOf().tabulasi_ketiga(hasil_kedua, hasil_req) View Source def tabulasi_ketiga ( self , data , data_index ) : # tabulasi ketiga \"\"\" fungsi ini digunakan untuk mengubah data data tabulasi kedua menjadi sebuah matriks indeks dan kolom yang saling berelasi satu sama lain . sehingga dengan cara ini , dapat terlihat relasi atomik , p # dalam sebuah kebutuhan partOf () . tabulasi_ketiga ( hasil_kedua , hasil_req ) \"\"\" part_srt = data . copy () list_data = [ part_srt . loc [ part_srt . ID == num ]. label for num in data_index . ID ] tb_part = pd . DataFrame ( list_data ) . fillna ( 0 ) tb_part . columns = part_srt . ID tb_part . index = data_index . ID return tb_part . reset_index ()","title":"tabulasi_ketiga"},{"location":"reference/extractreq/modul_ekspart/#tabulasi_pertama","text":"def tabulasi_pertama ( self , data , dataReq ) fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik, dari banyak kalimat yang digunakan. jika terdiri atas satu kalimat maka disebut sebagai atomik. namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik. partOf().tabulasi_pertama(hasil_filter, hasil_req) View Source def tabulasi_pertama ( self , data , dataReq ) : # tabulasi_pertama \"\"\" fungsi ini digunakan untuk mengubah data tabulasi filter menjadi data atomik dan non atomik , dari banyak kalimat yang digunakan . jika terdiri atas satu kalimat maka disebut sebagai atomik . namun sebaliknya jika lebih dari satu kalimat maka disebut non atomik . partOf () . tabulasi_pertama ( hasil_filter , hasil_req ) \"\"\" c_df = data . copy () data_df = pd . DataFrame ( [ sh for sh in c_df . data ], index = dataReq . ID ) list_column = [ \" data{} \" . format ( num ) for num in range ( data_df . columns . stop ) ] data_df . columns = list_column b_df = [] b_df_jumlah = [] for num in c_df . data : # menentukan data atomik dan if len ( num ) > 1 : # non atomik berdasarkan jumlah b_df . append ( ' non_atomik ' ) b_df_jumlah . append ( len ( num )) elif len ( num ) == 1 : b_df . append ( ' atomik ' ) b_df_jumlah . append ( len ( num )) c_df [ ' label ' ] = b_df c_df [ ' jumlah ' ] = b_df_jumlah return c_df","title":"tabulasi_pertama"},{"location":"reference/extractreq/modul_ekspart/#tabulasi_visual","text":"def tabulasi_visual ( self , data ) fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) View Source def tabulasi_visual ( self , data ) : # visualisasi \"\"\" fungsi ini digunakan untuk melihat data secara visual, fungsi efektif untuk merubah indeks data yang sama, mememiliki urutan sehingga penggunaan ini cocok untuk digunakan untuk proses selanjutnya yaitu visual data partOf().tabulasi_visual(data) \"\"\" c_df = data . copy () na_data = c_df . loc [ c_df['label' ] == 'non_atomik' ] part_list = [ ([na_data.ID[num ] , index , 'p{}_{}' . format ( idx , idy ) ] ) for idx , num in enumerate ( na_data . index ) for idy , index in enumerate ( na_data . data [ num ] ) if index is not None ] part_visual = pd . DataFrame ( part_list , columns = [ 'ID', 'req', 'label' ] ) return partOf . visualisasiGraph ( self , data , part_visual , srs_param )","title":"tabulasi_visual"},{"location":"reference/extractreq/modul_ekspart/#visualisasigraph","text":"def visualisasiGraph ( self , source_data , part_data , srs_param ) fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node. part_data untuk node child, dan indeks yang digunakan sesuai data srs yang digunakan partOf().visualisasiGraph(source_data, part_data, srs_param) View Source def visualisasiGraph ( self , source_data , part_data , srs_param ) : \"\"\" fungsi ini digunakan untuk memvisualisasikan dalam bentuk graf. data diambil berdasarkan referensi dari source data untuk parent node . part_data untuk node child , dan indeks yang digunakan sesuai data srs yang digunakan partOf () . visualisasiGraph ( source_data , part_data , srs_param ) \"\"\" f = gf . Digraph ( ' finite_state_machine ' , filename = ' partOf.gv ' , engine = ' neato ' ) f . attr ( rankdir = ' LR ' , size = ' 8,5 ' ) f . attr ( ' node ' , shape = ' doublecircle ' ) # node for angka in source_data . ID : f . node ( angka ) f . attr ( kw = ' node ' , shape = ' circle ' ) # edge for idx , num in zip ( part_data . label , part_data . ID ) : f . edge ( idx , num , label = ' partOf ' ) f . attr ( overlap = ' false ' ) f . attr ( label = r ' Visulasisasi relasi partOf {} \\n ' . format ( srs_param )) f . attr ( fontsize = ' 20 ' ) f . view ( data_simpan ) print ( \" Gambar disimpan ke {} \" . format ( data_simpan )) return f","title":"visualisasiGraph"},{"location":"reference/extractreq/modul_spacySent/","text":"Module extractreq.modul_spacySent None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # @title Modul : spacySent { vertical - output : true } spacy_param = 'en_core_web_sm' # @param { type : \"string\" } dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" # @param { type : \"string\" } srs_param = \"2005 - Grid 3D\" # @param [ \"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\" ] # dataFile = \"/content/drive/MyDrive/dataset/visualPartOf/partOf2005 - Grid 3D.xlsx\" # @param { type : \"string\" } # srs_param = \"tabel_partOf\" # @param { type : \"string\" } col_param = \"Requirement Statement\" # @param [ \"Requirement Statement\", \"req\" ] id_param = \"ID\" # @param [ \"ID\" ] import pandas as pd , spacy from tabulate import tabulate class spacyClause : def __init__ ( self , fileName = dataFile ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def find_root_of_sentence ( self , doc ) : root_token = None for token in doc : if ( token . dep_ == \"ROOT\" ) : root_token = token return root_token def find_other_verbs ( self , doc , root_token ) : other_verbs = [] for token in doc : ancestors = list ( token . ancestors ) if ( token . pos_ == \"VERB\" and len ( ancestors ) == 1 \\ and ancestors [ 0 ] == root_token ) : other_verbs . append ( token ) return other_verbs def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) : first_token_index = len ( doc ) last_token_index = 0 this_verb_children = list ( verb . children ) for child in this_verb_children : if ( child not in all_verbs ) : if ( child . i < first_token_index ) : first_token_index = child . i if ( child . i > last_token_index ) : last_token_index = child . i return first_token_index , last_token_index def extractData ( self , doc ) : root_token = spacyClause . find_root_of_sentence ( self , doc ) other_verbs = spacyClause . find_other_verbs ( self , doc , root_token ) all_verbs = [ root_token ] + other_verbs token_spans = [ spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs ] sentence_clauses = [ doc[token_span[0 ] : token_span [ 1 ] ] for token_span in token_spans if ( token_span [ 0 ] < token_span [ 1 ] ) ] sentence_clauses = sorted ( sentence_clauses , key = lambda tup : tup [ 0 ] ) clauses_text = [ clause.text for clause in sentence_clauses ] return clauses_text def main ( self , srs_param , id_param , col_param ) : id_req = spacyClause . fulldataset ( self , srs_param ) [ id_param ] req = spacyClause . fulldataset ( self , srs_param ) [ col_param ] dataSpacy = [] nlp = spacy . load ( spacy_param ) for id , num in zip ( id_req , req ) : doc = nlp ( num ) myClause = spacyClause . extractData ( self , doc ) jml_clausa = len ( myClause ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) dataSpacy . append ( [ id, num, myClause, label_df[0 ] , jml_clausa ] ) spacy_df = pd . DataFrame ( dataSpacy , columns = [ 'ID', 'req', 'data', 'label', 'jumlah' ] ) return spacy_df if __name__ == \"__main__\" : try : dataSpacy = spacyClause ( dataFile ). main ( srs_param , id_param , col_param ) print ( tabulate ( dataSpacy , headers = 'keys' , tablefmt = 'psql' )) except OSError as err : print ( \"OS error: {0}\" . format ( err )) Variables col_param dataFile id_param spacy_param srs_param Classes spacyClause class spacyClause ( fileName = '/content/drive/MyDrive/dataset/dataset_2.xlsx' ) View Source class spacyClause : def __init__ ( self , fileName = dataFile ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def find_root_of_sentence ( self , doc ) : root_token = None for token in doc : if ( token . dep_ == \"ROOT\" ) : root_token = token return root_token def find_other_verbs ( self , doc , root_token ) : other_verbs = [] for token in doc : ancestors = list ( token . ancestors ) if ( token . pos_ == \"VERB\" and len ( ancestors ) == 1 \\ and ancestors [ 0 ] == root_token ) : other_verbs . append ( token ) return other_verbs def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) : first_token_index = len ( doc ) last_token_index = 0 this_verb_children = list ( verb . children ) for child in this_verb_children : if ( child not in all_verbs ) : if ( child . i < first_token_index ) : first_token_index = child . i if ( child . i > last_token_index ) : last_token_index = child . i return first_token_index , last_token_index def extractData ( self , doc ) : root_token = spacyClause . find_root_of_sentence ( self , doc ) other_verbs = spacyClause . find_other_verbs ( self , doc , root_token ) all_verbs = [ root_token ] + other_verbs token_spans = [ spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs ] sentence_clauses = [ doc[token_span[0 ] : token_span [ 1 ] ] for token_span in token_spans if ( token_span [ 0 ] < token_span [ 1 ] ) ] sentence_clauses = sorted ( sentence_clauses , key = lambda tup : tup [ 0 ] ) clauses_text = [ clause.text for clause in sentence_clauses ] return clauses_text def main ( self , srs_param , id_param , col_param ) : id_req = spacyClause . fulldataset ( self , srs_param ) [ id_param ] req = spacyClause . fulldataset ( self , srs_param ) [ col_param ] dataSpacy = [] nlp = spacy . load ( spacy_param ) for id , num in zip ( id_req , req ) : doc = nlp ( num ) myClause = spacyClause . extractData ( self , doc ) jml_clausa = len ( myClause ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) dataSpacy . append ( [ id, num, myClause, label_df[0 ] , jml_clausa ] ) spacy_df = pd . DataFrame ( dataSpacy , columns = [ 'ID', 'req', 'data', 'label', 'jumlah' ] ) return spacy_df Methods extractData def extractData ( self , doc ) View Source def extractData ( self , doc ) : root_token = spacyClause . find_root_of_sentence ( self , doc ) other_verbs = spacyClause . find_other_verbs ( self , doc , root_token ) all_verbs = [ root_token ] + other_verbs token_spans = [ spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs ] sentence_clauses = [ doc[token_span[0 ] : token_span [ 1 ] ] for token_span in token_spans if ( token_span [ 0 ] < token_span [ 1 ] ) ] sentence_clauses = sorted ( sentence_clauses , key = lambda tup : tup [ 0 ] ) clauses_text = [ clause.text for clause in sentence_clauses ] return clauses_text find_other_verbs def find_other_verbs ( self , doc , root_token ) View Source def find_other_verbs ( self , doc , root_token ) : other_verbs = [] for token in doc : ancestors = list ( token . ancestors ) if ( token . pos_ == \" VERB \" and len ( ancestors ) == 1 \\ and ancestors [ 0 ] == root_token ) : other_verbs . append ( token ) return other_verbs find_root_of_sentence def find_root_of_sentence ( self , doc ) View Source def find_root_of_sentence ( self , doc ) : root_token = None for token in doc : if ( token . dep_ == \" ROOT \" ) : root_token = token return root_token fulldataset def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs get_clause_token_span_for_verb def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) View Source def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) : first_token_index = len ( doc ) last_token_index = 0 this_verb_children = list ( verb . children ) for child in this_verb_children : if ( child not in all_verbs ) : if ( child . i < first_token_index ) : first_token_index = child . i if ( child . i > last_token_index ) : last_token_index = child . i return first_token_index , last_token_index main def main ( self , srs_param , id_param , col_param ) View Source def main ( self , srs_param , id_param , col_param ) : id_req = spacyClause . fulldataset ( self , srs_param ) [ id_param ] req = spacyClause . fulldataset ( self , srs_param ) [ col_param ] dataSpacy = [] nlp = spacy . load ( spacy_param ) for id , num in zip ( id_req , req ) : doc = nlp ( num ) myClause = spacyClause . extractData ( self , doc ) jml_clausa = len ( myClause ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) dataSpacy . append ( [ id, num, myClause, label_df[0 ] , jml_clausa ] ) spacy_df = pd . DataFrame ( dataSpacy , columns = [ 'ID', 'req', 'data', 'label', 'jumlah' ] ) return spacy_df preprocessing def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"Modul Spacysent"},{"location":"reference/extractreq/modul_spacySent/#module-extractreqmodul_spacysent","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # @title Modul : spacySent { vertical - output : true } spacy_param = 'en_core_web_sm' # @param { type : \"string\" } dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" # @param { type : \"string\" } srs_param = \"2005 - Grid 3D\" # @param [ \"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\" ] # dataFile = \"/content/drive/MyDrive/dataset/visualPartOf/partOf2005 - Grid 3D.xlsx\" # @param { type : \"string\" } # srs_param = \"tabel_partOf\" # @param { type : \"string\" } col_param = \"Requirement Statement\" # @param [ \"Requirement Statement\", \"req\" ] id_param = \"ID\" # @param [ \"ID\" ] import pandas as pd , spacy from tabulate import tabulate class spacyClause : def __init__ ( self , fileName = dataFile ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def find_root_of_sentence ( self , doc ) : root_token = None for token in doc : if ( token . dep_ == \"ROOT\" ) : root_token = token return root_token def find_other_verbs ( self , doc , root_token ) : other_verbs = [] for token in doc : ancestors = list ( token . ancestors ) if ( token . pos_ == \"VERB\" and len ( ancestors ) == 1 \\ and ancestors [ 0 ] == root_token ) : other_verbs . append ( token ) return other_verbs def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) : first_token_index = len ( doc ) last_token_index = 0 this_verb_children = list ( verb . children ) for child in this_verb_children : if ( child not in all_verbs ) : if ( child . i < first_token_index ) : first_token_index = child . i if ( child . i > last_token_index ) : last_token_index = child . i return first_token_index , last_token_index def extractData ( self , doc ) : root_token = spacyClause . find_root_of_sentence ( self , doc ) other_verbs = spacyClause . find_other_verbs ( self , doc , root_token ) all_verbs = [ root_token ] + other_verbs token_spans = [ spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs ] sentence_clauses = [ doc[token_span[0 ] : token_span [ 1 ] ] for token_span in token_spans if ( token_span [ 0 ] < token_span [ 1 ] ) ] sentence_clauses = sorted ( sentence_clauses , key = lambda tup : tup [ 0 ] ) clauses_text = [ clause.text for clause in sentence_clauses ] return clauses_text def main ( self , srs_param , id_param , col_param ) : id_req = spacyClause . fulldataset ( self , srs_param ) [ id_param ] req = spacyClause . fulldataset ( self , srs_param ) [ col_param ] dataSpacy = [] nlp = spacy . load ( spacy_param ) for id , num in zip ( id_req , req ) : doc = nlp ( num ) myClause = spacyClause . extractData ( self , doc ) jml_clausa = len ( myClause ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) dataSpacy . append ( [ id, num, myClause, label_df[0 ] , jml_clausa ] ) spacy_df = pd . DataFrame ( dataSpacy , columns = [ 'ID', 'req', 'data', 'label', 'jumlah' ] ) return spacy_df if __name__ == \"__main__\" : try : dataSpacy = spacyClause ( dataFile ). main ( srs_param , id_param , col_param ) print ( tabulate ( dataSpacy , headers = 'keys' , tablefmt = 'psql' )) except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"Module extractreq.modul_spacySent"},{"location":"reference/extractreq/modul_spacySent/#variables","text":"col_param dataFile id_param spacy_param srs_param","title":"Variables"},{"location":"reference/extractreq/modul_spacySent/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/modul_spacySent/#spacyclause","text":"class spacyClause ( fileName = '/content/drive/MyDrive/dataset/dataset_2.xlsx' ) View Source class spacyClause : def __init__ ( self , fileName = dataFile ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def find_root_of_sentence ( self , doc ) : root_token = None for token in doc : if ( token . dep_ == \"ROOT\" ) : root_token = token return root_token def find_other_verbs ( self , doc , root_token ) : other_verbs = [] for token in doc : ancestors = list ( token . ancestors ) if ( token . pos_ == \"VERB\" and len ( ancestors ) == 1 \\ and ancestors [ 0 ] == root_token ) : other_verbs . append ( token ) return other_verbs def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) : first_token_index = len ( doc ) last_token_index = 0 this_verb_children = list ( verb . children ) for child in this_verb_children : if ( child not in all_verbs ) : if ( child . i < first_token_index ) : first_token_index = child . i if ( child . i > last_token_index ) : last_token_index = child . i return first_token_index , last_token_index def extractData ( self , doc ) : root_token = spacyClause . find_root_of_sentence ( self , doc ) other_verbs = spacyClause . find_other_verbs ( self , doc , root_token ) all_verbs = [ root_token ] + other_verbs token_spans = [ spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs ] sentence_clauses = [ doc[token_span[0 ] : token_span [ 1 ] ] for token_span in token_spans if ( token_span [ 0 ] < token_span [ 1 ] ) ] sentence_clauses = sorted ( sentence_clauses , key = lambda tup : tup [ 0 ] ) clauses_text = [ clause.text for clause in sentence_clauses ] return clauses_text def main ( self , srs_param , id_param , col_param ) : id_req = spacyClause . fulldataset ( self , srs_param ) [ id_param ] req = spacyClause . fulldataset ( self , srs_param ) [ col_param ] dataSpacy = [] nlp = spacy . load ( spacy_param ) for id , num in zip ( id_req , req ) : doc = nlp ( num ) myClause = spacyClause . extractData ( self , doc ) jml_clausa = len ( myClause ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) dataSpacy . append ( [ id, num, myClause, label_df[0 ] , jml_clausa ] ) spacy_df = pd . DataFrame ( dataSpacy , columns = [ 'ID', 'req', 'data', 'label', 'jumlah' ] ) return spacy_df","title":"spacyClause"},{"location":"reference/extractreq/modul_spacySent/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/modul_spacySent/#extractdata","text":"def extractData ( self , doc ) View Source def extractData ( self , doc ) : root_token = spacyClause . find_root_of_sentence ( self , doc ) other_verbs = spacyClause . find_other_verbs ( self , doc , root_token ) all_verbs = [ root_token ] + other_verbs token_spans = [ spacyClause.get_clause_token_span_for_verb(self, other_verb, doc, all_verbs) for other_verb in all_verbs ] sentence_clauses = [ doc[token_span[0 ] : token_span [ 1 ] ] for token_span in token_spans if ( token_span [ 0 ] < token_span [ 1 ] ) ] sentence_clauses = sorted ( sentence_clauses , key = lambda tup : tup [ 0 ] ) clauses_text = [ clause.text for clause in sentence_clauses ] return clauses_text","title":"extractData"},{"location":"reference/extractreq/modul_spacySent/#find_other_verbs","text":"def find_other_verbs ( self , doc , root_token ) View Source def find_other_verbs ( self , doc , root_token ) : other_verbs = [] for token in doc : ancestors = list ( token . ancestors ) if ( token . pos_ == \" VERB \" and len ( ancestors ) == 1 \\ and ancestors [ 0 ] == root_token ) : other_verbs . append ( token ) return other_verbs","title":"find_other_verbs"},{"location":"reference/extractreq/modul_spacySent/#find_root_of_sentence","text":"def find_root_of_sentence ( self , doc ) View Source def find_root_of_sentence ( self , doc ) : root_token = None for token in doc : if ( token . dep_ == \" ROOT \" ) : root_token = token return root_token","title":"find_root_of_sentence"},{"location":"reference/extractreq/modul_spacySent/#fulldataset","text":"def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs","title":"fulldataset"},{"location":"reference/extractreq/modul_spacySent/#get_clause_token_span_for_verb","text":"def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) View Source def get_clause_token_span_for_verb ( self , verb , doc , all_verbs ) : first_token_index = len ( doc ) last_token_index = 0 this_verb_children = list ( verb . children ) for child in this_verb_children : if ( child not in all_verbs ) : if ( child . i < first_token_index ) : first_token_index = child . i if ( child . i > last_token_index ) : last_token_index = child . i return first_token_index , last_token_index","title":"get_clause_token_span_for_verb"},{"location":"reference/extractreq/modul_spacySent/#main","text":"def main ( self , srs_param , id_param , col_param ) View Source def main ( self , srs_param , id_param , col_param ) : id_req = spacyClause . fulldataset ( self , srs_param ) [ id_param ] req = spacyClause . fulldataset ( self , srs_param ) [ col_param ] dataSpacy = [] nlp = spacy . load ( spacy_param ) for id , num in zip ( id_req , req ) : doc = nlp ( num ) myClause = spacyClause . extractData ( self , doc ) jml_clausa = len ( myClause ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) dataSpacy . append ( [ id, num, myClause, label_df[0 ] , jml_clausa ] ) spacy_df = pd . DataFrame ( dataSpacy , columns = [ 'ID', 'req', 'data', 'label', 'jumlah' ] ) return spacy_df","title":"main"},{"location":"reference/extractreq/modul_spacySent/#preprocessing","text":"def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/extractreq/modul_stanfordSent/","text":"Module extractreq.modul_stanfordSent None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # @title Modul : Stanford Clause { vertical - output : true } url_param = \"http://corenlp.run\" # @param { type : \"string\" } model_param = \"/content/drive/MyDrive/stanford-corenlp-4.0.0\" # @param { type : \"string\" } dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" # @param { type : \"string\" } srs_param = \"2005 - Grid 3D\" # @param [ \"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\" ] col_param = \"Requirement Statement\" # @param [ \"Requirement Statement\", \"req\" ] id_param = \"ID\" # @param [ \"ID\" ] import re , nltk , json , pandas as pd # from pycorenlp import StanfordCoreNLP from stanfordcorenlp import StanfordCoreNLP from tabulate import tabulate class stanford_clause : def __init__ ( self , fileName = dataFile , url_stanford = url_param , model_stanford = model_param ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" # self . nlp = StanfordCoreNLP ( url_stanford ) # pycoren ; lp self . nlp = StanfordCoreNLP ( url_stanford , port = 80 ) #stanfordcorenlp # self . nlp = StanfordCoreNLP ( model_stanford ) #stanfordcorenlp self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def get_verb_phrases ( self , t ) : verb_phrases = [] num_children = len ( t ) num_VP = sum ( 1 if t [ i ] . label () == \"VP\" else 0 for i in range ( 0 , num_children )) if t . label () != \"VP\" : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) elif t . label () == \"VP\" and num_VP > 1 : for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) else : verb_phrases . append ( ' ' . join ( t . leaves ())) return verb_phrases def get_pos ( self , t ) : vp_pos = [] sub_conj_pos = [] num_children = len ( t ) children = [ t[i ] . label () for i in range ( 0 , num_children ) ] flag = re . search ( r \"(S|SBAR|SBARQ|SINV|SQ)\" , ' ' . join ( children )) if \"VP\" in children and not flag : # print ( t [ i ] . label ()) for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : vp_pos . append ( t [ i ] . treeposition ()) elif not \"VP\" in children and not flag : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : for i in range ( 0 , num_children ) : if t [ i ] . label () in [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : sub_conj_pos . append ( t [ i ] . treeposition ()) return ( vp_pos , sub_conj_pos ) def get_clause_list ( self , sent ) : parser = self . nlp . annotate ( sent , properties = { \"annotators\" : \"parse\" , \"outputFormat\" : \"json\" } ) # sent_tree = nltk . tree . ParentedTree . fromstring ( parser [ \"sentences\" ][ 0 ][ \"parse\" ] ) parser_json = json . loads ( parser ) sent_tree = nltk . tree . ParentedTree . fromstring ( parser_json [ \"sentences\" ][ 0 ][ \"parse\" ] ) clause_level_list = [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] clause_list = [] sub_trees = [] # break the tree into subtrees of clauses using # clause levels \"S\" , \"SBAR\" , \"SBARQ\" , \"SINV\" , \"SQ\" for sub_tree in reversed ( list ( sent_tree . subtrees ())) : # print ( sub_tree . label () == 'CC' ) if sub_tree . label () in clause_level_list : if sub_tree . parent (). label () in clause_level_list : continue if ( len ( sub_tree ) == 1 and sub_tree . label () == \"S\" and sub_tree [ 0 ] . label () == \"VP\" and not sub_tree . parent (). label () in clause_level_list ) : continue sub_trees . append ( sub_tree ) del sent_tree [ sub_tree.treeposition() ] for t in sub_trees : # for each clause level subtree , extract relevant simple sentence verb_phrases = stanford_clause . get_verb_phrases ( self , t ) # get verb phrases from the new modified tree vp_pos , sub_conj_pos = stanford_clause . get_pos ( self , t ) for i in vp_pos : del t [ i ] for i in sub_conj_pos : del t [ i ] subject_phrase = ' ' . join ( t . leaves ()) for i in verb_phrases : # update the clause_list clause_list . append ( subject_phrase + \" \" + i ) return clause_list def __del__ ( self ) : pass def main ( self , srs_param , id_param , col_param ) : id_req = stanford_clause . fulldataset ( self , srs_param ) [ id_param ] req = stanford_clause . fulldataset ( self , srs_param ) [ col_param ] data_clausa = [] for id , num in zip ( id_req , req ) : sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , num ) clause_list = [ idx for idx in stanford_clause.get_clause_list(self, sent) ] jml_clausa = len ( clause_list ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) data_clausa . append ( [ id, num, clause_list, label_df[0 ] , jml_clausa ] ) clausa_df = pd . DataFrame ( data_clausa , columns = [ 'id', 'req', 'data', 'label', 'jumlah' ] ) stanford_clause . __del__ ( self ) return clausa_df if __name__ == \"__main__\" : try : klausa = stanford_clause ( dataFile ). main ( srs_param , id_param , col_param ) print ( tabulate ( klausa , headers = 'keys' , tablefmt = 'psql' )) # sent = 'Joe waited for the train, but the train was late.' # sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , sent ) # clause_list = stanford_clause (). get_clause_list ( sent ) # print ( clause_list ) except OSError as err : print ( \"OS error: {0}\" . format ( err )) Variables col_param dataFile id_param model_param srs_param url_param Classes stanford_clause class stanford_clause ( fileName = '/content/drive/MyDrive/dataset/dataset_2.xlsx' , url_stanford = 'http://corenlp.run' , model_stanford = '/content/drive/MyDrive/stanford-corenlp-4.0.0' ) View Source class stanford_clause : def __init__ ( self , fileName = dataFile , url_stanford = url_param , model_stanford = model_param ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" # self . nlp = StanfordCoreNLP ( url_stanford ) # pycoren ; lp self . nlp = StanfordCoreNLP ( url_stanford , port = 80 ) #stanfordcorenlp # self . nlp = StanfordCoreNLP ( model_stanford ) #stanfordcorenlp self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def get_verb_phrases ( self , t ) : verb_phrases = [] num_children = len ( t ) num_VP = sum ( 1 if t [ i ] . label () == \"VP\" else 0 for i in range ( 0 , num_children )) if t . label () != \"VP\" : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) elif t . label () == \"VP\" and num_VP > 1 : for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) else : verb_phrases . append ( ' ' . join ( t . leaves ())) return verb_phrases def get_pos ( self , t ) : vp_pos = [] sub_conj_pos = [] num_children = len ( t ) children = [ t[i ] . label () for i in range ( 0 , num_children ) ] flag = re . search ( r \"(S|SBAR|SBARQ|SINV|SQ)\" , ' ' . join ( children )) if \"VP\" in children and not flag : # print ( t [ i ] . label ()) for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : vp_pos . append ( t [ i ] . treeposition ()) elif not \"VP\" in children and not flag : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : for i in range ( 0 , num_children ) : if t [ i ] . label () in [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : sub_conj_pos . append ( t [ i ] . treeposition ()) return ( vp_pos , sub_conj_pos ) def get_clause_list ( self , sent ) : parser = self . nlp . annotate ( sent , properties = { \"annotators\" : \"parse\" , \"outputFormat\" : \"json\" } ) # sent_tree = nltk . tree . ParentedTree . fromstring ( parser [ \"sentences\" ][ 0 ][ \"parse\" ] ) parser_json = json . loads ( parser ) sent_tree = nltk . tree . ParentedTree . fromstring ( parser_json [ \"sentences\" ][ 0 ][ \"parse\" ] ) clause_level_list = [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] clause_list = [] sub_trees = [] # break the tree into subtrees of clauses using # clause levels \"S\" , \"SBAR\" , \"SBARQ\" , \"SINV\" , \"SQ\" for sub_tree in reversed ( list ( sent_tree . subtrees ())) : # print ( sub_tree . label () == 'CC' ) if sub_tree . label () in clause_level_list : if sub_tree . parent (). label () in clause_level_list : continue if ( len ( sub_tree ) == 1 and sub_tree . label () == \"S\" and sub_tree [ 0 ] . label () == \"VP\" and not sub_tree . parent (). label () in clause_level_list ) : continue sub_trees . append ( sub_tree ) del sent_tree [ sub_tree.treeposition() ] for t in sub_trees : # for each clause level subtree , extract relevant simple sentence verb_phrases = stanford_clause . get_verb_phrases ( self , t ) # get verb phrases from the new modified tree vp_pos , sub_conj_pos = stanford_clause . get_pos ( self , t ) for i in vp_pos : del t [ i ] for i in sub_conj_pos : del t [ i ] subject_phrase = ' ' . join ( t . leaves ()) for i in verb_phrases : # update the clause_list clause_list . append ( subject_phrase + \" \" + i ) return clause_list def __del__ ( self ) : pass def main ( self , srs_param , id_param , col_param ) : id_req = stanford_clause . fulldataset ( self , srs_param ) [ id_param ] req = stanford_clause . fulldataset ( self , srs_param ) [ col_param ] data_clausa = [] for id , num in zip ( id_req , req ) : sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , num ) clause_list = [ idx for idx in stanford_clause.get_clause_list(self, sent) ] jml_clausa = len ( clause_list ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) data_clausa . append ( [ id, num, clause_list, label_df[0 ] , jml_clausa ] ) clausa_df = pd . DataFrame ( data_clausa , columns = [ 'id', 'req', 'data', 'label', 'jumlah' ] ) stanford_clause . __del__ ( self ) return clausa_df Methods fulldataset def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs get_clause_list def get_clause_list ( self , sent ) View Source def get_clause_list ( self , sent ) : parser = self . nlp . annotate ( sent , properties = { \"annotators\" : \"parse\" , \"outputFormat\" : \"json\" } ) # sent_tree = nltk . tree . ParentedTree . fromstring ( parser [ \"sentences\" ][ 0 ][ \"parse\" ] ) parser_json = json . loads ( parser ) sent_tree = nltk . tree . ParentedTree . fromstring ( parser_json [ \"sentences\" ][ 0 ][ \"parse\" ] ) clause_level_list = [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] clause_list = [] sub_trees = [] # break the tree into subtrees of clauses using # clause levels \"S\" , \"SBAR\" , \"SBARQ\" , \"SINV\" , \"SQ\" for sub_tree in reversed ( list ( sent_tree . subtrees ())) : # print ( sub_tree . label () == 'CC' ) if sub_tree . label () in clause_level_list : if sub_tree . parent (). label () in clause_level_list : continue if ( len ( sub_tree ) == 1 and sub_tree . label () == \"S\" and sub_tree [ 0 ] . label () == \"VP\" and not sub_tree . parent (). label () in clause_level_list ) : continue sub_trees . append ( sub_tree ) del sent_tree [ sub_tree.treeposition() ] for t in sub_trees : # for each clause level subtree , extract relevant simple sentence verb_phrases = stanford_clause . get_verb_phrases ( self , t ) # get verb phrases from the new modified tree vp_pos , sub_conj_pos = stanford_clause . get_pos ( self , t ) for i in vp_pos : del t [ i ] for i in sub_conj_pos : del t [ i ] subject_phrase = ' ' . join ( t . leaves ()) for i in verb_phrases : # update the clause_list clause_list . append ( subject_phrase + \" \" + i ) return clause_list get_pos def get_pos ( self , t ) View Source def get_pos ( self , t ) : vp_pos = [] sub_conj_pos = [] num_children = len ( t ) children = [ t[i ] . label () for i in range ( 0 , num_children ) ] flag = re . search ( r \"(S|SBAR|SBARQ|SINV|SQ)\" , ' ' . join ( children )) if \"VP\" in children and not flag : # print ( t [ i ] . label ()) for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : vp_pos . append ( t [ i ] . treeposition ()) elif not \"VP\" in children and not flag : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : for i in range ( 0 , num_children ) : if t [ i ] . label () in [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : sub_conj_pos . append ( t [ i ] . treeposition ()) return ( vp_pos , sub_conj_pos ) get_verb_phrases def get_verb_phrases ( self , t ) View Source def get_verb_phrases ( self , t ) : verb_phrases = [] num_children = len ( t ) num_VP = sum ( 1 if t [ i ] . label () == \"VP\" else 0 for i in range ( 0 , num_children )) if t . label () != \"VP\" : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) elif t . label () == \"VP\" and num_VP > 1 : for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) else : verb_phrases . append ( ' ' . join ( t . leaves ())) return verb_phrases main def main ( self , srs_param , id_param , col_param ) View Source def main ( self , srs_param , id_param , col_param ) : id_req = stanford_clause . fulldataset ( self , srs_param ) [ id_param ] req = stanford_clause . fulldataset ( self , srs_param ) [ col_param ] data_clausa = [] for id , num in zip ( id_req , req ) : sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , num ) clause_list = [ idx for idx in stanford_clause.get_clause_list(self, sent) ] jml_clausa = len ( clause_list ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) data_clausa . append ( [ id, num, clause_list, label_df[0 ] , jml_clausa ] ) clausa_df = pd . DataFrame ( data_clausa , columns = [ 'id', 'req', 'data', 'label', 'jumlah' ] ) stanford_clause . __del__ ( self ) return clausa_df preprocessing def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"Modul Stanfordsent"},{"location":"reference/extractreq/modul_stanfordSent/#module-extractreqmodul_stanfordsent","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # @title Modul : Stanford Clause { vertical - output : true } url_param = \"http://corenlp.run\" # @param { type : \"string\" } model_param = \"/content/drive/MyDrive/stanford-corenlp-4.0.0\" # @param { type : \"string\" } dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" # @param { type : \"string\" } srs_param = \"2005 - Grid 3D\" # @param [ \"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\" ] col_param = \"Requirement Statement\" # @param [ \"Requirement Statement\", \"req\" ] id_param = \"ID\" # @param [ \"ID\" ] import re , nltk , json , pandas as pd # from pycorenlp import StanfordCoreNLP from stanfordcorenlp import StanfordCoreNLP from tabulate import tabulate class stanford_clause : def __init__ ( self , fileName = dataFile , url_stanford = url_param , model_stanford = model_param ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" # self . nlp = StanfordCoreNLP ( url_stanford ) # pycoren ; lp self . nlp = StanfordCoreNLP ( url_stanford , port = 80 ) #stanfordcorenlp # self . nlp = StanfordCoreNLP ( model_stanford ) #stanfordcorenlp self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def get_verb_phrases ( self , t ) : verb_phrases = [] num_children = len ( t ) num_VP = sum ( 1 if t [ i ] . label () == \"VP\" else 0 for i in range ( 0 , num_children )) if t . label () != \"VP\" : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) elif t . label () == \"VP\" and num_VP > 1 : for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) else : verb_phrases . append ( ' ' . join ( t . leaves ())) return verb_phrases def get_pos ( self , t ) : vp_pos = [] sub_conj_pos = [] num_children = len ( t ) children = [ t[i ] . label () for i in range ( 0 , num_children ) ] flag = re . search ( r \"(S|SBAR|SBARQ|SINV|SQ)\" , ' ' . join ( children )) if \"VP\" in children and not flag : # print ( t [ i ] . label ()) for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : vp_pos . append ( t [ i ] . treeposition ()) elif not \"VP\" in children and not flag : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : for i in range ( 0 , num_children ) : if t [ i ] . label () in [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : sub_conj_pos . append ( t [ i ] . treeposition ()) return ( vp_pos , sub_conj_pos ) def get_clause_list ( self , sent ) : parser = self . nlp . annotate ( sent , properties = { \"annotators\" : \"parse\" , \"outputFormat\" : \"json\" } ) # sent_tree = nltk . tree . ParentedTree . fromstring ( parser [ \"sentences\" ][ 0 ][ \"parse\" ] ) parser_json = json . loads ( parser ) sent_tree = nltk . tree . ParentedTree . fromstring ( parser_json [ \"sentences\" ][ 0 ][ \"parse\" ] ) clause_level_list = [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] clause_list = [] sub_trees = [] # break the tree into subtrees of clauses using # clause levels \"S\" , \"SBAR\" , \"SBARQ\" , \"SINV\" , \"SQ\" for sub_tree in reversed ( list ( sent_tree . subtrees ())) : # print ( sub_tree . label () == 'CC' ) if sub_tree . label () in clause_level_list : if sub_tree . parent (). label () in clause_level_list : continue if ( len ( sub_tree ) == 1 and sub_tree . label () == \"S\" and sub_tree [ 0 ] . label () == \"VP\" and not sub_tree . parent (). label () in clause_level_list ) : continue sub_trees . append ( sub_tree ) del sent_tree [ sub_tree.treeposition() ] for t in sub_trees : # for each clause level subtree , extract relevant simple sentence verb_phrases = stanford_clause . get_verb_phrases ( self , t ) # get verb phrases from the new modified tree vp_pos , sub_conj_pos = stanford_clause . get_pos ( self , t ) for i in vp_pos : del t [ i ] for i in sub_conj_pos : del t [ i ] subject_phrase = ' ' . join ( t . leaves ()) for i in verb_phrases : # update the clause_list clause_list . append ( subject_phrase + \" \" + i ) return clause_list def __del__ ( self ) : pass def main ( self , srs_param , id_param , col_param ) : id_req = stanford_clause . fulldataset ( self , srs_param ) [ id_param ] req = stanford_clause . fulldataset ( self , srs_param ) [ col_param ] data_clausa = [] for id , num in zip ( id_req , req ) : sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , num ) clause_list = [ idx for idx in stanford_clause.get_clause_list(self, sent) ] jml_clausa = len ( clause_list ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) data_clausa . append ( [ id, num, clause_list, label_df[0 ] , jml_clausa ] ) clausa_df = pd . DataFrame ( data_clausa , columns = [ 'id', 'req', 'data', 'label', 'jumlah' ] ) stanford_clause . __del__ ( self ) return clausa_df if __name__ == \"__main__\" : try : klausa = stanford_clause ( dataFile ). main ( srs_param , id_param , col_param ) print ( tabulate ( klausa , headers = 'keys' , tablefmt = 'psql' )) # sent = 'Joe waited for the train, but the train was late.' # sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , sent ) # clause_list = stanford_clause (). get_clause_list ( sent ) # print ( clause_list ) except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"Module extractreq.modul_stanfordSent"},{"location":"reference/extractreq/modul_stanfordSent/#variables","text":"col_param dataFile id_param model_param srs_param url_param","title":"Variables"},{"location":"reference/extractreq/modul_stanfordSent/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/modul_stanfordSent/#stanford_clause","text":"class stanford_clause ( fileName = '/content/drive/MyDrive/dataset/dataset_2.xlsx' , url_stanford = 'http://corenlp.run' , model_stanford = '/content/drive/MyDrive/stanford-corenlp-4.0.0' ) View Source class stanford_clause : def __init__ ( self , fileName = dataFile , url_stanford = url_param , model_stanford = model_param ) : \"\"\" parameter inisialisasi, data yang digunakan pertama kali untuk contruct data \"\"\" # self . nlp = StanfordCoreNLP ( url_stanford ) # pycoren ; lp self . nlp = StanfordCoreNLP ( url_stanford , port = 80 ) #stanfordcorenlp # self . nlp = StanfordCoreNLP ( model_stanford ) #stanfordcorenlp self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def get_verb_phrases ( self , t ) : verb_phrases = [] num_children = len ( t ) num_VP = sum ( 1 if t [ i ] . label () == \"VP\" else 0 for i in range ( 0 , num_children )) if t . label () != \"VP\" : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) elif t . label () == \"VP\" and num_VP > 1 : for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) else : verb_phrases . append ( ' ' . join ( t . leaves ())) return verb_phrases def get_pos ( self , t ) : vp_pos = [] sub_conj_pos = [] num_children = len ( t ) children = [ t[i ] . label () for i in range ( 0 , num_children ) ] flag = re . search ( r \"(S|SBAR|SBARQ|SINV|SQ)\" , ' ' . join ( children )) if \"VP\" in children and not flag : # print ( t [ i ] . label ()) for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : vp_pos . append ( t [ i ] . treeposition ()) elif not \"VP\" in children and not flag : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : for i in range ( 0 , num_children ) : if t [ i ] . label () in [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : sub_conj_pos . append ( t [ i ] . treeposition ()) return ( vp_pos , sub_conj_pos ) def get_clause_list ( self , sent ) : parser = self . nlp . annotate ( sent , properties = { \"annotators\" : \"parse\" , \"outputFormat\" : \"json\" } ) # sent_tree = nltk . tree . ParentedTree . fromstring ( parser [ \"sentences\" ][ 0 ][ \"parse\" ] ) parser_json = json . loads ( parser ) sent_tree = nltk . tree . ParentedTree . fromstring ( parser_json [ \"sentences\" ][ 0 ][ \"parse\" ] ) clause_level_list = [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] clause_list = [] sub_trees = [] # break the tree into subtrees of clauses using # clause levels \"S\" , \"SBAR\" , \"SBARQ\" , \"SINV\" , \"SQ\" for sub_tree in reversed ( list ( sent_tree . subtrees ())) : # print ( sub_tree . label () == 'CC' ) if sub_tree . label () in clause_level_list : if sub_tree . parent (). label () in clause_level_list : continue if ( len ( sub_tree ) == 1 and sub_tree . label () == \"S\" and sub_tree [ 0 ] . label () == \"VP\" and not sub_tree . parent (). label () in clause_level_list ) : continue sub_trees . append ( sub_tree ) del sent_tree [ sub_tree.treeposition() ] for t in sub_trees : # for each clause level subtree , extract relevant simple sentence verb_phrases = stanford_clause . get_verb_phrases ( self , t ) # get verb phrases from the new modified tree vp_pos , sub_conj_pos = stanford_clause . get_pos ( self , t ) for i in vp_pos : del t [ i ] for i in sub_conj_pos : del t [ i ] subject_phrase = ' ' . join ( t . leaves ()) for i in verb_phrases : # update the clause_list clause_list . append ( subject_phrase + \" \" + i ) return clause_list def __del__ ( self ) : pass def main ( self , srs_param , id_param , col_param ) : id_req = stanford_clause . fulldataset ( self , srs_param ) [ id_param ] req = stanford_clause . fulldataset ( self , srs_param ) [ col_param ] data_clausa = [] for id , num in zip ( id_req , req ) : sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , num ) clause_list = [ idx for idx in stanford_clause.get_clause_list(self, sent) ] jml_clausa = len ( clause_list ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) data_clausa . append ( [ id, num, clause_list, label_df[0 ] , jml_clausa ] ) clausa_df = pd . DataFrame ( data_clausa , columns = [ 'id', 'req', 'data', 'label', 'jumlah' ] ) stanford_clause . __del__ ( self ) return clausa_df","title":"stanford_clause"},{"location":"reference/extractreq/modul_stanfordSent/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/modul_stanfordSent/#fulldataset","text":"def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs","title":"fulldataset"},{"location":"reference/extractreq/modul_stanfordSent/#get_clause_list","text":"def get_clause_list ( self , sent ) View Source def get_clause_list ( self , sent ) : parser = self . nlp . annotate ( sent , properties = { \"annotators\" : \"parse\" , \"outputFormat\" : \"json\" } ) # sent_tree = nltk . tree . ParentedTree . fromstring ( parser [ \"sentences\" ][ 0 ][ \"parse\" ] ) parser_json = json . loads ( parser ) sent_tree = nltk . tree . ParentedTree . fromstring ( parser_json [ \"sentences\" ][ 0 ][ \"parse\" ] ) clause_level_list = [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] clause_list = [] sub_trees = [] # break the tree into subtrees of clauses using # clause levels \"S\" , \"SBAR\" , \"SBARQ\" , \"SINV\" , \"SQ\" for sub_tree in reversed ( list ( sent_tree . subtrees ())) : # print ( sub_tree . label () == 'CC' ) if sub_tree . label () in clause_level_list : if sub_tree . parent (). label () in clause_level_list : continue if ( len ( sub_tree ) == 1 and sub_tree . label () == \"S\" and sub_tree [ 0 ] . label () == \"VP\" and not sub_tree . parent (). label () in clause_level_list ) : continue sub_trees . append ( sub_tree ) del sent_tree [ sub_tree.treeposition() ] for t in sub_trees : # for each clause level subtree , extract relevant simple sentence verb_phrases = stanford_clause . get_verb_phrases ( self , t ) # get verb phrases from the new modified tree vp_pos , sub_conj_pos = stanford_clause . get_pos ( self , t ) for i in vp_pos : del t [ i ] for i in sub_conj_pos : del t [ i ] subject_phrase = ' ' . join ( t . leaves ()) for i in verb_phrases : # update the clause_list clause_list . append ( subject_phrase + \" \" + i ) return clause_list","title":"get_clause_list"},{"location":"reference/extractreq/modul_stanfordSent/#get_pos","text":"def get_pos ( self , t ) View Source def get_pos ( self , t ) : vp_pos = [] sub_conj_pos = [] num_children = len ( t ) children = [ t[i ] . label () for i in range ( 0 , num_children ) ] flag = re . search ( r \"(S|SBAR|SBARQ|SINV|SQ)\" , ' ' . join ( children )) if \"VP\" in children and not flag : # print ( t [ i ] . label ()) for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : vp_pos . append ( t [ i ] . treeposition ()) elif not \"VP\" in children and not flag : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : for i in range ( 0 , num_children ) : if t [ i ] . label () in [ \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\" ] : temp1 , temp2 = stanford_clause . get_pos ( self , t [ i ] ) vp_pos . extend ( temp1 ) sub_conj_pos . extend ( temp2 ) else : sub_conj_pos . append ( t [ i ] . treeposition ()) return ( vp_pos , sub_conj_pos )","title":"get_pos"},{"location":"reference/extractreq/modul_stanfordSent/#get_verb_phrases","text":"def get_verb_phrases ( self , t ) View Source def get_verb_phrases ( self , t ) : verb_phrases = [] num_children = len ( t ) num_VP = sum ( 1 if t [ i ] . label () == \"VP\" else 0 for i in range ( 0 , num_children )) if t . label () != \"VP\" : for i in range ( 0 , num_children ) : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) elif t . label () == \"VP\" and num_VP > 1 : for i in range ( 0 , num_children ) : if t [ i ] . label () == \"VP\" : if t [ i ] . height () > 2 : verb_phrases . extend ( stanford_clause . get_verb_phrases ( self , t [ i ] )) else : verb_phrases . append ( ' ' . join ( t . leaves ())) return verb_phrases","title":"get_verb_phrases"},{"location":"reference/extractreq/modul_stanfordSent/#main","text":"def main ( self , srs_param , id_param , col_param ) View Source def main ( self , srs_param , id_param , col_param ) : id_req = stanford_clause . fulldataset ( self , srs_param ) [ id_param ] req = stanford_clause . fulldataset ( self , srs_param ) [ col_param ] data_clausa = [] for id , num in zip ( id_req , req ) : sent = re . sub ( r \"(\\.|,|\\?|\\(|\\)|\\[|\\])\" , \" \" , num ) clause_list = [ idx for idx in stanford_clause.get_clause_list(self, sent) ] jml_clausa = len ( clause_list ) label_df = [] if jml_clausa > 1 : # non atomik berdasarkan jumlah label_df . append ( 'non_atomik' ) elif jml_clausa == 1 : label_df . append ( 'atomik' ) else : label_df . append ( 'unknown' ) data_clausa . append ( [ id, num, clause_list, label_df[0 ] , jml_clausa ] ) clausa_df = pd . DataFrame ( data_clausa , columns = [ 'id', 'req', 'data', 'label', 'jumlah' ] ) stanford_clause . __del__ ( self ) return clausa_df","title":"main"},{"location":"reference/extractreq/modul_stanfordSent/#preprocessing","text":"def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/extractreq/modul_triplet/","text":"Module extractreq.modul_triplet None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # @title Modul2c : Triplet NLTK - Stanford Triplet Parameter { vertical - output : true } url_param = \"http://corenlp.run/\" # @param { type : \"string\" } dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" # @param { type : \"string\" } srs_param = \"2003 - Tachonet\" # @param [ \"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\" ] id_param = \"ID\" # @param [ \"ID\" ] col_param = \"Requirement Statement\" # @param [ \"Requirement Statement\", \"req\" ] mode_param = \"parse_tree\" # @param [ \"parse_tree\", \"spo\", \"result\" ] import nltk , pandas as pd , numpy as np from nltk . tag . stanford import CoreNLPPOSTagger from nltk . tree import ParentedTree from tabulate import tabulate class extractNlp : def __init__ ( self , coreUrl = url_param , fileName = dataFile ) : self . __pos_tagger = CoreNLPPOSTagger ( url = coreUrl ) self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def triplet_extraction ( self , input_sent , output =[ 'parse_tree','spo','result' ] ) : parse_tree , = ParentedTree . convert ( list ( self . __pos_tagger . parse ( input_sent . split ())) [ 0 ] ) # Extract subject , predicate and object subject = extractNlp . extract_subject ( self , parse_tree ) predicate = extractNlp . extract_predicate ( self , parse_tree ) objects = extractNlp . extract_object ( self , parse_tree ) if 'parse_tree' in output : print ( '---Parse Tree---' ) parse_tree . pretty_print () elif 'spo' in output : return ( \"subject:\\t{}\\npredicate:\\t{}\\nobject:\\t{}\" . format ( subject , predicate , objects )) elif 'result' in output : return ( ' ' . join ( [ subject[0 ] , predicate [ 0 ] , objects [ 0 ] ] )) def extract_subject ( self , parse_tree ) : # Extract the first noun found in NP_subtree subject = [] for s in parse_tree . subtrees ( lambda x : x . label () == 'NP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'NN' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] # Avoid empty or repeated values if output != [] and output not in subject : subject . append ( output ) if len ( subject ) != 0 : return subject [ 0 ] else : return [ '' ] def extract_predicate ( self , parse_tree ) : # Extract the deepest ( last ) verb foybd ub VP_subtree output , predicate = [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'VB' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] if output != [] and output not in predicate : predicate . append ( output ) if len ( predicate ) != 0 : return predicate [ -1 ] else : return [ '' ] def extract_object ( self , parse_tree ) : # Extract the first noun or first adjective in NP , PP , ADP siblings of VP_subtree objects , output , word = [] , [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label () in [ 'NP','PP','ADP' ] ) : if t . label () in [ 'NP','PP' ] : for u in t . subtrees ( lambda z : z . label (). startswith ( 'NN' )) : word = u else : for u in t . subtrees ( lambda z : z . label (). startswith ( 'JJ' )) : word = u if len ( word ) != 0 : output = [ word[0 ] , extractNlp . extract_attr ( self , word ) ] if output != [] and output not in objects : objects . append ( output ) if len ( objects ) != 0 : return objects [ 0 ] else : return [ '' ] def extract_attr ( self , word ) : attrs = [] # Search among the word 's siblings if word.label().startswith(' JJ '): for p in word.parent(): if p.label() == ' RB ': attrs.append(p[0]) elif word.label().startswith(' NN '): for p in word.parent(): if p.label() in [' DT ',' PRP $ ',' POS ',' JJ ',' CD ',' ADJP ',' QP ',' NP ']: attrs.append(p[0]) elif word.label().startswith(' VB '): for p in word.parent(): if p.label() == ' ADVP ': attrs.append(p[0]) # Search among the word' s uncles if word . label (). startswith ( 'NN' ) or word . label (). startswith ( 'JJ' ) : for p in word . parent (). parent () : if p . label () == 'PP' and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) elif word . label (). startswith ( 'VB' ) : for p in word . parent (). parent () : if p . label (). startswith ( 'VB' ) and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) return attrs def __del__ ( self ) : print ( \"destructed\" ) def main ( self , srs_param , id_param , col_param , output = mode_param ) : id_req = extractNlp . fulldataset ( self , srs_param ) [ id_param ] data_num = extractNlp . fulldataset ( self , srs_param ) [ col_param ] data_triplet = [ extractNlp.triplet_extraction(self, num, output) for num in extractNlp.fulldataset(self, srs_param)[col_param ] ] triplet_df = pd . DataFrame ( [ data_num, data_triplet ] , index = [ 'origin', 'triplet' ] , columns = id_req ). T extractNlp . __del__ ( self ) return triplet_df if __name__ == \"__main__\" : try : triplet_data = extractNlp ( dataFile ). main ( srs_param , id_param , col_param , mode_param ) print ( tabulate ( triplet_data , headers = 'keys' , tablefmt = 'psql' )) except OSError as err : print ( \"OS error: {0}\" . format ( err )) Variables col_param dataFile id_param mode_param srs_param url_param Classes extractNlp class extractNlp ( coreUrl = 'http://corenlp.run/' , fileName = '/content/drive/MyDrive/dataset/dataset_2.xlsx' ) View Source class extractNlp : def __init__ ( self , coreUrl = url_param , fileName = dataFile ) : self . __pos_tagger = CoreNLPPOSTagger ( url = coreUrl ) self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def triplet_extraction ( self , input_sent , output =[ 'parse_tree','spo','result' ] ) : parse_tree , = ParentedTree . convert ( list ( self . __pos_tagger . parse ( input_sent . split ())) [ 0 ] ) # Extract subject , predicate and object subject = extractNlp . extract_subject ( self , parse_tree ) predicate = extractNlp . extract_predicate ( self , parse_tree ) objects = extractNlp . extract_object ( self , parse_tree ) if 'parse_tree' in output : print ( '---Parse Tree---' ) parse_tree . pretty_print () elif 'spo' in output : return ( \"subject:\\t{}\\npredicate:\\t{}\\nobject:\\t{}\" . format ( subject , predicate , objects )) elif 'result' in output : return ( ' ' . join ( [ subject[0 ] , predicate [ 0 ] , objects [ 0 ] ] )) def extract_subject ( self , parse_tree ) : # Extract the first noun found in NP_subtree subject = [] for s in parse_tree . subtrees ( lambda x : x . label () == 'NP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'NN' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] # Avoid empty or repeated values if output != [] and output not in subject : subject . append ( output ) if len ( subject ) != 0 : return subject [ 0 ] else : return [ '' ] def extract_predicate ( self , parse_tree ) : # Extract the deepest ( last ) verb foybd ub VP_subtree output , predicate = [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'VB' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] if output != [] and output not in predicate : predicate . append ( output ) if len ( predicate ) != 0 : return predicate [ -1 ] else : return [ '' ] def extract_object ( self , parse_tree ) : # Extract the first noun or first adjective in NP , PP , ADP siblings of VP_subtree objects , output , word = [] , [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label () in [ 'NP','PP','ADP' ] ) : if t . label () in [ 'NP','PP' ] : for u in t . subtrees ( lambda z : z . label (). startswith ( 'NN' )) : word = u else : for u in t . subtrees ( lambda z : z . label (). startswith ( 'JJ' )) : word = u if len ( word ) != 0 : output = [ word[0 ] , extractNlp . extract_attr ( self , word ) ] if output != [] and output not in objects : objects . append ( output ) if len ( objects ) != 0 : return objects [ 0 ] else : return [ '' ] def extract_attr ( self , word ) : attrs = [] # Search among the word 's siblings if word.label().startswith(' JJ '): for p in word.parent(): if p.label() == ' RB ': attrs.append(p[0]) elif word.label().startswith(' NN '): for p in word.parent(): if p.label() in [' DT ',' PRP $ ',' POS ',' JJ ',' CD ',' ADJP ',' QP ',' NP ']: attrs.append(p[0]) elif word.label().startswith(' VB '): for p in word.parent(): if p.label() == ' ADVP ': attrs.append(p[0]) # Search among the word' s uncles if word . label (). startswith ( 'NN' ) or word . label (). startswith ( 'JJ' ) : for p in word . parent (). parent () : if p . label () == 'PP' and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) elif word . label (). startswith ( 'VB' ) : for p in word . parent (). parent () : if p . label (). startswith ( 'VB' ) and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) return attrs def __del__ ( self ) : print ( \"destructed\" ) def main ( self , srs_param , id_param , col_param , output = mode_param ) : id_req = extractNlp . fulldataset ( self , srs_param ) [ id_param ] data_num = extractNlp . fulldataset ( self , srs_param ) [ col_param ] data_triplet = [ extractNlp.triplet_extraction(self, num, output) for num in extractNlp.fulldataset(self, srs_param)[col_param ] ] triplet_df = pd . DataFrame ( [ data_num, data_triplet ] , index = [ 'origin', 'triplet' ] , columns = id_req ). T extractNlp . __del__ ( self ) return triplet_df Methods extract_attr def extract_attr ( self , word ) View Source def extract_attr ( self , word ) : attrs = [] # Search among the word ' s siblings if word . label () . startswith ( ' JJ ' ) : for p in word . parent () : if p . label () == ' RB ' : attrs . append ( p [ 0 ] ) elif word . label () . startswith ( ' NN ' ) : for p in word . parent () : if p . label () in [ ' DT ' , ' PRP$ ' , ' POS ' , ' JJ ' , ' CD ' , ' ADJP ' , ' QP ' , ' NP ' ]: attrs . append ( p [ 0 ] ) elif word . label () . startswith ( ' VB ' ) : for p in word . parent () : if p . label () == ' ADVP ' : attrs . append ( p [ 0 ] ) # Search among the word ' s uncles if word . label () . startswith ( ' NN ' ) or word . label () . startswith ( ' JJ ' ) : for p in word . parent () . parent () : if p . label () == ' PP ' and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) elif word . label () . startswith ( ' VB ' ) : for p in word . parent () . parent () : if p . label () . startswith ( ' VB ' ) and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) return attrs extract_object def extract_object ( self , parse_tree ) View Source def extract_object ( self , parse_tree ) : # Extract the first noun or first adjective in NP , PP , ADP siblings of VP_subtree objects , output , word = [],[],[] for s in parse_tree . subtrees ( lambda x : x . label () == ' VP ' ) : for t in s . subtrees ( lambda y : y . label () in [ ' NP ' , ' PP ' , ' ADP ' ] ) : if t . label () in [ ' NP ' , ' PP ' ]: for u in t . subtrees ( lambda z : z . label () . startswith ( ' NN ' )) : word = u else : for u in t . subtrees ( lambda z : z . label () . startswith ( ' JJ ' )) : word = u if len ( word ) != 0 : output = [ word [ 0 ], extractNlp . extract_attr ( self , word ) ] if output != [] and output not in objects : objects . append ( output ) if len ( objects ) != 0 : return objects [ 0 ] else : return [ '' ] extract_predicate def extract_predicate ( self , parse_tree ) View Source def extract_predicate ( self , parse_tree ) : # Extract the deepest ( last ) verb foybd ub VP_subtree output , predicate = [],[] for s in parse_tree . subtrees ( lambda x : x . label () == ' VP ' ) : for t in s . subtrees ( lambda y : y . label () . startswith ( ' VB ' )) : output = [ t [ 0 ], extractNlp . extract_attr ( self , t ) ] if output != [] and output not in predicate : predicate . append ( output ) if len ( predicate ) != 0 : return predicate [ - 1 ] else : return [ '' ] extract_subject def extract_subject ( self , parse_tree ) View Source def extract_subject ( self , parse_tree ) : # Extract the first noun found in NP_subtree subject = [] for s in parse_tree . subtrees ( lambda x : x . label () == ' NP ' ) : for t in s . subtrees ( lambda y : y . label () . startswith ( ' NN ' )) : output = [ t [ 0 ], extractNlp . extract_attr ( self , t ) ] # Avoid empty or repeated values if output != [] and output not in subject : subject . append ( output ) if len ( subject ) != 0 : return subject [ 0 ] else : return [ '' ] fulldataset def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs main def main ( self , srs_param , id_param , col_param , output = 'parse_tree' ) View Source def main ( self , srs_param , id_param , col_param , output = mode_param ) : id_req = extractNlp . fulldataset ( self , srs_param ) [ id_param ] data_num = extractNlp . fulldataset ( self , srs_param ) [ col_param ] data_triplet = [ extractNlp.triplet_extraction(self, num, output) for num in extractNlp.fulldataset(self, srs_param)[col_param ] ] triplet_df = pd . DataFrame ( [ data_num, data_triplet ] , index = [ 'origin', 'triplet' ] , columns = id_req ). T extractNlp . __del__ ( self ) return triplet_df preprocessing def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ()) triplet_extraction def triplet_extraction ( self , input_sent , output = [ 'parse_tree' , 'spo' , 'result' ] ) View Source def triplet_extraction ( self , input_sent , output = [ ' parse_tree ' , ' spo ' , ' result ' ] ) : parse_tree , = ParentedTree . convert ( list ( self . __pos_tagger . parse ( input_sent . split ())) [ 0 ] ) # Extract subject , predicate and object subject = extractNlp . extract_subject ( self , parse_tree ) predicate = extractNlp . extract_predicate ( self , parse_tree ) objects = extractNlp . extract_object ( self , parse_tree ) if ' parse_tree ' in output : print ( ' ---Parse Tree--- ' ) parse_tree . pretty_print () elif ' spo ' in output : return ( \" subject: \\t {} \\n predicate: \\t {} \\n object: \\t {} \" . format ( subject , predicate , objects )) elif ' result ' in output : return ( ' ' . join ( [ subject [ 0 ], predicate [ 0 ], objects [ 0 ]] ))","title":"Modul Triplet"},{"location":"reference/extractreq/modul_triplet/#module-extractreqmodul_triplet","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # @title Modul2c : Triplet NLTK - Stanford Triplet Parameter { vertical - output : true } url_param = \"http://corenlp.run/\" # @param { type : \"string\" } dataFile = \"/content/drive/MyDrive/dataset/dataset_2.xlsx\" # @param { type : \"string\" } srs_param = \"2003 - Tachonet\" # @param [ \"0000 - cctns\", \"0000 - gamma j\", \"0000 - Inventory\", \"1998 - themas\", \"1999 - dii\", \"1999 - multi-mahjong\", \"1999 - tcs\", \"2000 - nasa x38\", \"2001 - ctc network\", \"2001 - esa\", \"2001 - hats\", \"2001 -libra\", \"2001 - npac\", \"2001 - space fractions\", \"2002 - evia back\", \"2002 - evia corr\", \"2003 - agentmom\", \"2003 - pnnl\", \"2003 - qheadache\", \"2003 - Tachonet\", \"2004 - colorcast\", \"2004 - eprocurement\", \"2004 - grid bgc\", \"2004 - ijis\", \"2004 - Phillip\", \"2004 - rlcs\", \"2004 - sprat\", \"2005 - clarus high\", \"2005 - clarus low\", \"2005 - Grid 3D\", \"2005 - nenios\", \"2005 - phin\", \"2005 - pontis\", \"2005 - triangle\", \"2005 - znix\", \"2006 - stewards\", \"2007 - ertms\", \"2007 - estore\", \"2007 - nde\", \"2007 - get real 0.2\", \"2007 - mdot\", \"2007 - nlm\", \"2007 - puget sound\", \"2007 - water use\", \"2008 - caiso\", \"2008 - keepass\", \"2008 - peering\", \"2008 - viper\", \"2008 - virtual ed\", \"2008 - vub\", \"2009 - email\", \"2009 - gaia\", \"2009 - inventory 2.0\", \"2009 - library\", \"2009 - library2\", \"2009 - peazip\", \"2009 - video search\", \"2009 - warc III\", \"2010 - blit draft\", \"2010 - fishing\", \"2010 - gparted\", \"2010 - home\", \"2010 - mashboot\", \"2010 - split merge\" ] id_param = \"ID\" # @param [ \"ID\" ] col_param = \"Requirement Statement\" # @param [ \"Requirement Statement\", \"req\" ] mode_param = \"parse_tree\" # @param [ \"parse_tree\", \"spo\", \"result\" ] import nltk , pandas as pd , numpy as np from nltk . tag . stanford import CoreNLPPOSTagger from nltk . tree import ParentedTree from tabulate import tabulate class extractNlp : def __init__ ( self , coreUrl = url_param , fileName = dataFile ) : self . __pos_tagger = CoreNLPPOSTagger ( url = coreUrl ) self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def triplet_extraction ( self , input_sent , output =[ 'parse_tree','spo','result' ] ) : parse_tree , = ParentedTree . convert ( list ( self . __pos_tagger . parse ( input_sent . split ())) [ 0 ] ) # Extract subject , predicate and object subject = extractNlp . extract_subject ( self , parse_tree ) predicate = extractNlp . extract_predicate ( self , parse_tree ) objects = extractNlp . extract_object ( self , parse_tree ) if 'parse_tree' in output : print ( '---Parse Tree---' ) parse_tree . pretty_print () elif 'spo' in output : return ( \"subject:\\t{}\\npredicate:\\t{}\\nobject:\\t{}\" . format ( subject , predicate , objects )) elif 'result' in output : return ( ' ' . join ( [ subject[0 ] , predicate [ 0 ] , objects [ 0 ] ] )) def extract_subject ( self , parse_tree ) : # Extract the first noun found in NP_subtree subject = [] for s in parse_tree . subtrees ( lambda x : x . label () == 'NP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'NN' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] # Avoid empty or repeated values if output != [] and output not in subject : subject . append ( output ) if len ( subject ) != 0 : return subject [ 0 ] else : return [ '' ] def extract_predicate ( self , parse_tree ) : # Extract the deepest ( last ) verb foybd ub VP_subtree output , predicate = [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'VB' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] if output != [] and output not in predicate : predicate . append ( output ) if len ( predicate ) != 0 : return predicate [ -1 ] else : return [ '' ] def extract_object ( self , parse_tree ) : # Extract the first noun or first adjective in NP , PP , ADP siblings of VP_subtree objects , output , word = [] , [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label () in [ 'NP','PP','ADP' ] ) : if t . label () in [ 'NP','PP' ] : for u in t . subtrees ( lambda z : z . label (). startswith ( 'NN' )) : word = u else : for u in t . subtrees ( lambda z : z . label (). startswith ( 'JJ' )) : word = u if len ( word ) != 0 : output = [ word[0 ] , extractNlp . extract_attr ( self , word ) ] if output != [] and output not in objects : objects . append ( output ) if len ( objects ) != 0 : return objects [ 0 ] else : return [ '' ] def extract_attr ( self , word ) : attrs = [] # Search among the word 's siblings if word.label().startswith(' JJ '): for p in word.parent(): if p.label() == ' RB ': attrs.append(p[0]) elif word.label().startswith(' NN '): for p in word.parent(): if p.label() in [' DT ',' PRP $ ',' POS ',' JJ ',' CD ',' ADJP ',' QP ',' NP ']: attrs.append(p[0]) elif word.label().startswith(' VB '): for p in word.parent(): if p.label() == ' ADVP ': attrs.append(p[0]) # Search among the word' s uncles if word . label (). startswith ( 'NN' ) or word . label (). startswith ( 'JJ' ) : for p in word . parent (). parent () : if p . label () == 'PP' and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) elif word . label (). startswith ( 'VB' ) : for p in word . parent (). parent () : if p . label (). startswith ( 'VB' ) and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) return attrs def __del__ ( self ) : print ( \"destructed\" ) def main ( self , srs_param , id_param , col_param , output = mode_param ) : id_req = extractNlp . fulldataset ( self , srs_param ) [ id_param ] data_num = extractNlp . fulldataset ( self , srs_param ) [ col_param ] data_triplet = [ extractNlp.triplet_extraction(self, num, output) for num in extractNlp.fulldataset(self, srs_param)[col_param ] ] triplet_df = pd . DataFrame ( [ data_num, data_triplet ] , index = [ 'origin', 'triplet' ] , columns = id_req ). T extractNlp . __del__ ( self ) return triplet_df if __name__ == \"__main__\" : try : triplet_data = extractNlp ( dataFile ). main ( srs_param , id_param , col_param , mode_param ) print ( tabulate ( triplet_data , headers = 'keys' , tablefmt = 'psql' )) except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"Module extractreq.modul_triplet"},{"location":"reference/extractreq/modul_triplet/#variables","text":"col_param dataFile id_param mode_param srs_param url_param","title":"Variables"},{"location":"reference/extractreq/modul_triplet/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/modul_triplet/#extractnlp","text":"class extractNlp ( coreUrl = 'http://corenlp.run/' , fileName = '/content/drive/MyDrive/dataset/dataset_2.xlsx' ) View Source class extractNlp : def __init__ ( self , coreUrl = url_param , fileName = dataFile ) : self . __pos_tagger = CoreNLPPOSTagger ( url = coreUrl ) self . __data = fileName def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def triplet_extraction ( self , input_sent , output =[ 'parse_tree','spo','result' ] ) : parse_tree , = ParentedTree . convert ( list ( self . __pos_tagger . parse ( input_sent . split ())) [ 0 ] ) # Extract subject , predicate and object subject = extractNlp . extract_subject ( self , parse_tree ) predicate = extractNlp . extract_predicate ( self , parse_tree ) objects = extractNlp . extract_object ( self , parse_tree ) if 'parse_tree' in output : print ( '---Parse Tree---' ) parse_tree . pretty_print () elif 'spo' in output : return ( \"subject:\\t{}\\npredicate:\\t{}\\nobject:\\t{}\" . format ( subject , predicate , objects )) elif 'result' in output : return ( ' ' . join ( [ subject[0 ] , predicate [ 0 ] , objects [ 0 ] ] )) def extract_subject ( self , parse_tree ) : # Extract the first noun found in NP_subtree subject = [] for s in parse_tree . subtrees ( lambda x : x . label () == 'NP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'NN' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] # Avoid empty or repeated values if output != [] and output not in subject : subject . append ( output ) if len ( subject ) != 0 : return subject [ 0 ] else : return [ '' ] def extract_predicate ( self , parse_tree ) : # Extract the deepest ( last ) verb foybd ub VP_subtree output , predicate = [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label (). startswith ( 'VB' )) : output = [ t[0 ] , extractNlp . extract_attr ( self , t ) ] if output != [] and output not in predicate : predicate . append ( output ) if len ( predicate ) != 0 : return predicate [ -1 ] else : return [ '' ] def extract_object ( self , parse_tree ) : # Extract the first noun or first adjective in NP , PP , ADP siblings of VP_subtree objects , output , word = [] , [] , [] for s in parse_tree . subtrees ( lambda x : x . label () == 'VP' ) : for t in s . subtrees ( lambda y : y . label () in [ 'NP','PP','ADP' ] ) : if t . label () in [ 'NP','PP' ] : for u in t . subtrees ( lambda z : z . label (). startswith ( 'NN' )) : word = u else : for u in t . subtrees ( lambda z : z . label (). startswith ( 'JJ' )) : word = u if len ( word ) != 0 : output = [ word[0 ] , extractNlp . extract_attr ( self , word ) ] if output != [] and output not in objects : objects . append ( output ) if len ( objects ) != 0 : return objects [ 0 ] else : return [ '' ] def extract_attr ( self , word ) : attrs = [] # Search among the word 's siblings if word.label().startswith(' JJ '): for p in word.parent(): if p.label() == ' RB ': attrs.append(p[0]) elif word.label().startswith(' NN '): for p in word.parent(): if p.label() in [' DT ',' PRP $ ',' POS ',' JJ ',' CD ',' ADJP ',' QP ',' NP ']: attrs.append(p[0]) elif word.label().startswith(' VB '): for p in word.parent(): if p.label() == ' ADVP ': attrs.append(p[0]) # Search among the word' s uncles if word . label (). startswith ( 'NN' ) or word . label (). startswith ( 'JJ' ) : for p in word . parent (). parent () : if p . label () == 'PP' and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) elif word . label (). startswith ( 'VB' ) : for p in word . parent (). parent () : if p . label (). startswith ( 'VB' ) and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) return attrs def __del__ ( self ) : print ( \"destructed\" ) def main ( self , srs_param , id_param , col_param , output = mode_param ) : id_req = extractNlp . fulldataset ( self , srs_param ) [ id_param ] data_num = extractNlp . fulldataset ( self , srs_param ) [ col_param ] data_triplet = [ extractNlp.triplet_extraction(self, num, output) for num in extractNlp.fulldataset(self, srs_param)[col_param ] ] triplet_df = pd . DataFrame ( [ data_num, data_triplet ] , index = [ 'origin', 'triplet' ] , columns = id_req ). T extractNlp . __del__ ( self ) return triplet_df","title":"extractNlp"},{"location":"reference/extractreq/modul_triplet/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/modul_triplet/#extract_attr","text":"def extract_attr ( self , word ) View Source def extract_attr ( self , word ) : attrs = [] # Search among the word ' s siblings if word . label () . startswith ( ' JJ ' ) : for p in word . parent () : if p . label () == ' RB ' : attrs . append ( p [ 0 ] ) elif word . label () . startswith ( ' NN ' ) : for p in word . parent () : if p . label () in [ ' DT ' , ' PRP$ ' , ' POS ' , ' JJ ' , ' CD ' , ' ADJP ' , ' QP ' , ' NP ' ]: attrs . append ( p [ 0 ] ) elif word . label () . startswith ( ' VB ' ) : for p in word . parent () : if p . label () == ' ADVP ' : attrs . append ( p [ 0 ] ) # Search among the word ' s uncles if word . label () . startswith ( ' NN ' ) or word . label () . startswith ( ' JJ ' ) : for p in word . parent () . parent () : if p . label () == ' PP ' and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) elif word . label () . startswith ( ' VB ' ) : for p in word . parent () . parent () : if p . label () . startswith ( ' VB ' ) and p != word . parent () : attrs . append ( ' ' . join ( p . flatten ())) return attrs","title":"extract_attr"},{"location":"reference/extractreq/modul_triplet/#extract_object","text":"def extract_object ( self , parse_tree ) View Source def extract_object ( self , parse_tree ) : # Extract the first noun or first adjective in NP , PP , ADP siblings of VP_subtree objects , output , word = [],[],[] for s in parse_tree . subtrees ( lambda x : x . label () == ' VP ' ) : for t in s . subtrees ( lambda y : y . label () in [ ' NP ' , ' PP ' , ' ADP ' ] ) : if t . label () in [ ' NP ' , ' PP ' ]: for u in t . subtrees ( lambda z : z . label () . startswith ( ' NN ' )) : word = u else : for u in t . subtrees ( lambda z : z . label () . startswith ( ' JJ ' )) : word = u if len ( word ) != 0 : output = [ word [ 0 ], extractNlp . extract_attr ( self , word ) ] if output != [] and output not in objects : objects . append ( output ) if len ( objects ) != 0 : return objects [ 0 ] else : return [ '' ]","title":"extract_object"},{"location":"reference/extractreq/modul_triplet/#extract_predicate","text":"def extract_predicate ( self , parse_tree ) View Source def extract_predicate ( self , parse_tree ) : # Extract the deepest ( last ) verb foybd ub VP_subtree output , predicate = [],[] for s in parse_tree . subtrees ( lambda x : x . label () == ' VP ' ) : for t in s . subtrees ( lambda y : y . label () . startswith ( ' VB ' )) : output = [ t [ 0 ], extractNlp . extract_attr ( self , t ) ] if output != [] and output not in predicate : predicate . append ( output ) if len ( predicate ) != 0 : return predicate [ - 1 ] else : return [ '' ]","title":"extract_predicate"},{"location":"reference/extractreq/modul_triplet/#extract_subject","text":"def extract_subject ( self , parse_tree ) View Source def extract_subject ( self , parse_tree ) : # Extract the first noun found in NP_subtree subject = [] for s in parse_tree . subtrees ( lambda x : x . label () == ' NP ' ) : for t in s . subtrees ( lambda y : y . label () . startswith ( ' NN ' )) : output = [ t [ 0 ], extractNlp . extract_attr ( self , t ) ] # Avoid empty or repeated values if output != [] and output not in subject : subject . append ( output ) if len ( subject ) != 0 : return subject [ 0 ] else : return [ '' ]","title":"extract_subject"},{"location":"reference/extractreq/modul_triplet/#fulldataset","text":"def fulldataset ( self , inputSRS ) fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS ) : # function membuat dataset \"\"\" fungsi ini digunakan untuk menentukand dataset yang digunakan berdasarkan indeks srs yang dipilih, maka dari itu hal ini penting untuk menyiapkan data selanjutnya. partOf().fulldataset(inputSRS) \"\"\" xl = pd . ExcelFile ( self . __data ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs","title":"fulldataset"},{"location":"reference/extractreq/modul_triplet/#main","text":"def main ( self , srs_param , id_param , col_param , output = 'parse_tree' ) View Source def main ( self , srs_param , id_param , col_param , output = mode_param ) : id_req = extractNlp . fulldataset ( self , srs_param ) [ id_param ] data_num = extractNlp . fulldataset ( self , srs_param ) [ col_param ] data_triplet = [ extractNlp.triplet_extraction(self, num, output) for num in extractNlp.fulldataset(self, srs_param)[col_param ] ] triplet_df = pd . DataFrame ( [ data_num, data_triplet ] , index = [ 'origin', 'triplet' ] , columns = id_req ). T extractNlp . __del__ ( self ) return triplet_df","title":"main"},{"location":"reference/extractreq/modul_triplet/#preprocessing","text":"def preprocessing ( self ) fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji, sebab memperlihatkan data excel beseerta tab yang digunakan. partOf().preprocssing() View Source def preprocessing ( self ) : # function melihat struktur dataset di excel \"\"\" fungsi ini digunakan untuk preprocessing untuk melihat dataset excel yang digunakan fungsi ini dapat melihat struktur dataset yang diuji , sebab memperlihatkan data excel beseerta tab yang digunakan . partOf () . preprocssing () \"\"\" xl = pd . ExcelFile ( self . __data ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/extractreq/modul_triplet/#triplet_extraction","text":"def triplet_extraction ( self , input_sent , output = [ 'parse_tree' , 'spo' , 'result' ] ) View Source def triplet_extraction ( self , input_sent , output = [ ' parse_tree ' , ' spo ' , ' result ' ] ) : parse_tree , = ParentedTree . convert ( list ( self . __pos_tagger . parse ( input_sent . split ())) [ 0 ] ) # Extract subject , predicate and object subject = extractNlp . extract_subject ( self , parse_tree ) predicate = extractNlp . extract_predicate ( self , parse_tree ) objects = extractNlp . extract_object ( self , parse_tree ) if ' parse_tree ' in output : print ( ' ---Parse Tree--- ' ) parse_tree . pretty_print () elif ' spo ' in output : return ( \" subject: \\t {} \\n predicate: \\t {} \\n object: \\t {} \" . format ( subject , predicate , objects )) elif ' result ' in output : return ( ' ' . join ( [ subject [ 0 ], predicate [ 0 ], objects [ 0 ]] ))","title":"triplet_extraction"},{"location":"reference/extractreq/usecase_modul1/","text":"Module extractreq.usecase_modul1 None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # -*- coding: utf-8 -*- \"\"\"modul_relasi.ipynb Author Rakha Asyrofi / 05111950010038 Automatically generated by Colaboratory. Original file is located at https://colab.research.google.com/drive/1h6HKNeALV8bXjrxWB0Jn0ztHtLv2cXz8 \"\"\" \"\"\"# Modul1: xmlparser\"\"\" # function import xml.etree.ElementTree as ET import pandas as pd from tabulate import tabulate from time import time # template class xmlparser class xmlParser : # inisialisasi def __init__ ( self , filename = 'IRCI_Researcher.xmi' , tipe_xmi = '{http://schema.omg.org/spec/XMI/2.1}type' , id_xmi = '{http://schema.omg.org/spec/XMI/2.1}id' ): self . namaFile = filename self . xmi_type = tipe_xmi self . xmi_id = id_xmi #fungsi parse tree elemen def elemenTreeParse ( self ): tree = ET . parse ( self . namaFile ) root = tree . getroot () try : elemenTag = [ elem . tag for elem in root . iter ()] elemenAtribut = [ elem . attrib for elem in root . iter ()] tabelElemen = pd . DataFrame ([ elemenTag , elemenAtribut ], index = [ 'Berdasarkan Tag' , 'Berdsarkan Atribut' ]) . T return tabelElemen except OSError as err : print ( \"OS error: {0} \" . format ( err )) # fungsi pencarian elemen def cariTreeElemen ( self , elemen ): try : tree1 = ET . parse ( self . namaFile ) root1 = tree1 . getroot () pencarian = [ num . findall ( elemen ) for num in root1 . iter ()] return pencarian except OSError as err : print ( \"OS error: {0} \" . format ( err )) #fungsi list elemen def listElemen ( self , elemen ): try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () listElemen = [ berdasarkanOwnEnd . attrib for berdasarkanOwnEnd in root . iter ( elemen )] tabelElement = pd . DataFrame ( listElemen ) return tabelElement except OSError as err : print ( \"OS error: {0} \" . format ( err )) # fungsi mencari table spesifik def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement . attrib for packagedElement in root . iter ( elemen )] for num in berdasarkanPackagedELement : a1 = num [ kolom1 ] c1 = num [ self . xmi_type ] d1 = num [ self . xmi_id ] hasil . append ([ a1 , c1 , d1 ]) cleanPackagedELement = pd . DataFrame ( hasil , columns = [ kolom1 , self . xmi_type , self . xmi_id ]) return cleanPackagedELement except OSError as err : print ( \"OS error: {0} \" . format ( err )) # fungsi mencari string def doString ( self ): try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () print ( ET . tostring ( root , encoding = 'utf8' ) . decode ( 'utf8' )) except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataPaketElemen ( self , category = 'packagedElement' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement . attrib for packagedElement in root . iter ( category )] for num in berdasarkanPackagedELement : a1 = num [ self . xmi_id ] b1 = num [ 'name' ] d1 = num [ self . xmi_type ] hasil . append ([ a1 , b1 , d1 ]) paketElemen = pd . DataFrame ( hasil , columns = [ 'id' , 'name' , 'type' ]) return paketElemen except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataExtend ( self , category = 'extend' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanExtend = [ packagedElement . attrib for packagedElement in root . iter ( category )] for num in berdasarkanExtend : a1 = num [ self . xmi_id ] b1 = num [ self . xmi_type ] c1 = num [ 'extendedCase' ] d1 = paketElemen [ paketElemen [ 'id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'extension' ] f1 = paketElemen [ paketElemen [ 'id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ([ a1 , b1 , c1 , d1 , e1 , f1 ]) extendTable = pd . DataFrame ( hasil , columns = [ 'id' , 'type' , 'source' , 'sourceName' , 'destination' , 'destinationName' ]) return extendTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataInclude ( self , category = 'include' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () byinclude = [ packagedElement . attrib for packagedElement in root . iter ( category )] for num in byinclude : a1 = num [ '{http://schema.omg.org/spec/XMI/2.1}id' ] b1 = num [ '{http://schema.omg.org/spec/XMI/2.1}type' ] c1 = num [ 'includingCase' ] d1 = paketElemen [ paketElemen [ 'id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'addition' ] f1 = paketElemen [ paketElemen [ 'id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ([ a1 , b1 , c1 , d1 , e1 , f1 ]) includeTable = pd . DataFrame ( hasil , columns = [ 'id' , 'tipe' , 'include' , 'includeName' , 'addition' , 'additionName' ]) return includeTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataOwnedEnd ( self , category = 'ownedEnd' ): try : # berdasarkan ownedEnd hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedEnd = [ packagedElement . attrib for packagedElement in root . iter ( category )] berdasarkanOwnedEnd for num in berdasarkanOwnedEnd : a1 = num [ 'type' ] b1 = num [ self . xmi_id ] c1 = num [ self . xmi_type ] d1 = paketElemen [ paketElemen [ 'id' ] == a1 ] . iloc [ 0 ][ 'name' ] hasil . append ([ a1 , b1 , c1 , d1 ]) ownedEndTable = pd . DataFrame ( hasil , columns = [ 'id_data' , 'id_property' , 'type_property' , 'id_name' ]) return ownedEndTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataOwnedMember ( self , category = 'ownedMember' ): try : # berdasarkan UML Model hasilNum = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedMember = [ packagedElement for packagedElement in root . iter ( category )] for num in berdasarkanOwnedMember : a = num . attrib [ self . xmi_id ] b = num . attrib [ self . xmi_type ] for index , angka in enumerate ( num . iter ( 'ownedEnd' )): if index == 0 : c = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] else : d = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] hasilNum . append ([ a , b , c , d ]) ownedMemberTable = pd . DataFrame ( hasilNum , columns = [ 'id' , 'type_property' , 'actor' , 'usecase' ]) return ownedMemberTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) if __name__ == \"__main__\" : try : t0 = time () # myXmlParser = xmlParser(filename= 'IRCI_Topic.xmi') # myXmlParser = xmlParser(filename= 'IRCI_Researcher.xmi') # myXmlParser = xmlParser(filename= 'rAnalyzerUC.xmi') myXmlParser = xmlParser () paketElemen = myXmlParser . dataPaketElemen () extendTable = myXmlParser . dataExtend () ownedEndTable = myXmlParser . dataOwnedEnd () ownedMemberTable = myXmlParser . dataOwnedMember () \"\"\"# Modul 1 Parsing file xmi menjadi tabel2 (daftar aktor, daftar use case, dan relasi antara actor use case dan antar use case) \"\"\" print ( \"actorTable\" ) actorTable = paketElemen [ paketElemen [ 'type' ] == 'uml:Actor' ] print ( tabulate ( actorTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n useCaseTable\" ) useCaseTable = paketElemen [ paketElemen [ 'type' ] == 'uml:UseCase' ] print ( tabulate ( useCaseTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n extendTable\" ) print ( tabulate ( extendTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n associationTable\" ) print ( tabulate ( ownedMemberTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n propertyTable\" ) print ( tabulate ( ownedEndTable , headers = 'keys' , tablefmt = 'psql' )) #untuk extend - data researcher dan topic hasilAktor = [] hasilDestinasi = [] for idx , num in enumerate ( extendTable . sourceName ): c = ownedMemberTable [ ownedMemberTable [ 'usecase' ] == extendTable . sourceName [ idx ]] if len ( c ) > 0 : for aktor in c . actor : hasilAktor . append ( aktor ) hasilDestinasi . append ( extendTable . destinationName [ idx ]) else : temp = 2 d = ownedMemberTable [ ownedMemberTable [ 'usecase' ] == extendTable . sourceName [ idx - temp ]] for dAktor in d . actor : hasilAktor . append ( dAktor ) hasilDestinasi . append ( extendTable . destinationName [ idx ]) df_a = pd . DataFrame ([ hasilAktor , hasilDestinasi ], index = [ 'actor' , 'action' ]) . T df_a [ 'actor' ] = df_a . groupby ([ 'action' ])[ 'actor' ] . transform ( lambda x : ';' . join ( x )) df_a = df_a [[ 'action' , 'actor' ]] . drop_duplicates () df_a [ 'actor' ][ 0 ] = set ( df_a [ 'actor' ][ 0 ] . split ( \";\" )) # fungsi ini digunakan untuk menyempurnakan format df_a [ 'actor' ][ 0 ] = \";\" . join ( df_a [ 'actor' ][ 0 ]) ownedMemberTable . rename ( columns = { 'usecase' : 'action' }, inplace = True ) dt_b = pd . concat ([ df_a , ownedMemberTable ]) dt_actor_action = dt_b . drop ([ 'id' , 'type_property' ], axis = 1 ) dt_actor_action [ 'actor' ] = dt_actor_action . groupby ([ 'action' ])[ 'actor' ] . transform ( lambda x : ';' . join ( x )) dt_actor_action = dt_actor_action [[ 'action' , 'actor' ]] . drop_duplicates () print ( \" \\n actorActionTable\" ) print ( tabulate ( dt_actor_action , headers = 'keys' , tablefmt = 'psql' )) # print(\"\\nincludeTable\") # print(tabulate(includeTable, headers = 'keys', tablefmt = 'psql')) # # untuk include data ranalyzer # hasilAktor = [] # hasilDestinasi = [] # for idy, angka in enumerate(includeTable.includeName): # f = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy]] # if len(f) > 0: # for aktor in f.actor: # hasilAktor.append(aktor) # hasilDestinasi.append(includeTable.additionName[idy]) # else: # tempY = 2 # g = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy-tempY]] # for dAktor in g.actor: # hasilAktor.append(dAktor) # hasilDestinasi.append(includeTable.additionName[idy]) # df_a = pd.DataFrame([hasilAktor, hasilDestinasi], index= ['actor', 'action']).T # df_a['actor'] = df_a.groupby(['action'])['actor'].transform(lambda x: ';'.join(x)) # df_a = df_a[['action','actor']].drop_duplicates() # df_a['actor'][0] = set(df_a['actor'][0].split(\";\")) # fungsi ini digunakan untuk menyempurnakan format # df_a['actor'][0] = \";\".join(df_a['actor'][0]) # ownedMemberTable.rename(columns = {'usecase':'action'}, inplace = True) # dt_b = pd.concat([df_a, ownedMemberTable]) # dt_actor_action = dt_b.drop(['id', 'type_property'], axis= 1) # print(\"\\nactorActionTable\") # print(tabulate(dt_actor_action, headers = 'keys', tablefmt = 'psql')) print ( \"done in %0.3f s.\" % ( time () - t0 )) input ( 'Press ENTER to exit' ) myXmlParser . __del__ () except OSError as err : print ( \"OS error: {0} \" . format ( err )) Classes xmlParser class xmlParser ( filename = 'IRCI_Researcher.xmi' , tipe_xmi = '{http://schema.omg.org/spec/XMI/2.1}type' , id_xmi = '{http://schema.omg.org/spec/XMI/2.1}id' ) View Source class xmlParser : # inisialisasi def __init__ ( self , filename = 'IRCI_Researcher.xmi' , tipe_xmi = '{http://schema.omg.org/spec/XMI/2.1}type' , id_xmi = '{http://schema.omg.org/spec/XMI/2.1}id' ) : self . namaFile = filename self . xmi_type = tipe_xmi self . xmi_id = id_xmi #fungsi parse tree elemen def elemenTreeParse ( self ) : tree = ET . parse ( self . namaFile ) root = tree . getroot () try : elemenTag = [ elem.tag for elem in root.iter() ] elemenAtribut = [ elem.attrib for elem in root.iter() ] tabelElemen = pd . DataFrame ( [ elemenTag, elemenAtribut ] , index =[ 'Berdasarkan Tag', 'Berdsarkan Atribut' ] ). T return tabelElemen except OSError as err : print ( \"OS error: {0}\" . format ( err )) # fungsi pencarian elemen def cariTreeElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root1 = tree1 . getroot () pencarian = [ num.findall(elemen) for num in root1.iter() ] return pencarian except OSError as err : print ( \"OS error: {0}\" . format ( err )) #fungsi list elemen def listElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () listElemen = [ berdasarkanOwnEnd.attrib for berdasarkanOwnEnd in root.iter(elemen) ] tabelElement = pd . DataFrame ( listElemen ) return tabelElement except OSError as err : print ( \"OS error: {0}\" . format ( err )) # fungsi mencari table spesifik def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement.attrib for packagedElement in root.iter(elemen) ] for num in berdasarkanPackagedELement : a1 = num [ kolom1 ] c1 = num [ self.xmi_type ] d1 = num [ self.xmi_id ] hasil . append ( [ a1, c1, d1 ] ) cleanPackagedELement = pd . DataFrame ( hasil , columns =[ kolom1, self.xmi_type, self.xmi_id ] ) return cleanPackagedELement except OSError as err : print ( \"OS error: {0}\" . format ( err )) # fungsi mencari string def doString ( self ) : try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () print ( ET . tostring ( root , encoding = 'utf8' ). decode ( 'utf8' )) except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataPaketElemen ( self , category = 'packagedElement' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement.attrib for packagedElement in root.iter(category) ] for num in berdasarkanPackagedELement : a1 = num [ self.xmi_id ] b1 = num [ 'name' ] d1 = num [ self.xmi_type ] hasil . append ( [ a1, b1, d1 ] ) paketElemen = pd . DataFrame ( hasil , columns =[ 'id', 'name', 'type' ] ) return paketElemen except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataExtend ( self , category = 'extend' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanExtend = [ packagedElement.attrib for packagedElement in root.iter(category) ] for num in berdasarkanExtend : a1 = num [ self.xmi_id ] b1 = num [ self.xmi_type ] c1 = num [ 'extendedCase' ] d1 = paketElemen [ paketElemen['id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'extension' ] f1 = paketElemen [ paketElemen['id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ( [ a1, b1, c1, d1, e1, f1 ] ) extendTable = pd . DataFrame ( hasil , columns =[ 'id', 'type', 'source', 'sourceName', 'destination', 'destinationName' ] ) return extendTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataInclude ( self , category = 'include' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () byinclude = [ packagedElement.attrib for packagedElement in root.iter(category) ] for num in byinclude : a1 = num [ '{http://schema.omg.org/spec/XMI/2.1}id' ] b1 = num [ '{http://schema.omg.org/spec/XMI/2.1}type' ] c1 = num [ 'includingCase' ] d1 = paketElemen [ paketElemen['id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'addition' ] f1 = paketElemen [ paketElemen['id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ( [ a1, b1, c1, d1, e1, f1 ] ) includeTable = pd . DataFrame ( hasil , columns = [ 'id', 'tipe', 'include', 'includeName', 'addition', 'additionName' ] ) return includeTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataOwnedEnd ( self , category = 'ownedEnd' ) : try : # berdasarkan ownedEnd hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedEnd = [ packagedElement.attrib for packagedElement in root.iter(category) ] berdasarkanOwnedEnd for num in berdasarkanOwnedEnd : a1 = num [ 'type' ] b1 = num [ self.xmi_id ] c1 = num [ self.xmi_type ] d1 = paketElemen [ paketElemen['id' ] == a1 ] . iloc [ 0 ][ 'name' ] hasil . append ( [ a1, b1, c1, d1 ] ) ownedEndTable = pd . DataFrame ( hasil , columns =[ 'id_data', 'id_property', 'type_property', 'id_name' ] ) return ownedEndTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataOwnedMember ( self , category = 'ownedMember' ) : try : # berdasarkan UML Model hasilNum = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedMember = [ packagedElement for packagedElement in root.iter(category) ] for num in berdasarkanOwnedMember : a = num . attrib [ self.xmi_id ] b = num . attrib [ self.xmi_type ] for index , angka in enumerate ( num . iter ( 'ownedEnd' )) : if index == 0 : c = paketElemen [ paketElemen['id' ] == angka . attrib [ 'type' ] ] . iloc [ 0 ][ 'name' ] else : d = paketElemen [ paketElemen['id' ] == angka . attrib [ 'type' ] ] . iloc [ 0 ][ 'name' ] hasilNum . append ( [ a, b, c, d ] ) ownedMemberTable = pd . DataFrame ( hasilNum , columns =[ 'id', 'type_property', 'actor', 'usecase' ] ) return ownedMemberTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def __del__ ( self ) : print ( 'Destructor called.' ) Methods cariTreeElemen def cariTreeElemen ( self , elemen ) View Source def cariTreeElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root1 = tree1 . getroot () pencarian = [ num . findall ( elemen ) for num in root1 . iter () ] return pencarian except OSError as err : print ( \" OS error: {0} \" . format ( err )) dataExtend def dataExtend ( self , category = 'extend' ) View Source def dataExtend ( self , category = ' extend ' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanExtend = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] for num in berdasarkanExtend : a1 = num [ self . xmi_id ] b1 = num [ self . xmi_type ] c1 = num [ ' extendedCase ' ] d1 = paketElemen [ paketElemen [ ' id ' ] == c1 ]. iloc [ 0 ][ ' name ' ] e1 = num [ ' extension ' ] f1 = paketElemen [ paketElemen [ ' id ' ] == e1 ]. iloc [ 0 ][ ' name ' ] hasil . append ( [ a1 , b1 , c1 , d1 , e1 , f1 ] ) extendTable = pd . DataFrame ( hasil , columns = [ ' id ' , ' type ' , ' source ' , ' sourceName ' , ' destination ' , ' destinationName ' ] ) return extendTable except OSError as err : print ( \" OS error: {0} \" . format ( err )) dataInclude def dataInclude ( self , category = 'include' ) View Source def dataInclude ( self , category = ' include ' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () byinclude = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] for num in byinclude : a1 = num [ ' {http://schema.omg.org/spec/XMI/2.1}id ' ] b1 = num [ ' {http://schema.omg.org/spec/XMI/2.1}type ' ] c1 = num [ ' includingCase ' ] d1 = paketElemen [ paketElemen [ ' id ' ] == c1 ]. iloc [ 0 ][ ' name ' ] e1 = num [ ' addition ' ] f1 = paketElemen [ paketElemen [ ' id ' ] == e1 ]. iloc [ 0 ][ ' name ' ] hasil . append ( [ a1 , b1 , c1 , d1 , e1 , f1 ] ) includeTable = pd . DataFrame ( hasil , columns = [ ' id ' , ' tipe ' , ' include ' , ' includeName ' , ' addition ' , ' additionName ' ] ) return includeTable except OSError as err : print ( \" OS error: {0} \" . format ( err )) dataOwnedEnd def dataOwnedEnd ( self , category = 'ownedEnd' ) View Source def dataOwnedEnd ( self , category = ' ownedEnd ' ) : try : # berdasarkan ownedEnd hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedEnd = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] berdasarkanOwnedEnd for num in berdasarkanOwnedEnd : a1 = num [ ' type ' ] b1 = num [ self . xmi_id ] c1 = num [ self . xmi_type ] d1 = paketElemen [ paketElemen [ ' id ' ] == a1 ]. iloc [ 0 ][ ' name ' ] hasil . append ( [ a1 , b1 , c1 , d1 ] ) ownedEndTable = pd . DataFrame ( hasil , columns = [ ' id_data ' , ' id_property ' , ' type_property ' , ' id_name ' ] ) return ownedEndTable except OSError as err : print ( \" OS error: {0} \" . format ( err )) dataOwnedMember def dataOwnedMember ( self , category = 'ownedMember' ) View Source def dataOwnedMember ( self , category = 'ownedMember' ): try : # berdasarkan UML Model hasilNum = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedMember = [ packagedElement for packagedElement in root . iter ( category )] for num in berdasarkanOwnedMember : a = num . attrib [ self . xmi_id ] b = num . attrib [ self . xmi_type ] for index , angka in enumerate ( num . iter ( 'ownedEnd' )): if index == 0 : c = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] else : d = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] hasilNum . append ([ a , b , c , d ]) ownedMemberTable = pd . DataFrame ( hasilNum , columns = [ 'id' , 'type_property' , 'actor' , 'usecase' ]) return ownedMemberTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) dataPaketElemen def dataPaketElemen ( self , category = 'packagedElement' ) View Source def dataPaketElemen ( self , category = ' packagedElement ' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] for num in berdasarkanPackagedELement : a1 = num [ self . xmi_id ] b1 = num [ ' name ' ] d1 = num [ self . xmi_type ] hasil . append ( [ a1 , b1 , d1 ] ) paketElemen = pd . DataFrame ( hasil , columns = [ ' id ' , ' name ' , ' type ' ] ) return paketElemen except OSError as err : print ( \" OS error: {0} \" . format ( err )) doString def doString ( self ) View Source def doString(self): try: tree1 = ET.parse(self.namaFile) root = tree1.getroot() print(ET.tostring(root, encoding='utf8').decode('utf8')) except OSError as err: print(\"OS error: {0}\".format(err)) elemenTreeParse def elemenTreeParse ( self ) View Source def elemenTreeParse ( self ) : tree = ET . parse ( self . namaFile ) root = tree . getroot () try : elemenTag = [ elem . tag for elem in root . iter () ] elemenAtribut = [ elem . attrib for elem in root . iter () ] tabelElemen = pd . DataFrame ( [ elemenTag , elemenAtribut ], index = [ ' Berdasarkan Tag ' , ' Berdsarkan Atribut ' ] ) . T return tabelElemen except OSError as err : print ( \" OS error: {0} \" . format ( err )) listElemen def listElemen ( self , elemen ) View Source def listElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () listElemen = [ berdasarkanOwnEnd . attrib for berdasarkanOwnEnd in root . iter ( elemen ) ] tabelElement = pd . DataFrame ( listElemen ) return tabelElement except OSError as err : print ( \" OS error: {0} \" . format ( err )) tableElemenSpesifik def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ) View Source def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement.attrib for packagedElement in root.iter(elemen) ] for num in berdasarkanPackagedELement : a1 = num [ kolom1 ] c1 = num [ self.xmi_type ] d1 = num [ self.xmi_id ] hasil . append ( [ a1, c1, d1 ] ) cleanPackagedELement = pd . DataFrame ( hasil , columns =[ kolom1, self.xmi_type, self.xmi_id ] ) return cleanPackagedELement except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"Usecase Modul1"},{"location":"reference/extractreq/usecase_modul1/#module-extractrequsecase_modul1","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # -*- coding: utf-8 -*- \"\"\"modul_relasi.ipynb Author Rakha Asyrofi / 05111950010038 Automatically generated by Colaboratory. Original file is located at https://colab.research.google.com/drive/1h6HKNeALV8bXjrxWB0Jn0ztHtLv2cXz8 \"\"\" \"\"\"# Modul1: xmlparser\"\"\" # function import xml.etree.ElementTree as ET import pandas as pd from tabulate import tabulate from time import time # template class xmlparser class xmlParser : # inisialisasi def __init__ ( self , filename = 'IRCI_Researcher.xmi' , tipe_xmi = '{http://schema.omg.org/spec/XMI/2.1}type' , id_xmi = '{http://schema.omg.org/spec/XMI/2.1}id' ): self . namaFile = filename self . xmi_type = tipe_xmi self . xmi_id = id_xmi #fungsi parse tree elemen def elemenTreeParse ( self ): tree = ET . parse ( self . namaFile ) root = tree . getroot () try : elemenTag = [ elem . tag for elem in root . iter ()] elemenAtribut = [ elem . attrib for elem in root . iter ()] tabelElemen = pd . DataFrame ([ elemenTag , elemenAtribut ], index = [ 'Berdasarkan Tag' , 'Berdsarkan Atribut' ]) . T return tabelElemen except OSError as err : print ( \"OS error: {0} \" . format ( err )) # fungsi pencarian elemen def cariTreeElemen ( self , elemen ): try : tree1 = ET . parse ( self . namaFile ) root1 = tree1 . getroot () pencarian = [ num . findall ( elemen ) for num in root1 . iter ()] return pencarian except OSError as err : print ( \"OS error: {0} \" . format ( err )) #fungsi list elemen def listElemen ( self , elemen ): try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () listElemen = [ berdasarkanOwnEnd . attrib for berdasarkanOwnEnd in root . iter ( elemen )] tabelElement = pd . DataFrame ( listElemen ) return tabelElement except OSError as err : print ( \"OS error: {0} \" . format ( err )) # fungsi mencari table spesifik def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement . attrib for packagedElement in root . iter ( elemen )] for num in berdasarkanPackagedELement : a1 = num [ kolom1 ] c1 = num [ self . xmi_type ] d1 = num [ self . xmi_id ] hasil . append ([ a1 , c1 , d1 ]) cleanPackagedELement = pd . DataFrame ( hasil , columns = [ kolom1 , self . xmi_type , self . xmi_id ]) return cleanPackagedELement except OSError as err : print ( \"OS error: {0} \" . format ( err )) # fungsi mencari string def doString ( self ): try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () print ( ET . tostring ( root , encoding = 'utf8' ) . decode ( 'utf8' )) except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataPaketElemen ( self , category = 'packagedElement' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement . attrib for packagedElement in root . iter ( category )] for num in berdasarkanPackagedELement : a1 = num [ self . xmi_id ] b1 = num [ 'name' ] d1 = num [ self . xmi_type ] hasil . append ([ a1 , b1 , d1 ]) paketElemen = pd . DataFrame ( hasil , columns = [ 'id' , 'name' , 'type' ]) return paketElemen except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataExtend ( self , category = 'extend' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanExtend = [ packagedElement . attrib for packagedElement in root . iter ( category )] for num in berdasarkanExtend : a1 = num [ self . xmi_id ] b1 = num [ self . xmi_type ] c1 = num [ 'extendedCase' ] d1 = paketElemen [ paketElemen [ 'id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'extension' ] f1 = paketElemen [ paketElemen [ 'id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ([ a1 , b1 , c1 , d1 , e1 , f1 ]) extendTable = pd . DataFrame ( hasil , columns = [ 'id' , 'type' , 'source' , 'sourceName' , 'destination' , 'destinationName' ]) return extendTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataInclude ( self , category = 'include' ): try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () byinclude = [ packagedElement . attrib for packagedElement in root . iter ( category )] for num in byinclude : a1 = num [ '{http://schema.omg.org/spec/XMI/2.1}id' ] b1 = num [ '{http://schema.omg.org/spec/XMI/2.1}type' ] c1 = num [ 'includingCase' ] d1 = paketElemen [ paketElemen [ 'id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'addition' ] f1 = paketElemen [ paketElemen [ 'id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ([ a1 , b1 , c1 , d1 , e1 , f1 ]) includeTable = pd . DataFrame ( hasil , columns = [ 'id' , 'tipe' , 'include' , 'includeName' , 'addition' , 'additionName' ]) return includeTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataOwnedEnd ( self , category = 'ownedEnd' ): try : # berdasarkan ownedEnd hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedEnd = [ packagedElement . attrib for packagedElement in root . iter ( category )] berdasarkanOwnedEnd for num in berdasarkanOwnedEnd : a1 = num [ 'type' ] b1 = num [ self . xmi_id ] c1 = num [ self . xmi_type ] d1 = paketElemen [ paketElemen [ 'id' ] == a1 ] . iloc [ 0 ][ 'name' ] hasil . append ([ a1 , b1 , c1 , d1 ]) ownedEndTable = pd . DataFrame ( hasil , columns = [ 'id_data' , 'id_property' , 'type_property' , 'id_name' ]) return ownedEndTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def dataOwnedMember ( self , category = 'ownedMember' ): try : # berdasarkan UML Model hasilNum = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedMember = [ packagedElement for packagedElement in root . iter ( category )] for num in berdasarkanOwnedMember : a = num . attrib [ self . xmi_id ] b = num . attrib [ self . xmi_type ] for index , angka in enumerate ( num . iter ( 'ownedEnd' )): if index == 0 : c = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] else : d = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] hasilNum . append ([ a , b , c , d ]) ownedMemberTable = pd . DataFrame ( hasilNum , columns = [ 'id' , 'type_property' , 'actor' , 'usecase' ]) return ownedMemberTable except OSError as err : print ( \"OS error: {0} \" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) if __name__ == \"__main__\" : try : t0 = time () # myXmlParser = xmlParser(filename= 'IRCI_Topic.xmi') # myXmlParser = xmlParser(filename= 'IRCI_Researcher.xmi') # myXmlParser = xmlParser(filename= 'rAnalyzerUC.xmi') myXmlParser = xmlParser () paketElemen = myXmlParser . dataPaketElemen () extendTable = myXmlParser . dataExtend () ownedEndTable = myXmlParser . dataOwnedEnd () ownedMemberTable = myXmlParser . dataOwnedMember () \"\"\"# Modul 1 Parsing file xmi menjadi tabel2 (daftar aktor, daftar use case, dan relasi antara actor use case dan antar use case) \"\"\" print ( \"actorTable\" ) actorTable = paketElemen [ paketElemen [ 'type' ] == 'uml:Actor' ] print ( tabulate ( actorTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n useCaseTable\" ) useCaseTable = paketElemen [ paketElemen [ 'type' ] == 'uml:UseCase' ] print ( tabulate ( useCaseTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n extendTable\" ) print ( tabulate ( extendTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n associationTable\" ) print ( tabulate ( ownedMemberTable , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n propertyTable\" ) print ( tabulate ( ownedEndTable , headers = 'keys' , tablefmt = 'psql' )) #untuk extend - data researcher dan topic hasilAktor = [] hasilDestinasi = [] for idx , num in enumerate ( extendTable . sourceName ): c = ownedMemberTable [ ownedMemberTable [ 'usecase' ] == extendTable . sourceName [ idx ]] if len ( c ) > 0 : for aktor in c . actor : hasilAktor . append ( aktor ) hasilDestinasi . append ( extendTable . destinationName [ idx ]) else : temp = 2 d = ownedMemberTable [ ownedMemberTable [ 'usecase' ] == extendTable . sourceName [ idx - temp ]] for dAktor in d . actor : hasilAktor . append ( dAktor ) hasilDestinasi . append ( extendTable . destinationName [ idx ]) df_a = pd . DataFrame ([ hasilAktor , hasilDestinasi ], index = [ 'actor' , 'action' ]) . T df_a [ 'actor' ] = df_a . groupby ([ 'action' ])[ 'actor' ] . transform ( lambda x : ';' . join ( x )) df_a = df_a [[ 'action' , 'actor' ]] . drop_duplicates () df_a [ 'actor' ][ 0 ] = set ( df_a [ 'actor' ][ 0 ] . split ( \";\" )) # fungsi ini digunakan untuk menyempurnakan format df_a [ 'actor' ][ 0 ] = \";\" . join ( df_a [ 'actor' ][ 0 ]) ownedMemberTable . rename ( columns = { 'usecase' : 'action' }, inplace = True ) dt_b = pd . concat ([ df_a , ownedMemberTable ]) dt_actor_action = dt_b . drop ([ 'id' , 'type_property' ], axis = 1 ) dt_actor_action [ 'actor' ] = dt_actor_action . groupby ([ 'action' ])[ 'actor' ] . transform ( lambda x : ';' . join ( x )) dt_actor_action = dt_actor_action [[ 'action' , 'actor' ]] . drop_duplicates () print ( \" \\n actorActionTable\" ) print ( tabulate ( dt_actor_action , headers = 'keys' , tablefmt = 'psql' )) # print(\"\\nincludeTable\") # print(tabulate(includeTable, headers = 'keys', tablefmt = 'psql')) # # untuk include data ranalyzer # hasilAktor = [] # hasilDestinasi = [] # for idy, angka in enumerate(includeTable.includeName): # f = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy]] # if len(f) > 0: # for aktor in f.actor: # hasilAktor.append(aktor) # hasilDestinasi.append(includeTable.additionName[idy]) # else: # tempY = 2 # g = ownedMemberTable[ownedMemberTable.usecase == includeTable.includeName[idy-tempY]] # for dAktor in g.actor: # hasilAktor.append(dAktor) # hasilDestinasi.append(includeTable.additionName[idy]) # df_a = pd.DataFrame([hasilAktor, hasilDestinasi], index= ['actor', 'action']).T # df_a['actor'] = df_a.groupby(['action'])['actor'].transform(lambda x: ';'.join(x)) # df_a = df_a[['action','actor']].drop_duplicates() # df_a['actor'][0] = set(df_a['actor'][0].split(\";\")) # fungsi ini digunakan untuk menyempurnakan format # df_a['actor'][0] = \";\".join(df_a['actor'][0]) # ownedMemberTable.rename(columns = {'usecase':'action'}, inplace = True) # dt_b = pd.concat([df_a, ownedMemberTable]) # dt_actor_action = dt_b.drop(['id', 'type_property'], axis= 1) # print(\"\\nactorActionTable\") # print(tabulate(dt_actor_action, headers = 'keys', tablefmt = 'psql')) print ( \"done in %0.3f s.\" % ( time () - t0 )) input ( 'Press ENTER to exit' ) myXmlParser . __del__ () except OSError as err : print ( \"OS error: {0} \" . format ( err ))","title":"Module extractreq.usecase_modul1"},{"location":"reference/extractreq/usecase_modul1/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/usecase_modul1/#xmlparser","text":"class xmlParser ( filename = 'IRCI_Researcher.xmi' , tipe_xmi = '{http://schema.omg.org/spec/XMI/2.1}type' , id_xmi = '{http://schema.omg.org/spec/XMI/2.1}id' ) View Source class xmlParser : # inisialisasi def __init__ ( self , filename = 'IRCI_Researcher.xmi' , tipe_xmi = '{http://schema.omg.org/spec/XMI/2.1}type' , id_xmi = '{http://schema.omg.org/spec/XMI/2.1}id' ) : self . namaFile = filename self . xmi_type = tipe_xmi self . xmi_id = id_xmi #fungsi parse tree elemen def elemenTreeParse ( self ) : tree = ET . parse ( self . namaFile ) root = tree . getroot () try : elemenTag = [ elem.tag for elem in root.iter() ] elemenAtribut = [ elem.attrib for elem in root.iter() ] tabelElemen = pd . DataFrame ( [ elemenTag, elemenAtribut ] , index =[ 'Berdasarkan Tag', 'Berdsarkan Atribut' ] ). T return tabelElemen except OSError as err : print ( \"OS error: {0}\" . format ( err )) # fungsi pencarian elemen def cariTreeElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root1 = tree1 . getroot () pencarian = [ num.findall(elemen) for num in root1.iter() ] return pencarian except OSError as err : print ( \"OS error: {0}\" . format ( err )) #fungsi list elemen def listElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () listElemen = [ berdasarkanOwnEnd.attrib for berdasarkanOwnEnd in root.iter(elemen) ] tabelElement = pd . DataFrame ( listElemen ) return tabelElement except OSError as err : print ( \"OS error: {0}\" . format ( err )) # fungsi mencari table spesifik def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement.attrib for packagedElement in root.iter(elemen) ] for num in berdasarkanPackagedELement : a1 = num [ kolom1 ] c1 = num [ self.xmi_type ] d1 = num [ self.xmi_id ] hasil . append ( [ a1, c1, d1 ] ) cleanPackagedELement = pd . DataFrame ( hasil , columns =[ kolom1, self.xmi_type, self.xmi_id ] ) return cleanPackagedELement except OSError as err : print ( \"OS error: {0}\" . format ( err )) # fungsi mencari string def doString ( self ) : try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () print ( ET . tostring ( root , encoding = 'utf8' ). decode ( 'utf8' )) except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataPaketElemen ( self , category = 'packagedElement' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement.attrib for packagedElement in root.iter(category) ] for num in berdasarkanPackagedELement : a1 = num [ self.xmi_id ] b1 = num [ 'name' ] d1 = num [ self.xmi_type ] hasil . append ( [ a1, b1, d1 ] ) paketElemen = pd . DataFrame ( hasil , columns =[ 'id', 'name', 'type' ] ) return paketElemen except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataExtend ( self , category = 'extend' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanExtend = [ packagedElement.attrib for packagedElement in root.iter(category) ] for num in berdasarkanExtend : a1 = num [ self.xmi_id ] b1 = num [ self.xmi_type ] c1 = num [ 'extendedCase' ] d1 = paketElemen [ paketElemen['id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'extension' ] f1 = paketElemen [ paketElemen['id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ( [ a1, b1, c1, d1, e1, f1 ] ) extendTable = pd . DataFrame ( hasil , columns =[ 'id', 'type', 'source', 'sourceName', 'destination', 'destinationName' ] ) return extendTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataInclude ( self , category = 'include' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () byinclude = [ packagedElement.attrib for packagedElement in root.iter(category) ] for num in byinclude : a1 = num [ '{http://schema.omg.org/spec/XMI/2.1}id' ] b1 = num [ '{http://schema.omg.org/spec/XMI/2.1}type' ] c1 = num [ 'includingCase' ] d1 = paketElemen [ paketElemen['id' ] == c1 ] . iloc [ 0 ][ 'name' ] e1 = num [ 'addition' ] f1 = paketElemen [ paketElemen['id' ] == e1 ] . iloc [ 0 ][ 'name' ] hasil . append ( [ a1, b1, c1, d1, e1, f1 ] ) includeTable = pd . DataFrame ( hasil , columns = [ 'id', 'tipe', 'include', 'includeName', 'addition', 'additionName' ] ) return includeTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataOwnedEnd ( self , category = 'ownedEnd' ) : try : # berdasarkan ownedEnd hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedEnd = [ packagedElement.attrib for packagedElement in root.iter(category) ] berdasarkanOwnedEnd for num in berdasarkanOwnedEnd : a1 = num [ 'type' ] b1 = num [ self.xmi_id ] c1 = num [ self.xmi_type ] d1 = paketElemen [ paketElemen['id' ] == a1 ] . iloc [ 0 ][ 'name' ] hasil . append ( [ a1, b1, c1, d1 ] ) ownedEndTable = pd . DataFrame ( hasil , columns =[ 'id_data', 'id_property', 'type_property', 'id_name' ] ) return ownedEndTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def dataOwnedMember ( self , category = 'ownedMember' ) : try : # berdasarkan UML Model hasilNum = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedMember = [ packagedElement for packagedElement in root.iter(category) ] for num in berdasarkanOwnedMember : a = num . attrib [ self.xmi_id ] b = num . attrib [ self.xmi_type ] for index , angka in enumerate ( num . iter ( 'ownedEnd' )) : if index == 0 : c = paketElemen [ paketElemen['id' ] == angka . attrib [ 'type' ] ] . iloc [ 0 ][ 'name' ] else : d = paketElemen [ paketElemen['id' ] == angka . attrib [ 'type' ] ] . iloc [ 0 ][ 'name' ] hasilNum . append ( [ a, b, c, d ] ) ownedMemberTable = pd . DataFrame ( hasilNum , columns =[ 'id', 'type_property', 'actor', 'usecase' ] ) return ownedMemberTable except OSError as err : print ( \"OS error: {0}\" . format ( err )) def __del__ ( self ) : print ( 'Destructor called.' )","title":"xmlParser"},{"location":"reference/extractreq/usecase_modul1/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/usecase_modul1/#caritreeelemen","text":"def cariTreeElemen ( self , elemen ) View Source def cariTreeElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root1 = tree1 . getroot () pencarian = [ num . findall ( elemen ) for num in root1 . iter () ] return pencarian except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"cariTreeElemen"},{"location":"reference/extractreq/usecase_modul1/#dataextend","text":"def dataExtend ( self , category = 'extend' ) View Source def dataExtend ( self , category = ' extend ' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanExtend = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] for num in berdasarkanExtend : a1 = num [ self . xmi_id ] b1 = num [ self . xmi_type ] c1 = num [ ' extendedCase ' ] d1 = paketElemen [ paketElemen [ ' id ' ] == c1 ]. iloc [ 0 ][ ' name ' ] e1 = num [ ' extension ' ] f1 = paketElemen [ paketElemen [ ' id ' ] == e1 ]. iloc [ 0 ][ ' name ' ] hasil . append ( [ a1 , b1 , c1 , d1 , e1 , f1 ] ) extendTable = pd . DataFrame ( hasil , columns = [ ' id ' , ' type ' , ' source ' , ' sourceName ' , ' destination ' , ' destinationName ' ] ) return extendTable except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"dataExtend"},{"location":"reference/extractreq/usecase_modul1/#datainclude","text":"def dataInclude ( self , category = 'include' ) View Source def dataInclude ( self , category = ' include ' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () byinclude = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] for num in byinclude : a1 = num [ ' {http://schema.omg.org/spec/XMI/2.1}id ' ] b1 = num [ ' {http://schema.omg.org/spec/XMI/2.1}type ' ] c1 = num [ ' includingCase ' ] d1 = paketElemen [ paketElemen [ ' id ' ] == c1 ]. iloc [ 0 ][ ' name ' ] e1 = num [ ' addition ' ] f1 = paketElemen [ paketElemen [ ' id ' ] == e1 ]. iloc [ 0 ][ ' name ' ] hasil . append ( [ a1 , b1 , c1 , d1 , e1 , f1 ] ) includeTable = pd . DataFrame ( hasil , columns = [ ' id ' , ' tipe ' , ' include ' , ' includeName ' , ' addition ' , ' additionName ' ] ) return includeTable except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"dataInclude"},{"location":"reference/extractreq/usecase_modul1/#dataownedend","text":"def dataOwnedEnd ( self , category = 'ownedEnd' ) View Source def dataOwnedEnd ( self , category = ' ownedEnd ' ) : try : # berdasarkan ownedEnd hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedEnd = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] berdasarkanOwnedEnd for num in berdasarkanOwnedEnd : a1 = num [ ' type ' ] b1 = num [ self . xmi_id ] c1 = num [ self . xmi_type ] d1 = paketElemen [ paketElemen [ ' id ' ] == a1 ]. iloc [ 0 ][ ' name ' ] hasil . append ( [ a1 , b1 , c1 , d1 ] ) ownedEndTable = pd . DataFrame ( hasil , columns = [ ' id_data ' , ' id_property ' , ' type_property ' , ' id_name ' ] ) return ownedEndTable except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"dataOwnedEnd"},{"location":"reference/extractreq/usecase_modul1/#dataownedmember","text":"def dataOwnedMember ( self , category = 'ownedMember' ) View Source def dataOwnedMember ( self , category = 'ownedMember' ): try : # berdasarkan UML Model hasilNum = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanOwnedMember = [ packagedElement for packagedElement in root . iter ( category )] for num in berdasarkanOwnedMember : a = num . attrib [ self . xmi_id ] b = num . attrib [ self . xmi_type ] for index , angka in enumerate ( num . iter ( 'ownedEnd' )): if index == 0 : c = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] else : d = paketElemen [ paketElemen [ 'id' ] == angka . attrib [ 'type' ]] . iloc [ 0 ][ 'name' ] hasilNum . append ([ a , b , c , d ]) ownedMemberTable = pd . DataFrame ( hasilNum , columns = [ 'id' , 'type_property' , 'actor' , 'usecase' ]) return ownedMemberTable except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"dataOwnedMember"},{"location":"reference/extractreq/usecase_modul1/#datapaketelemen","text":"def dataPaketElemen ( self , category = 'packagedElement' ) View Source def dataPaketElemen ( self , category = ' packagedElement ' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement . attrib for packagedElement in root . iter ( category ) ] for num in berdasarkanPackagedELement : a1 = num [ self . xmi_id ] b1 = num [ ' name ' ] d1 = num [ self . xmi_type ] hasil . append ( [ a1 , b1 , d1 ] ) paketElemen = pd . DataFrame ( hasil , columns = [ ' id ' , ' name ' , ' type ' ] ) return paketElemen except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"dataPaketElemen"},{"location":"reference/extractreq/usecase_modul1/#dostring","text":"def doString ( self ) View Source def doString(self): try: tree1 = ET.parse(self.namaFile) root = tree1.getroot() print(ET.tostring(root, encoding='utf8').decode('utf8')) except OSError as err: print(\"OS error: {0}\".format(err))","title":"doString"},{"location":"reference/extractreq/usecase_modul1/#elementreeparse","text":"def elemenTreeParse ( self ) View Source def elemenTreeParse ( self ) : tree = ET . parse ( self . namaFile ) root = tree . getroot () try : elemenTag = [ elem . tag for elem in root . iter () ] elemenAtribut = [ elem . attrib for elem in root . iter () ] tabelElemen = pd . DataFrame ( [ elemenTag , elemenAtribut ], index = [ ' Berdasarkan Tag ' , ' Berdsarkan Atribut ' ] ) . T return tabelElemen except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"elemenTreeParse"},{"location":"reference/extractreq/usecase_modul1/#listelemen","text":"def listElemen ( self , elemen ) View Source def listElemen ( self , elemen ) : try : tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () listElemen = [ berdasarkanOwnEnd . attrib for berdasarkanOwnEnd in root . iter ( elemen ) ] tabelElement = pd . DataFrame ( listElemen ) return tabelElement except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"listElemen"},{"location":"reference/extractreq/usecase_modul1/#tableelemenspesifik","text":"def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ) View Source def tableElemenSpesifik ( self , elemen = 'packagedElement' , kolom1 = 'name' ) : try : hasil = [] tree1 = ET . parse ( self . namaFile ) root = tree1 . getroot () berdasarkanPackagedELement = [ packagedElement.attrib for packagedElement in root.iter(elemen) ] for num in berdasarkanPackagedELement : a1 = num [ kolom1 ] c1 = num [ self.xmi_type ] d1 = num [ self.xmi_id ] hasil . append ( [ a1, c1, d1 ] ) cleanPackagedELement = pd . DataFrame ( hasil , columns =[ kolom1, self.xmi_type, self.xmi_id ] ) return cleanPackagedELement except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"tableElemenSpesifik"},{"location":"reference/extractreq/usecase_modul2/","text":"Module extractreq.usecase_modul2 None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # -*- coding: utf-8 -*- \"\"\"modul_relasi.ipynb Author Rakha Asyrofi / 05111950010038 Automatically generated by Colaboratory. Original file is located at https://colab.research.google.com/drive/1h6HKNeALV8bXjrxWB0Jn0ztHtLv2cXz8 \"\"\" \"\"\"# Modul2: parsing aksi dan aktor\"\"\" import pandas as pd from tabulate import tabulate import spacy from spacy.lang.en import English from spacy.tokenizer import Tokenizer from tqdm.auto import tqdm from time import time , sleep # template class parsingRequirement class parsingRequirement : # inisialisasi def __init__ ( self , filename ): self . namaFile = filename def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressDf ( self , data ): return data . groupby ( 'id' ) . progress_apply ( lambda x : x ) #fungsi parse tree elemen def membacaCSV ( self ): try : modul_pembacaan = pd . read_csv ( self . namaFile , delimiter = ',' ) return modul_pembacaan except OSError as err : print ( \"OS error: {0} \" . format ( err )) def clean_text ( self , raw_text ): try : nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( raw_text ) lemma_list = [ num . lemma_ . lower () for num in doc if num . is_stop is False and num . is_punct is False and num . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words except OSError as err : print ( \"OS error: {0} \" . format ( err )) # cleaning text def apply_cleaning_function_to_list ( self , X ): try : cleaned_X = [ parsingRequirement . clean_text ( self , element ) for element in X ] parsingRequirement . progressBar ( self , cleaned_X ) return cleaned_X except OSError as err : print ( \"OS error: {0} \" . format ( err )) def data_raw ( self , data_raw ): # get data raw dt = [ num2 for num in data_raw . fillna ( 'empty' ) for num1 in num . split ( ';' ) for num2 in num1 . split ( '.' ) if 'Submitter' in num2 or 'Viewer' in num2 or 'system' in num2 or 'actor' in num2 or 'empty' in num2 ] parsingRequirement . progressBar ( self , dt ) return dt def aksi_aktor ( self , data ): # get data aksi dan aktor try : nlp = English () tokenizer = Tokenizer ( nlp . vocab ) tokens = tokenizer ( data ) a = [ token . text . lower () for token in tokens ] b = [ x for x in a if x == 'submitter' or x == 'Submitter' or x == 'viewer' or x == 'Viewer' or x == 'system' or x == 'actor' ] b1 = \";\" . join ( b ) b1 = b1 . replace ( \"actor\" , \"submitter; viewer\" ) c = [ x for x in a if x != 'submitter' and x != 'Submitter' and x != 'viewer' and x != 'Viewer' and x != 'system' and x != 'actor' ] c1 = \" \" . join ( c ) return b1 , c1 except OSError as err : print ( \"OS error: {0} \" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) if __name__ == \"__main__\" : try : # parsing functional t0 = time () MyParsingRequirement = parsingRequirement ( filename = \"freqs_researcher.txt\" ) freqs = MyParsingRequirement . membacaCSV () data_freqs = MyParsingRequirement . data_raw ( freqs . requirement ) cleaned_freq = MyParsingRequirement . apply_cleaning_function_to_list ( data_freqs ) freqs [ 'aksi' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 1 ] for num in cleaned_freq ] freqs [ 'aktor' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 0 ] for num in cleaned_freq ] tqdm . pandas ( desc = \"freq process\" ) MyParsingRequirement . progressDf ( freqs ) print ( tabulate ( freqs , headers = 'keys' , tablefmt = 'psql' )) # parsing ucd1 MyParsingRequirement = parsingRequirement ( filename = \"researcher_insert_metadata.txt\" ) ucd1 = MyParsingRequirement . membacaCSV () data_ucd1 = MyParsingRequirement . data_raw ( ucd1 . flowOfEvents ) list_index = [( \"data {} \" . format ( idx )) for idx , num in enumerate ( data_ucd1 )] data_list = pd . DataFrame ( data_ucd1 , index = list_index ) data_list = data_list . drop ( index = \"data5\" ) . reset_index () . drop ( labels = [ 'index' ], axis = 1 ) # data_list = data_list.reset_index().drop(labels= ['index'], axis= 1) ucd1 [ 'aksi' ] = data_list cleaned1_ucd = MyParsingRequirement . apply_cleaning_function_to_list ( list ( ucd1 . aksi )) ucd1 [ 'aksi' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 1 ] for num in cleaned1_ucd ] ucd1 [ 'aktor' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 0 ] for num in cleaned1_ucd ] tqdm . pandas ( desc = \"ucd1 process\" ) MyParsingRequirement . progressDf ( ucd1 ) print ( tabulate ( ucd1 , headers = 'keys' , tablefmt = 'psql' )) # parsing ucd2 MyParsingRequirement = parsingRequirement ( filename = \"researcher_search_researcher.txt\" ) ucd2 = MyParsingRequirement . membacaCSV () data_ucd2 = MyParsingRequirement . data_raw ( ucd2 . flowOfEvents ) list2_index = [( \"data {} \" . format ( idx )) for idx , num in enumerate ( data_ucd2 )] data2_list = pd . DataFrame ( data_ucd2 , index = list2_index ) data2_list = data2_list . reset_index () . drop ( labels = [ 'index' ], axis = 1 ) ucd2 [ 'aksi' ] = data2_list cleaned2_ucd = MyParsingRequirement . apply_cleaning_function_to_list ( list ( ucd2 . aksi )) ucd2 [ 'aksi' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 1 ] for num in cleaned2_ucd ] ucd2 [ 'aktor' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 0 ] for num in cleaned2_ucd ] tqdm . pandas ( desc = \"ucd2 process\" ) MyParsingRequirement . progressDf ( ucd2 ) print ( tabulate ( ucd2 , headers = 'keys' , tablefmt = 'psql' )) print ( \"done in %0.3f s.\" % ( time () - t0 )) input ( 'Press ENTER to exit' ) except OSError as err : print ( \"OS error: {0} \" . format ( err )) Classes parsingRequirement class parsingRequirement ( filename ) View Source class parsingRequirement : # inisialisasi def __init__ ( self , filename ): self . namaFile = filename def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressDf ( self , data ): return data . groupby ( 'id' ) . progress_apply ( lambda x : x ) #fungsi parse tree elemen def membacaCSV ( self ): try : modul_pembacaan = pd . read_csv ( self . namaFile , delimiter = ',' ) return modul_pembacaan except OSError as err : print ( \"OS error: {0}\" . format ( err )) def clean_text ( self , raw_text ): try : nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( raw_text ) lemma_list = [ num . lemma_ . lower () for num in doc if num . is_stop is False and num . is_punct is False and num . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words except OSError as err : print ( \"OS error: {0}\" . format ( err )) # cleaning text def apply_cleaning_function_to_list ( self , X ): try : cleaned_X = [ parsingRequirement . clean_text ( self , element ) for element in X ] parsingRequirement . progressBar ( self , cleaned_X ) return cleaned_X except OSError as err : print ( \"OS error: {0}\" . format ( err )) def data_raw ( self , data_raw ): # get data raw dt = [ num2 for num in data_raw . fillna ( 'empty' ) for num1 in num . split ( ';' ) for num2 in num1 . split ( '.' ) if 'Submitter' in num2 or 'Viewer' in num2 or 'system' in num2 or 'actor' in num2 or 'empty' in num2 ] parsingRequirement . progressBar ( self , dt ) return dt def aksi_aktor ( self , data ): # get data aksi dan aktor try : nlp = English () tokenizer = Tokenizer ( nlp . vocab ) tokens = tokenizer ( data ) a = [ token . text . lower () for token in tokens ] b = [ x for x in a if x == 'submitter' or x == 'Submitter' or x == 'viewer' or x == 'Viewer' or x == 'system' or x == 'actor' ] b1 = \";\" . join ( b ) b1 = b1 . replace ( \"actor\" , \"submitter; viewer\" ) c = [ x for x in a if x != 'submitter' and x != 'Submitter' and x != 'viewer' and x != 'Viewer' and x != 'system' and x != 'actor' ] c1 = \" \" . join ( c ) return b1 , c1 except OSError as err : print ( \"OS error: {0}\" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) Methods aksi_aktor def aksi_aktor ( self , data ) View Source def aksi_aktor ( self , data ): # get data aksi dan aktor try : nlp = English () tokenizer = Tokenizer ( nlp . vocab ) tokens = tokenizer ( data ) a = [ token.text.lower () for token in tokens ] b = [ x for x in a if x == 'submitter' or x == 'Submitter' or x == 'viewer' or x == 'Viewer' or x == 'system' or x == 'actor' ] b1 = \";\" . join ( b ) b1 = b1 . replace ( \"actor\" , \"submitter; viewer\" ) c = [ x for x in a if x != 'submitter' and x != 'Submitter' and x != 'viewer' and x != 'Viewer' and x != 'system' and x != 'actor' ] c1 = \" \" . join ( c ) return b1 , c1 except OSError as err : print ( \"OS error: {0}\" . format ( err )) apply_cleaning_function_to_list def apply_cleaning_function_to_list ( self , X ) View Source def apply_cleaning_function_to_list ( self , X ) : try : cleaned_X = [ parsingRequirement . clean_text ( self , element ) for element in X ] parsingRequirement . progressBar ( self , cleaned_X ) return cleaned_X except OSError as err : print ( \" OS error: {0} \" . format ( err )) clean_text def clean_text ( self , raw_text ) View Source def clean_text ( self , raw_text ): try : nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( raw_text ) lemma_list = [ num . lemma_ . lower () for num in doc if num . is_stop is False and num . is_punct is False and num . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words except OSError as err : print ( \"OS error: {0}\" . format ( err )) data_raw def data_raw ( self , data_raw ) View Source def data_raw ( self , data_raw ) : # get data raw dt = [ num2 for num in data_raw . fillna ( ' empty ' ) for num1 in num . split ( ' ; ' ) for num2 in num1 . split ( ' . ' ) if ' Submitter ' in num2 or ' Viewer ' in num2 or ' system ' in num2 or ' actor ' in num2 or ' empty ' in num2 ] parsingRequirement . progressBar ( self , dt ) return dt membacaCSV def membacaCSV ( self ) View Source def membacaCSV ( self ) : try : modul_pembacaan = pd . read_csv ( self . namaFile , delimiter = ' , ' ) return modul_pembacaan except OSError as err : print ( \" OS error: {0} \" . format ( err )) progressBar def progressBar ( self , data ) View Source def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () progressDf def progressDf ( self , data ) View Source def progressDf ( self , data ) : return data . groupby ( ' id ' ) . progress_apply ( lambda x : x )","title":"Usecase Modul2"},{"location":"reference/extractreq/usecase_modul2/#module-extractrequsecase_modul2","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # -*- coding: utf-8 -*- \"\"\"modul_relasi.ipynb Author Rakha Asyrofi / 05111950010038 Automatically generated by Colaboratory. Original file is located at https://colab.research.google.com/drive/1h6HKNeALV8bXjrxWB0Jn0ztHtLv2cXz8 \"\"\" \"\"\"# Modul2: parsing aksi dan aktor\"\"\" import pandas as pd from tabulate import tabulate import spacy from spacy.lang.en import English from spacy.tokenizer import Tokenizer from tqdm.auto import tqdm from time import time , sleep # template class parsingRequirement class parsingRequirement : # inisialisasi def __init__ ( self , filename ): self . namaFile = filename def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressDf ( self , data ): return data . groupby ( 'id' ) . progress_apply ( lambda x : x ) #fungsi parse tree elemen def membacaCSV ( self ): try : modul_pembacaan = pd . read_csv ( self . namaFile , delimiter = ',' ) return modul_pembacaan except OSError as err : print ( \"OS error: {0} \" . format ( err )) def clean_text ( self , raw_text ): try : nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( raw_text ) lemma_list = [ num . lemma_ . lower () for num in doc if num . is_stop is False and num . is_punct is False and num . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words except OSError as err : print ( \"OS error: {0} \" . format ( err )) # cleaning text def apply_cleaning_function_to_list ( self , X ): try : cleaned_X = [ parsingRequirement . clean_text ( self , element ) for element in X ] parsingRequirement . progressBar ( self , cleaned_X ) return cleaned_X except OSError as err : print ( \"OS error: {0} \" . format ( err )) def data_raw ( self , data_raw ): # get data raw dt = [ num2 for num in data_raw . fillna ( 'empty' ) for num1 in num . split ( ';' ) for num2 in num1 . split ( '.' ) if 'Submitter' in num2 or 'Viewer' in num2 or 'system' in num2 or 'actor' in num2 or 'empty' in num2 ] parsingRequirement . progressBar ( self , dt ) return dt def aksi_aktor ( self , data ): # get data aksi dan aktor try : nlp = English () tokenizer = Tokenizer ( nlp . vocab ) tokens = tokenizer ( data ) a = [ token . text . lower () for token in tokens ] b = [ x for x in a if x == 'submitter' or x == 'Submitter' or x == 'viewer' or x == 'Viewer' or x == 'system' or x == 'actor' ] b1 = \";\" . join ( b ) b1 = b1 . replace ( \"actor\" , \"submitter; viewer\" ) c = [ x for x in a if x != 'submitter' and x != 'Submitter' and x != 'viewer' and x != 'Viewer' and x != 'system' and x != 'actor' ] c1 = \" \" . join ( c ) return b1 , c1 except OSError as err : print ( \"OS error: {0} \" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) if __name__ == \"__main__\" : try : # parsing functional t0 = time () MyParsingRequirement = parsingRequirement ( filename = \"freqs_researcher.txt\" ) freqs = MyParsingRequirement . membacaCSV () data_freqs = MyParsingRequirement . data_raw ( freqs . requirement ) cleaned_freq = MyParsingRequirement . apply_cleaning_function_to_list ( data_freqs ) freqs [ 'aksi' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 1 ] for num in cleaned_freq ] freqs [ 'aktor' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 0 ] for num in cleaned_freq ] tqdm . pandas ( desc = \"freq process\" ) MyParsingRequirement . progressDf ( freqs ) print ( tabulate ( freqs , headers = 'keys' , tablefmt = 'psql' )) # parsing ucd1 MyParsingRequirement = parsingRequirement ( filename = \"researcher_insert_metadata.txt\" ) ucd1 = MyParsingRequirement . membacaCSV () data_ucd1 = MyParsingRequirement . data_raw ( ucd1 . flowOfEvents ) list_index = [( \"data {} \" . format ( idx )) for idx , num in enumerate ( data_ucd1 )] data_list = pd . DataFrame ( data_ucd1 , index = list_index ) data_list = data_list . drop ( index = \"data5\" ) . reset_index () . drop ( labels = [ 'index' ], axis = 1 ) # data_list = data_list.reset_index().drop(labels= ['index'], axis= 1) ucd1 [ 'aksi' ] = data_list cleaned1_ucd = MyParsingRequirement . apply_cleaning_function_to_list ( list ( ucd1 . aksi )) ucd1 [ 'aksi' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 1 ] for num in cleaned1_ucd ] ucd1 [ 'aktor' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 0 ] for num in cleaned1_ucd ] tqdm . pandas ( desc = \"ucd1 process\" ) MyParsingRequirement . progressDf ( ucd1 ) print ( tabulate ( ucd1 , headers = 'keys' , tablefmt = 'psql' )) # parsing ucd2 MyParsingRequirement = parsingRequirement ( filename = \"researcher_search_researcher.txt\" ) ucd2 = MyParsingRequirement . membacaCSV () data_ucd2 = MyParsingRequirement . data_raw ( ucd2 . flowOfEvents ) list2_index = [( \"data {} \" . format ( idx )) for idx , num in enumerate ( data_ucd2 )] data2_list = pd . DataFrame ( data_ucd2 , index = list2_index ) data2_list = data2_list . reset_index () . drop ( labels = [ 'index' ], axis = 1 ) ucd2 [ 'aksi' ] = data2_list cleaned2_ucd = MyParsingRequirement . apply_cleaning_function_to_list ( list ( ucd2 . aksi )) ucd2 [ 'aksi' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 1 ] for num in cleaned2_ucd ] ucd2 [ 'aktor' ] = [ MyParsingRequirement . aksi_aktor ( num )[ 0 ] for num in cleaned2_ucd ] tqdm . pandas ( desc = \"ucd2 process\" ) MyParsingRequirement . progressDf ( ucd2 ) print ( tabulate ( ucd2 , headers = 'keys' , tablefmt = 'psql' )) print ( \"done in %0.3f s.\" % ( time () - t0 )) input ( 'Press ENTER to exit' ) except OSError as err : print ( \"OS error: {0} \" . format ( err ))","title":"Module extractreq.usecase_modul2"},{"location":"reference/extractreq/usecase_modul2/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/usecase_modul2/#parsingrequirement","text":"class parsingRequirement ( filename ) View Source class parsingRequirement : # inisialisasi def __init__ ( self , filename ): self . namaFile = filename def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressDf ( self , data ): return data . groupby ( 'id' ) . progress_apply ( lambda x : x ) #fungsi parse tree elemen def membacaCSV ( self ): try : modul_pembacaan = pd . read_csv ( self . namaFile , delimiter = ',' ) return modul_pembacaan except OSError as err : print ( \"OS error: {0}\" . format ( err )) def clean_text ( self , raw_text ): try : nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( raw_text ) lemma_list = [ num . lemma_ . lower () for num in doc if num . is_stop is False and num . is_punct is False and num . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words except OSError as err : print ( \"OS error: {0}\" . format ( err )) # cleaning text def apply_cleaning_function_to_list ( self , X ): try : cleaned_X = [ parsingRequirement . clean_text ( self , element ) for element in X ] parsingRequirement . progressBar ( self , cleaned_X ) return cleaned_X except OSError as err : print ( \"OS error: {0}\" . format ( err )) def data_raw ( self , data_raw ): # get data raw dt = [ num2 for num in data_raw . fillna ( 'empty' ) for num1 in num . split ( ';' ) for num2 in num1 . split ( '.' ) if 'Submitter' in num2 or 'Viewer' in num2 or 'system' in num2 or 'actor' in num2 or 'empty' in num2 ] parsingRequirement . progressBar ( self , dt ) return dt def aksi_aktor ( self , data ): # get data aksi dan aktor try : nlp = English () tokenizer = Tokenizer ( nlp . vocab ) tokens = tokenizer ( data ) a = [ token . text . lower () for token in tokens ] b = [ x for x in a if x == 'submitter' or x == 'Submitter' or x == 'viewer' or x == 'Viewer' or x == 'system' or x == 'actor' ] b1 = \";\" . join ( b ) b1 = b1 . replace ( \"actor\" , \"submitter; viewer\" ) c = [ x for x in a if x != 'submitter' and x != 'Submitter' and x != 'viewer' and x != 'Viewer' and x != 'system' and x != 'actor' ] c1 = \" \" . join ( c ) return b1 , c1 except OSError as err : print ( \"OS error: {0}\" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' )","title":"parsingRequirement"},{"location":"reference/extractreq/usecase_modul2/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/usecase_modul2/#aksi_aktor","text":"def aksi_aktor ( self , data ) View Source def aksi_aktor ( self , data ): # get data aksi dan aktor try : nlp = English () tokenizer = Tokenizer ( nlp . vocab ) tokens = tokenizer ( data ) a = [ token.text.lower () for token in tokens ] b = [ x for x in a if x == 'submitter' or x == 'Submitter' or x == 'viewer' or x == 'Viewer' or x == 'system' or x == 'actor' ] b1 = \";\" . join ( b ) b1 = b1 . replace ( \"actor\" , \"submitter; viewer\" ) c = [ x for x in a if x != 'submitter' and x != 'Submitter' and x != 'viewer' and x != 'Viewer' and x != 'system' and x != 'actor' ] c1 = \" \" . join ( c ) return b1 , c1 except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"aksi_aktor"},{"location":"reference/extractreq/usecase_modul2/#apply_cleaning_function_to_list","text":"def apply_cleaning_function_to_list ( self , X ) View Source def apply_cleaning_function_to_list ( self , X ) : try : cleaned_X = [ parsingRequirement . clean_text ( self , element ) for element in X ] parsingRequirement . progressBar ( self , cleaned_X ) return cleaned_X except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"apply_cleaning_function_to_list"},{"location":"reference/extractreq/usecase_modul2/#clean_text","text":"def clean_text ( self , raw_text ) View Source def clean_text ( self , raw_text ): try : nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( raw_text ) lemma_list = [ num . lemma_ . lower () for num in doc if num . is_stop is False and num . is_punct is False and num . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"clean_text"},{"location":"reference/extractreq/usecase_modul2/#data_raw","text":"def data_raw ( self , data_raw ) View Source def data_raw ( self , data_raw ) : # get data raw dt = [ num2 for num in data_raw . fillna ( ' empty ' ) for num1 in num . split ( ' ; ' ) for num2 in num1 . split ( ' . ' ) if ' Submitter ' in num2 or ' Viewer ' in num2 or ' system ' in num2 or ' actor ' in num2 or ' empty ' in num2 ] parsingRequirement . progressBar ( self , dt ) return dt","title":"data_raw"},{"location":"reference/extractreq/usecase_modul2/#membacacsv","text":"def membacaCSV ( self ) View Source def membacaCSV ( self ) : try : modul_pembacaan = pd . read_csv ( self . namaFile , delimiter = ' , ' ) return modul_pembacaan except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"membacaCSV"},{"location":"reference/extractreq/usecase_modul2/#progressbar","text":"def progressBar ( self , data ) View Source def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close ()","title":"progressBar"},{"location":"reference/extractreq/usecase_modul2/#progressdf","text":"def progressDf ( self , data ) View Source def progressDf ( self , data ) : return data . groupby ( ' id ' ) . progress_apply ( lambda x : x )","title":"progressDf"},{"location":"reference/extractreq/usecase_modul3/","text":"Module extractreq.usecase_modul3 None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # -*- coding: utf-8 -*- \"\"\"modul_relasi.ipynb Author Rakha Asyrofi / 05111950010038 Automatically generated by Colaboratory. Original file is located at https://colab.research.google.com/drive/1h6HKNeALV8bXjrxWB0Jn0ztHtLv2cXz8 \"\"\" \"\"\"# Modul3: pencarian relasi\"\"\" # !pip install -U pywsd # !pip install -U wn==0.0.23 import pandas as pd import numpy as np from spacy.lang.en import English from spacy.tokenizer import Tokenizer from pywsd import disambiguate from pywsd.similarity import similarity_by_path from tabulate import tabulate from tqdm.auto import tqdm from time import sleep , time # template class ucdReq class ucdReq : #inicsialisasi def __init__ ( self , data_aksi_aktor , tabel_usecase ): self . aksi_aktor = data_aksi_aktor self . dt_usecase = tabel_usecase def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressBarDataFrame ( self , data ): return data . progress_apply ( lambda x : x ) def fulldataset ( self , inputData ): xl = pd . ExcelFile ( self . aksi_aktor ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputData ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def fulldataset_xmi ( self , inputXMI ): xl = pd . ExcelFile ( self . dt_usecase ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputXMI ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ): xl = pd . ExcelFile ( self . aksi_aktor ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [ {} ] ...' . format ( sh )) print ( df . head ()) def synset_word ( self , data ): # pencarian synset try : nlp = English () # tokenizer = nlp.Defaults.create_tokenizer(nlp) tokenizer = tokenizer = Tokenizer ( nlp . vocab ) num_token = [ token . text for token in tokenizer ( data )] word = [ disambiguate ( x ) for x in num_token ] wordsynset = [[ n [ 1 ] for n in y if n [ 1 ] is not None ] for y in word ] final_synset = [ val [ 0 ] for val in wordsynset if len ( val ) > 0 ] # ucdReq.progressBar(self, final_synset) return final_synset except OSError as err : print ( \"OS error: {0} \" . format ( err )) def wsd_greedy ( self , s1 , s2 ): # kombinasi algoritma wsd dan greedy try : scores = [[ x for x in [ similarity_by_path ( i1 , i2 , option = 'wup' ) for i2 in s2 ] if x is not None ] for i1 in s1 ] flt_scores = [ val for val in scores if len ( val ) > 0 ] ucdReq . progressBar ( self , flt_scores ) # visualisasi proses list_wsd = [ np . max ( num ) for num in flt_scores ] dt = ( sorted ( list_wsd , reverse = True )) dt_value = dt [: len ( dt ) - 1 ] kalkulasi = 2 * sum ( dt_value ) / (( len ( flt_scores )) + ( len ( flt_scores [ 0 ]))) return kalkulasi except OSError as err : print ( \"OS error: {0} \" . format ( err )) def similaritas_doc ( self , doc1 , doc2 ): # pencarian kesmaaan dokumen try : synsets1 = ucdReq . synset_word ( self , doc1 ) synsets2 = ucdReq . synset_word ( self , doc2 ) kalkulasi_synset = ( ucdReq . wsd_greedy ( self , synsets1 , synsets2 ) + ucdReq . wsd_greedy ( self , synsets2 , synsets1 )) / 2 return kalkulasi_synset except OSError as err : print ( \"OS error: {0} \" . format ( err )) def ucdMeasurement ( self , keyword1 , keyword2 ): #pengukuran try : hasil_wsd = [[ ucdReq . similaritas_doc ( self , num , angka ) for angka in keyword2 ] for num in keyword1 ] wsd_df = pd . DataFrame ( hasil_wsd ) # ucdReq.progressBar(self, wsd_df) return wsd_df except OSError as err : print ( \"OS error: {0} \" . format ( err )) def change_case ( self , word ): return '' . join ([ ' ' + i . lower () if i . isupper () else i for i in word ]) . lstrip ( ' ' ) def thresholdvalue ( self , threshold , data ): try : dt = data . values >= threshold d1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = d1 . isin ([ True ]) d2 = d1 . where ( mask , other = 0 ) mask2 = d1 . isin ([ False ]) return d2 . where ( mask2 , other = 1 ) except OSError as err : print ( \"OS error: {0} \" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) if __name__ == \"__main__\" : try : # openfile t0 = time () MyucdReq = ucdReq ( data_aksi_aktor = r 'data_aksi_aktor.xlsx' , tabel_usecase = r 'data_xmi.xlsx' ) tabel_freq = 'tabel_freqs' freqs = MyucdReq . fulldataset ( inputData = tabel_freq ) # data dari txt tabel_ucd1 = 'tabel_ucd1' ucd1 = MyucdReq . fulldataset ( inputData = tabel_ucd1 ) # data dari txt tabel_ucd2 = 'tabel_ucd2' ucd2 = MyucdReq . fulldataset ( inputData = tabel_ucd2 ) # data dari txt namaUsecase = 'tabel_usecase' useCaseTable = MyucdReq . fulldataset_xmi ( inputXMI = namaUsecase ) # dari xmi tbl_1 = MyucdReq . ucdMeasurement ( freqs . aksi , ucd1 . dropna () . aksi ) tbl_1 . columns = ucd1 . dropna () . usecase tbl_1 . index = freqs . id tbl_1 . rename ( columns = { 'insertMetadata_1' : 'UC01' , 'insertMetadata_2' : 'UC01' , 'insertMetadata_3' : 'UC01' , 'insertMetadata_4' : 'UC01' , 'insertMetadata_5' : 'UC01' , 'searchArticle_1' : 'UC03' , 'searchArticle_2' : 'UC03' , 'searchArticle_3' : 'UC03' , 'searchArticle_4' : 'UC03' , 'searchArticle_5' : 'UC03' , 'viewNextResult_1' : 'UC04' , 'viewNextResult_2' : 'UC04' , 'viewNextResult_3' : 'UC04' , }, inplace = True ) tqdm . pandas ( desc = \"Data Pengukuran antara functional dan ucd1 (txt)\" ) MyucdReq . progressBarDataFrame ( tbl_1 ) print ( tabulate ( tbl_1 , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_2 = MyucdReq . ucdMeasurement ( freqs . aksi , ucd2 . dropna () . aksi ) tbl_2 . columns = ucd2 . dropna () . usecase tbl_2 . index = freqs . id tbl_2 . rename ( columns = { 'searchResearcher_1' : 'UC02' , 'searchResearcher_2' : 'UC02' , 'searchResearcher_3' : 'UC02' , 'orderByRelevancy_1' : 'UC05' , 'orderByRelevancy_2' : 'UC05' , 'orderByRelevancy_3' : 'UC05' , 'orderByScore_1' : 'UC06' , 'orderByScore_2' : 'UC06' , 'orderByScore_3' : 'UC06' , 'viewDetailResearcher_1' : 'UC07' , 'viewDetailResearcher_2' : 'UC07' , 'viewDetailResearcher_3' : 'UC07' , 'removeArticle_1' : 'UC09' , 'removeArticle_2' : 'UC09' , 'removeArticle_3' : 'UC09' , 'editProfile_1' : 'UC08' , 'editProfile_2' : 'UC08' , 'editProfile_3' : 'UC08' , 'editProfile_4' : 'UC08' , 'editProfile_5' : 'UC08' , }, inplace = True ) tqdm . pandas ( desc = \"Data filter pengukuran maksmimum antara functional terhadap ucd1 dan ucd2 (txt)\" ) MyucdReq . progressBarDataFrame ( tbl_2 ) print ( tabulate ( tbl_2 , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_3 = pd . concat ([ tbl_1 , tbl_2 ], axis = 1 ) tbl_3 [ 'uc01' ] = tbl_3 . UC01 . values . max ( 1 ) tbl_3 [ 'uc02' ] = tbl_3 . UC02 . values . max ( 1 ) tbl_3 [ 'uc03' ] = tbl_3 . UC03 . values . max ( 1 ) tbl_3 [ 'uc04' ] = tbl_3 . UC04 . values . max ( 1 ) tbl_3 [ 'uc05' ] = tbl_3 . UC05 . values . max ( 1 ) tbl_3 [ 'uc06' ] = tbl_3 . UC06 . values . max ( 1 ) tbl_3 [ 'uc07' ] = tbl_3 . UC07 . values . max ( 1 ) tbl_3 [ 'uc08' ] = tbl_3 . UC08 . values . max ( 1 ) tbl_3 [ 'uc09' ] = tbl_3 . UC09 . values . max ( 1 ) tbl_3filter = tbl_3 . drop ([ 'UC01' , 'UC02' , 'UC03' , 'UC04' , 'UC05' , 'UC06' , 'UC07' , 'UC08' , 'UC09' ], axis = 1 ) tqdm . pandas ( desc = \"Data filter pengukuran maksmimum antara functional terhadap ucd1 dan ucd2 (txt)\" ) MyucdReq . progressBarDataFrame ( tbl_3filter ) print ( tabulate ( tbl_3filter , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_4 = MyucdReq . thresholdvalue ( 0.4 , tbl_3filter ) tqdm . pandas ( desc = \"Data hasil relasi antara kebutuhan dan kasus penggunaan (txt) \\n \" ) MyucdReq . progressBarDataFrame ( tbl_4 ) print ( tabulate ( tbl_4 , headers = 'keys' , tablefmt = 'psql' )) # xmi code print ( \" \\n\\n \" ) data_ucd = [ MyucdReq . change_case ( num ) for num in useCaseTable . name ] tbl_1x = MyucdReq . ucdMeasurement ( freqs . aksi , data_ucd ) tbl_1x . index = freqs . id tbl_1x . columns = useCaseTable . name tbl_1x . rename ( columns = { 'insertMetadata' : 'uc01' , 'searchArticle' : 'uc03' , 'viewNextResult' : 'uc04' , 'searchResearcher' : 'uc02' , 'orderByRelevancy' : 'uc05' , 'orderByScore' : 'uc06' , 'viewDetailOfResearcher' : 'uc07' , 'removeArticle' : 'uc09' , 'editProfile' : 'uc08' }, inplace = True ) print ( \" \\n\\n \" ) tqdm . pandas ( desc = \"Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\" ) MyucdReq . progressBarDataFrame ( tbl_1x ) print ( tabulate ( tbl_1x , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_5 = MyucdReq . thresholdvalue ( 0.3 , tbl_1x ) tqdm . pandas ( desc = \"Data hasil threshold relasi (xmi) \\n \" ) MyucdReq . progressBarDataFrame ( tbl_5 ) print ( tabulate ( tbl_5 , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) list_usecase = [ num for num in tbl_5 . columns ] # tbl_6 = tbl_4.merge(tbl_5, how= 'inner', left_index= True, right_index= True, on= list_usecase) tbl_6 = tbl_4 . merge ( tbl_5 , how = 'inner' , on = list_usecase ) tqdm . pandas ( desc = \"Data hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi) \\n \" ) MyucdReq . progressBarDataFrame ( tbl_6 ) print ( tabulate ( tbl_6 , headers = 'keys' , tablefmt = 'psql' )) print ( \"done in %0.3f s.\" % ( time () - t0 )) input ( 'Press ENTER to exit' ) MyucdReq . __del__ () except OSError as err : print ( \"OS error: {0} \" . format ( err )) Classes ucdReq class ucdReq ( data_aksi_aktor , tabel_usecase ) View Source class ucdReq : #inicsialisasi def __init__ ( self , data_aksi_aktor , tabel_usecase ) : self . aksi_aktor = data_aksi_aktor self . dt_usecase = tabel_usecase def progressBar ( self , data ) : pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressBarDataFrame ( self , data ) : return data . progress_apply ( lambda x : x ) def fulldataset ( self , inputData ) : xl = pd . ExcelFile ( self . aksi_aktor ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputData ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def fulldataset_xmi ( self , inputXMI ) : xl = pd . ExcelFile ( self . dt_usecase ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputXMI ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ) : xl = pd . ExcelFile ( self . aksi_aktor ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def synset_word ( self , data ) : # pencarian synset try : nlp = English () # tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokenizer = tokenizer = Tokenizer ( nlp . vocab ) num_token = [ token.text for token in tokenizer(data) ] word = [ disambiguate(x) for x in num_token ] wordsynset = [ [n[1 ] for n in y if n [ 1 ] is not None ] for y in word ] final_synset = [ val[0 ] for val in wordsynset if len ( val ) > 0 ] # ucdReq . progressBar ( self , final_synset ) return final_synset except OSError as err : print ( \"OS error: {0}\" . format ( err )) def wsd_greedy ( self , s1 , s2 ) : # kombinasi algoritma wsd dan greedy try : scores = [ [x for x in [similarity_by_path(i1, i2, option= 'wup') for i2 in s2 ] if x is not None ] for i1 in s1 ] flt_scores = [ val for val in scores if len(val) > 0 ] ucdReq . progressBar ( self , flt_scores ) # visualisasi proses list_wsd = [ np.max(num) for num in flt_scores ] dt = ( sorted ( list_wsd , reverse = True )) dt_value = dt [ :len(dt)-1 ] kalkulasi = 2 * sum ( dt_value ) / (( len ( flt_scores )) + ( len ( flt_scores [ 0 ] ))) return kalkulasi except OSError as err : print ( \"OS error: {0}\" . format ( err )) def similaritas_doc ( self , doc1 , doc2 ) : # pencarian kesmaaan dokumen try : synsets1 = ucdReq . synset_word ( self , doc1 ) synsets2 = ucdReq . synset_word ( self , doc2 ) kalkulasi_synset = ( ucdReq . wsd_greedy ( self , synsets1 , synsets2 ) + ucdReq . wsd_greedy ( self , synsets2 , synsets1 )) / 2 return kalkulasi_synset except OSError as err : print ( \"OS error: {0}\" . format ( err )) def ucdMeasurement ( self , keyword1 , keyword2 ) : #pengukuran try : hasil_wsd = [ [ucdReq.similaritas_doc(self, num, angka) for angka in keyword2 ] for num in keyword1 ] wsd_df = pd . DataFrame ( hasil_wsd ) # ucdReq . progressBar ( self , wsd_df ) return wsd_df except OSError as err : print ( \"OS error: {0}\" . format ( err )) def change_case ( self , word ) : return '' . join ( [ ' '+i.lower() if i.isupper() else i for i in word ] ). lstrip ( ' ' ) def thresholdvalue ( self , threshold , data ) : try : dt = data . values >= threshold d1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = d1 . isin ( [ True ] ) d2 = d1 . where ( mask , other = 0 ) mask2 = d1 . isin ( [ False ] ) return d2 . where ( mask2 , other = 1 ) except OSError as err : print ( \"OS error: {0}\" . format ( err )) def __del__ ( self ) : print ( 'Destructor called.' ) Methods change_case def change_case ( self , word ) View Source def change_case ( self , word ) : return '' . join ( [ ' ' + i . lower () if i . isupper () else i for i in word ] ) . lstrip ( ' ' ) fulldataset def fulldataset ( self , inputData ) View Source def fulldataset ( self , inputData ) : xl = pd . ExcelFile ( self . aksi_aktor ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputData ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua fulldataset_xmi def fulldataset_xmi ( self , inputXMI ) View Source def fulldataset_xmi ( self , inputXMI ) : xl = pd . ExcelFile ( self . dt_usecase ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputXMI ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua preprocessing def preprocessing ( self ) View Source def preprocessing ( self ) : xl = pd . ExcelFile ( self . aksi_aktor ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ()) progressBar def progressBar ( self , data ) View Source def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () progressBarDataFrame def progressBarDataFrame ( self , data ) View Source def progressBarDataFrame ( self , data ) : return data . progress_apply ( lambda x : x ) similaritas_doc def similaritas_doc ( self , doc1 , doc2 ) View Source def similaritas_doc ( self , doc1 , doc2 ) : # pencarian kesmaaan dokumen try : synsets1 = ucdReq . synset_word ( self , doc1 ) synsets2 = ucdReq . synset_word ( self , doc2 ) kalkulasi_synset = ( ucdReq . wsd_greedy ( self , synsets1 , synsets2 ) + ucdReq . wsd_greedy ( self , synsets2 , synsets1 )) / 2 return kalkulasi_synset except OSError as err : print ( \" OS error: {0} \" . format ( err )) synset_word def synset_word ( self , data ) View Source def synset_word ( self , data ) : # pencarian synset try : nlp = English () # tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokenizer = tokenizer = Tokenizer ( nlp . vocab ) num_token = [ token . text for token in tokenizer ( data ) ] word = [ disambiguate ( x ) for x in num_token ] wordsynset = [[ n [ 1 ] for n in y if n [ 1 ] is not None ] for y in word ] final_synset = [ val [ 0 ] for val in wordsynset if len ( val ) > 0 ] # ucdReq . progressBar ( self , final_synset ) return final_synset except OSError as err : print ( \" OS error: {0} \" . format ( err )) thresholdvalue def thresholdvalue ( self , threshold , data ) View Source def thresholdvalue ( self , threshold , data ) : try : dt = data . values >= threshold d1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = d1 . isin ( [ True ] ) d2 = d1 . where ( mask , other = 0 ) mask2 = d1 . isin ( [ False ] ) return d2 . where ( mask2 , other = 1 ) except OSError as err : print ( \"OS error: {0}\" . format ( err )) ucdMeasurement def ucdMeasurement ( self , keyword1 , keyword2 ) View Source def ucdMeasurement ( self , keyword1 , keyword2 ) : # pengukuran try : hasil_wsd = [[ ucdReq . similaritas_doc ( self , num , angka ) for angka in keyword2 ] for num in keyword1 ] wsd_df = pd . DataFrame ( hasil_wsd ) # ucdReq . progressBar ( self , wsd_df ) return wsd_df except OSError as err : print ( \" OS error: {0} \" . format ( err )) wsd_greedy def wsd_greedy ( self , s1 , s2 ) View Source def wsd_greedy ( self , s1 , s2 ) : # kombinasi algoritma wsd dan greedy try : scores = [[ x for x in [ similarity_by_path ( i1 , i2 , option = ' wup ' ) for i2 in s2 ] if x is not None ] for i1 in s1 ] flt_scores = [ val for val in scores if len ( val ) > 0 ] ucdReq . progressBar ( self , flt_scores ) # visualisasi proses list_wsd = [ np . max ( num ) for num in flt_scores ] dt = ( sorted ( list_wsd , reverse = True )) dt_value = dt [: len ( dt ) - 1 ] kalkulasi = 2 * sum ( dt_value ) / (( len ( flt_scores )) + ( len ( flt_scores [ 0 ] ))) return kalkulasi except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"Usecase Modul3"},{"location":"reference/extractreq/usecase_modul3/#module-extractrequsecase_modul3","text":"None None View Source __copyright__ = \"Copyright (c) 2021\" __author__ = \"Rakha Asyrofi\" __date__ = \"2021-10-08:18:07:39\" # -*- coding: utf-8 -*- \"\"\"modul_relasi.ipynb Author Rakha Asyrofi / 05111950010038 Automatically generated by Colaboratory. Original file is located at https://colab.research.google.com/drive/1h6HKNeALV8bXjrxWB0Jn0ztHtLv2cXz8 \"\"\" \"\"\"# Modul3: pencarian relasi\"\"\" # !pip install -U pywsd # !pip install -U wn==0.0.23 import pandas as pd import numpy as np from spacy.lang.en import English from spacy.tokenizer import Tokenizer from pywsd import disambiguate from pywsd.similarity import similarity_by_path from tabulate import tabulate from tqdm.auto import tqdm from time import sleep , time # template class ucdReq class ucdReq : #inicsialisasi def __init__ ( self , data_aksi_aktor , tabel_usecase ): self . aksi_aktor = data_aksi_aktor self . dt_usecase = tabel_usecase def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressBarDataFrame ( self , data ): return data . progress_apply ( lambda x : x ) def fulldataset ( self , inputData ): xl = pd . ExcelFile ( self . aksi_aktor ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputData ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def fulldataset_xmi ( self , inputXMI ): xl = pd . ExcelFile ( self . dt_usecase ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputXMI ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ): xl = pd . ExcelFile ( self . aksi_aktor ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [ {} ] ...' . format ( sh )) print ( df . head ()) def synset_word ( self , data ): # pencarian synset try : nlp = English () # tokenizer = nlp.Defaults.create_tokenizer(nlp) tokenizer = tokenizer = Tokenizer ( nlp . vocab ) num_token = [ token . text for token in tokenizer ( data )] word = [ disambiguate ( x ) for x in num_token ] wordsynset = [[ n [ 1 ] for n in y if n [ 1 ] is not None ] for y in word ] final_synset = [ val [ 0 ] for val in wordsynset if len ( val ) > 0 ] # ucdReq.progressBar(self, final_synset) return final_synset except OSError as err : print ( \"OS error: {0} \" . format ( err )) def wsd_greedy ( self , s1 , s2 ): # kombinasi algoritma wsd dan greedy try : scores = [[ x for x in [ similarity_by_path ( i1 , i2 , option = 'wup' ) for i2 in s2 ] if x is not None ] for i1 in s1 ] flt_scores = [ val for val in scores if len ( val ) > 0 ] ucdReq . progressBar ( self , flt_scores ) # visualisasi proses list_wsd = [ np . max ( num ) for num in flt_scores ] dt = ( sorted ( list_wsd , reverse = True )) dt_value = dt [: len ( dt ) - 1 ] kalkulasi = 2 * sum ( dt_value ) / (( len ( flt_scores )) + ( len ( flt_scores [ 0 ]))) return kalkulasi except OSError as err : print ( \"OS error: {0} \" . format ( err )) def similaritas_doc ( self , doc1 , doc2 ): # pencarian kesmaaan dokumen try : synsets1 = ucdReq . synset_word ( self , doc1 ) synsets2 = ucdReq . synset_word ( self , doc2 ) kalkulasi_synset = ( ucdReq . wsd_greedy ( self , synsets1 , synsets2 ) + ucdReq . wsd_greedy ( self , synsets2 , synsets1 )) / 2 return kalkulasi_synset except OSError as err : print ( \"OS error: {0} \" . format ( err )) def ucdMeasurement ( self , keyword1 , keyword2 ): #pengukuran try : hasil_wsd = [[ ucdReq . similaritas_doc ( self , num , angka ) for angka in keyword2 ] for num in keyword1 ] wsd_df = pd . DataFrame ( hasil_wsd ) # ucdReq.progressBar(self, wsd_df) return wsd_df except OSError as err : print ( \"OS error: {0} \" . format ( err )) def change_case ( self , word ): return '' . join ([ ' ' + i . lower () if i . isupper () else i for i in word ]) . lstrip ( ' ' ) def thresholdvalue ( self , threshold , data ): try : dt = data . values >= threshold d1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = d1 . isin ([ True ]) d2 = d1 . where ( mask , other = 0 ) mask2 = d1 . isin ([ False ]) return d2 . where ( mask2 , other = 1 ) except OSError as err : print ( \"OS error: {0} \" . format ( err )) def __del__ ( self ): print ( 'Destructor called.' ) if __name__ == \"__main__\" : try : # openfile t0 = time () MyucdReq = ucdReq ( data_aksi_aktor = r 'data_aksi_aktor.xlsx' , tabel_usecase = r 'data_xmi.xlsx' ) tabel_freq = 'tabel_freqs' freqs = MyucdReq . fulldataset ( inputData = tabel_freq ) # data dari txt tabel_ucd1 = 'tabel_ucd1' ucd1 = MyucdReq . fulldataset ( inputData = tabel_ucd1 ) # data dari txt tabel_ucd2 = 'tabel_ucd2' ucd2 = MyucdReq . fulldataset ( inputData = tabel_ucd2 ) # data dari txt namaUsecase = 'tabel_usecase' useCaseTable = MyucdReq . fulldataset_xmi ( inputXMI = namaUsecase ) # dari xmi tbl_1 = MyucdReq . ucdMeasurement ( freqs . aksi , ucd1 . dropna () . aksi ) tbl_1 . columns = ucd1 . dropna () . usecase tbl_1 . index = freqs . id tbl_1 . rename ( columns = { 'insertMetadata_1' : 'UC01' , 'insertMetadata_2' : 'UC01' , 'insertMetadata_3' : 'UC01' , 'insertMetadata_4' : 'UC01' , 'insertMetadata_5' : 'UC01' , 'searchArticle_1' : 'UC03' , 'searchArticle_2' : 'UC03' , 'searchArticle_3' : 'UC03' , 'searchArticle_4' : 'UC03' , 'searchArticle_5' : 'UC03' , 'viewNextResult_1' : 'UC04' , 'viewNextResult_2' : 'UC04' , 'viewNextResult_3' : 'UC04' , }, inplace = True ) tqdm . pandas ( desc = \"Data Pengukuran antara functional dan ucd1 (txt)\" ) MyucdReq . progressBarDataFrame ( tbl_1 ) print ( tabulate ( tbl_1 , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_2 = MyucdReq . ucdMeasurement ( freqs . aksi , ucd2 . dropna () . aksi ) tbl_2 . columns = ucd2 . dropna () . usecase tbl_2 . index = freqs . id tbl_2 . rename ( columns = { 'searchResearcher_1' : 'UC02' , 'searchResearcher_2' : 'UC02' , 'searchResearcher_3' : 'UC02' , 'orderByRelevancy_1' : 'UC05' , 'orderByRelevancy_2' : 'UC05' , 'orderByRelevancy_3' : 'UC05' , 'orderByScore_1' : 'UC06' , 'orderByScore_2' : 'UC06' , 'orderByScore_3' : 'UC06' , 'viewDetailResearcher_1' : 'UC07' , 'viewDetailResearcher_2' : 'UC07' , 'viewDetailResearcher_3' : 'UC07' , 'removeArticle_1' : 'UC09' , 'removeArticle_2' : 'UC09' , 'removeArticle_3' : 'UC09' , 'editProfile_1' : 'UC08' , 'editProfile_2' : 'UC08' , 'editProfile_3' : 'UC08' , 'editProfile_4' : 'UC08' , 'editProfile_5' : 'UC08' , }, inplace = True ) tqdm . pandas ( desc = \"Data filter pengukuran maksmimum antara functional terhadap ucd1 dan ucd2 (txt)\" ) MyucdReq . progressBarDataFrame ( tbl_2 ) print ( tabulate ( tbl_2 , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_3 = pd . concat ([ tbl_1 , tbl_2 ], axis = 1 ) tbl_3 [ 'uc01' ] = tbl_3 . UC01 . values . max ( 1 ) tbl_3 [ 'uc02' ] = tbl_3 . UC02 . values . max ( 1 ) tbl_3 [ 'uc03' ] = tbl_3 . UC03 . values . max ( 1 ) tbl_3 [ 'uc04' ] = tbl_3 . UC04 . values . max ( 1 ) tbl_3 [ 'uc05' ] = tbl_3 . UC05 . values . max ( 1 ) tbl_3 [ 'uc06' ] = tbl_3 . UC06 . values . max ( 1 ) tbl_3 [ 'uc07' ] = tbl_3 . UC07 . values . max ( 1 ) tbl_3 [ 'uc08' ] = tbl_3 . UC08 . values . max ( 1 ) tbl_3 [ 'uc09' ] = tbl_3 . UC09 . values . max ( 1 ) tbl_3filter = tbl_3 . drop ([ 'UC01' , 'UC02' , 'UC03' , 'UC04' , 'UC05' , 'UC06' , 'UC07' , 'UC08' , 'UC09' ], axis = 1 ) tqdm . pandas ( desc = \"Data filter pengukuran maksmimum antara functional terhadap ucd1 dan ucd2 (txt)\" ) MyucdReq . progressBarDataFrame ( tbl_3filter ) print ( tabulate ( tbl_3filter , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_4 = MyucdReq . thresholdvalue ( 0.4 , tbl_3filter ) tqdm . pandas ( desc = \"Data hasil relasi antara kebutuhan dan kasus penggunaan (txt) \\n \" ) MyucdReq . progressBarDataFrame ( tbl_4 ) print ( tabulate ( tbl_4 , headers = 'keys' , tablefmt = 'psql' )) # xmi code print ( \" \\n\\n \" ) data_ucd = [ MyucdReq . change_case ( num ) for num in useCaseTable . name ] tbl_1x = MyucdReq . ucdMeasurement ( freqs . aksi , data_ucd ) tbl_1x . index = freqs . id tbl_1x . columns = useCaseTable . name tbl_1x . rename ( columns = { 'insertMetadata' : 'uc01' , 'searchArticle' : 'uc03' , 'viewNextResult' : 'uc04' , 'searchResearcher' : 'uc02' , 'orderByRelevancy' : 'uc05' , 'orderByScore' : 'uc06' , 'viewDetailOfResearcher' : 'uc07' , 'removeArticle' : 'uc09' , 'editProfile' : 'uc08' }, inplace = True ) print ( \" \\n\\n \" ) tqdm . pandas ( desc = \"Data hasil relasi antara kebutuhan dan kasus penggunaan (xmi)\" ) MyucdReq . progressBarDataFrame ( tbl_1x ) print ( tabulate ( tbl_1x , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) tbl_5 = MyucdReq . thresholdvalue ( 0.3 , tbl_1x ) tqdm . pandas ( desc = \"Data hasil threshold relasi (xmi) \\n \" ) MyucdReq . progressBarDataFrame ( tbl_5 ) print ( tabulate ( tbl_5 , headers = 'keys' , tablefmt = 'psql' )) print ( \" \\n\\n \" ) list_usecase = [ num for num in tbl_5 . columns ] # tbl_6 = tbl_4.merge(tbl_5, how= 'inner', left_index= True, right_index= True, on= list_usecase) tbl_6 = tbl_4 . merge ( tbl_5 , how = 'inner' , on = list_usecase ) tqdm . pandas ( desc = \"Data hasil join relasi antara kebutuhan dan kasus penggunaan (txt dan xmi) \\n \" ) MyucdReq . progressBarDataFrame ( tbl_6 ) print ( tabulate ( tbl_6 , headers = 'keys' , tablefmt = 'psql' )) print ( \"done in %0.3f s.\" % ( time () - t0 )) input ( 'Press ENTER to exit' ) MyucdReq . __del__ () except OSError as err : print ( \"OS error: {0} \" . format ( err ))","title":"Module extractreq.usecase_modul3"},{"location":"reference/extractreq/usecase_modul3/#classes","text":"","title":"Classes"},{"location":"reference/extractreq/usecase_modul3/#ucdreq","text":"class ucdReq ( data_aksi_aktor , tabel_usecase ) View Source class ucdReq : #inicsialisasi def __init__ ( self , data_aksi_aktor , tabel_usecase ) : self . aksi_aktor = data_aksi_aktor self . dt_usecase = tabel_usecase def progressBar ( self , data ) : pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close () def progressBarDataFrame ( self , data ) : return data . progress_apply ( lambda x : x ) def fulldataset ( self , inputData ) : xl = pd . ExcelFile ( self . aksi_aktor ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputData ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def fulldataset_xmi ( self , inputXMI ) : xl = pd . ExcelFile ( self . dt_usecase ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputXMI ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ) : xl = pd . ExcelFile ( self . aksi_aktor ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def synset_word ( self , data ) : # pencarian synset try : nlp = English () # tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokenizer = tokenizer = Tokenizer ( nlp . vocab ) num_token = [ token.text for token in tokenizer(data) ] word = [ disambiguate(x) for x in num_token ] wordsynset = [ [n[1 ] for n in y if n [ 1 ] is not None ] for y in word ] final_synset = [ val[0 ] for val in wordsynset if len ( val ) > 0 ] # ucdReq . progressBar ( self , final_synset ) return final_synset except OSError as err : print ( \"OS error: {0}\" . format ( err )) def wsd_greedy ( self , s1 , s2 ) : # kombinasi algoritma wsd dan greedy try : scores = [ [x for x in [similarity_by_path(i1, i2, option= 'wup') for i2 in s2 ] if x is not None ] for i1 in s1 ] flt_scores = [ val for val in scores if len(val) > 0 ] ucdReq . progressBar ( self , flt_scores ) # visualisasi proses list_wsd = [ np.max(num) for num in flt_scores ] dt = ( sorted ( list_wsd , reverse = True )) dt_value = dt [ :len(dt)-1 ] kalkulasi = 2 * sum ( dt_value ) / (( len ( flt_scores )) + ( len ( flt_scores [ 0 ] ))) return kalkulasi except OSError as err : print ( \"OS error: {0}\" . format ( err )) def similaritas_doc ( self , doc1 , doc2 ) : # pencarian kesmaaan dokumen try : synsets1 = ucdReq . synset_word ( self , doc1 ) synsets2 = ucdReq . synset_word ( self , doc2 ) kalkulasi_synset = ( ucdReq . wsd_greedy ( self , synsets1 , synsets2 ) + ucdReq . wsd_greedy ( self , synsets2 , synsets1 )) / 2 return kalkulasi_synset except OSError as err : print ( \"OS error: {0}\" . format ( err )) def ucdMeasurement ( self , keyword1 , keyword2 ) : #pengukuran try : hasil_wsd = [ [ucdReq.similaritas_doc(self, num, angka) for angka in keyword2 ] for num in keyword1 ] wsd_df = pd . DataFrame ( hasil_wsd ) # ucdReq . progressBar ( self , wsd_df ) return wsd_df except OSError as err : print ( \"OS error: {0}\" . format ( err )) def change_case ( self , word ) : return '' . join ( [ ' '+i.lower() if i.isupper() else i for i in word ] ). lstrip ( ' ' ) def thresholdvalue ( self , threshold , data ) : try : dt = data . values >= threshold d1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = d1 . isin ( [ True ] ) d2 = d1 . where ( mask , other = 0 ) mask2 = d1 . isin ( [ False ] ) return d2 . where ( mask2 , other = 1 ) except OSError as err : print ( \"OS error: {0}\" . format ( err )) def __del__ ( self ) : print ( 'Destructor called.' )","title":"ucdReq"},{"location":"reference/extractreq/usecase_modul3/#methods","text":"","title":"Methods"},{"location":"reference/extractreq/usecase_modul3/#change_case","text":"def change_case ( self , word ) View Source def change_case ( self , word ) : return '' . join ( [ ' ' + i . lower () if i . isupper () else i for i in word ] ) . lstrip ( ' ' )","title":"change_case"},{"location":"reference/extractreq/usecase_modul3/#fulldataset","text":"def fulldataset ( self , inputData ) View Source def fulldataset ( self , inputData ) : xl = pd . ExcelFile ( self . aksi_aktor ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputData ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua","title":"fulldataset"},{"location":"reference/extractreq/usecase_modul3/#fulldataset_xmi","text":"def fulldataset_xmi ( self , inputXMI ) View Source def fulldataset_xmi ( self , inputXMI ) : xl = pd . ExcelFile ( self . dt_usecase ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputXMI ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua","title":"fulldataset_xmi"},{"location":"reference/extractreq/usecase_modul3/#preprocessing","text":"def preprocessing ( self ) View Source def preprocessing ( self ) : xl = pd . ExcelFile ( self . aksi_aktor ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/extractreq/usecase_modul3/#progressbar","text":"def progressBar ( self , data ) View Source def progressBar ( self , data ): pbar = tqdm ( iterable = range ( 0 , len ( data )), leave = False , desc = 'loading..' , unit = 'synsets' ) for i in pbar : sleep ( 0.1 ) pbar . update ( 1 ) return pbar . close ()","title":"progressBar"},{"location":"reference/extractreq/usecase_modul3/#progressbardataframe","text":"def progressBarDataFrame ( self , data ) View Source def progressBarDataFrame ( self , data ) : return data . progress_apply ( lambda x : x )","title":"progressBarDataFrame"},{"location":"reference/extractreq/usecase_modul3/#similaritas_doc","text":"def similaritas_doc ( self , doc1 , doc2 ) View Source def similaritas_doc ( self , doc1 , doc2 ) : # pencarian kesmaaan dokumen try : synsets1 = ucdReq . synset_word ( self , doc1 ) synsets2 = ucdReq . synset_word ( self , doc2 ) kalkulasi_synset = ( ucdReq . wsd_greedy ( self , synsets1 , synsets2 ) + ucdReq . wsd_greedy ( self , synsets2 , synsets1 )) / 2 return kalkulasi_synset except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"similaritas_doc"},{"location":"reference/extractreq/usecase_modul3/#synset_word","text":"def synset_word ( self , data ) View Source def synset_word ( self , data ) : # pencarian synset try : nlp = English () # tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokenizer = tokenizer = Tokenizer ( nlp . vocab ) num_token = [ token . text for token in tokenizer ( data ) ] word = [ disambiguate ( x ) for x in num_token ] wordsynset = [[ n [ 1 ] for n in y if n [ 1 ] is not None ] for y in word ] final_synset = [ val [ 0 ] for val in wordsynset if len ( val ) > 0 ] # ucdReq . progressBar ( self , final_synset ) return final_synset except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"synset_word"},{"location":"reference/extractreq/usecase_modul3/#thresholdvalue","text":"def thresholdvalue ( self , threshold , data ) View Source def thresholdvalue ( self , threshold , data ) : try : dt = data . values >= threshold d1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = d1 . isin ( [ True ] ) d2 = d1 . where ( mask , other = 0 ) mask2 = d1 . isin ( [ False ] ) return d2 . where ( mask2 , other = 1 ) except OSError as err : print ( \"OS error: {0}\" . format ( err ))","title":"thresholdvalue"},{"location":"reference/extractreq/usecase_modul3/#ucdmeasurement","text":"def ucdMeasurement ( self , keyword1 , keyword2 ) View Source def ucdMeasurement ( self , keyword1 , keyword2 ) : # pengukuran try : hasil_wsd = [[ ucdReq . similaritas_doc ( self , num , angka ) for angka in keyword2 ] for num in keyword1 ] wsd_df = pd . DataFrame ( hasil_wsd ) # ucdReq . progressBar ( self , wsd_df ) return wsd_df except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"ucdMeasurement"},{"location":"reference/extractreq/usecase_modul3/#wsd_greedy","text":"def wsd_greedy ( self , s1 , s2 ) View Source def wsd_greedy ( self , s1 , s2 ) : # kombinasi algoritma wsd dan greedy try : scores = [[ x for x in [ similarity_by_path ( i1 , i2 , option = ' wup ' ) for i2 in s2 ] if x is not None ] for i1 in s1 ] flt_scores = [ val for val in scores if len ( val ) > 0 ] ucdReq . progressBar ( self , flt_scores ) # visualisasi proses list_wsd = [ np . max ( num ) for num in flt_scores ] dt = ( sorted ( list_wsd , reverse = True )) dt_value = dt [: len ( dt ) - 1 ] kalkulasi = 2 * sum ( dt_value ) / (( len ( flt_scores )) + ( len ( flt_scores [ 0 ] ))) return kalkulasi except OSError as err : print ( \" OS error: {0} \" . format ( err ))","title":"wsd_greedy"}]}